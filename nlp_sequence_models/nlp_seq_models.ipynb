{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "836e503e",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Sequence Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Sequence Models da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb99097",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a860861",
   "metadata": {},
   "source": [
    "## Neural Networks for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09068229",
   "metadata": {},
   "source": [
    "Modelos mais simples são um ótimo baseline mas podem não capturar bem a o sentimento de uma frase. Nesses casos, podemos utilizar redes neurais.\n",
    "\n",
    "As redes neurais (Neural Networks) são um tipo de modelo de aprendizado de máquina inspirado na estrutura e no funcionamento do cérebro humano. Elas consistem em camadas de unidades interconectadas (neurônios) que processam informações e aprendem a realizar tarefas através de exemplos. As principais componentes das redes neurais são:\n",
    "\n",
    "1. **Camada de Entrada (Input Layer):** Recebe os dados brutos.\n",
    "2. **Camadas Ocultas (Hidden Layers):** Realizam processamento intermediário. Cada neurônio em uma camada oculta está conectado a neurônios da camada anterior e da camada seguinte, e aplica uma função de ativação para transformar a entrada.\n",
    "3. **Camada de Saída (Output Layer):** Produz o resultado final, como uma classificação ou previsão.\n",
    "\n",
    "A análise de sentimento envolve a **classificação de textos**, como comentários ou tweets, para determinar se expressam sentimentos positivos, negativos ou neutros. As redes neurais, especialmente as arquiteturas avançadas como LSTM (Long Short-Term Memory) e GRU (Gated Recurrent Units), são amplamente usadas nessa tarefa devido à sua capacidade de **capturar dependências de longo prazo em sequências de texto**.\n",
    "\n",
    "O processo de análise de sentimento com redes neurais segue os seguintes passos:\n",
    "\n",
    "1. **Pré-processamento:** O texto bruto é limpo e transformado em uma **representação numérica**, como vetores de palavras (Word Embeddings) ou **sequências de índices de palavras**.\n",
    "2. **Arquitetura do Modelo:**\n",
    "   - **Embedding Layer:** Transforma as palavras em vetores densos que capturam seus significados.\n",
    "   - **Camadas Recurrentes (LSTM/GRU):** Processam a sequência de vetores, mantendo informações contextuais ao longo do tempo.\n",
    "   - **Camadas Densas (Fully Connected):** Agregam informações das camadas anteriores e produzem a probabilidade de cada classe de sentimento (positivo, negativo, neutro).\n",
    "3. **Treinamento:** O modelo é treinado em um conjunto de dados rotulados, ajustando os pesos das conexões entre neurônios para minimizar o erro na classificação.\n",
    "4. **Avaliação e Ajuste:** O desempenho do modelo é avaliado em dados de validação, e ajustes são feitos para melhorar a precisão.\n",
    "5. **Predição:** O modelo treinado é usado para analisar novos textos e prever o sentimento.\n",
    "\n",
    "As redes neurais permitem uma análise de sentimento mais robusta, capturando nuances e contextos que métodos mais simples podem perder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2da600",
   "metadata": {},
   "source": [
    "Os dados de entrada como já mencionados são **representações numéricas** de palavras, como as ilustradas a seguir\n",
    "\n",
    "<img src=\"./imgs/word_representation.png\">\n",
    "\n",
    "<img src=\"./imgs/forward_propagation.png\">\n",
    "\n",
    "Observe que a rede acima possui três camadas (input layer, hidden layer e output layer) e três saídas (ex. positivo, negativo e neutro). Para ir de uma camada para outra você pode usar uma matriz $W^{i}$ para propagar para a próxima camada. Portanto, chamamos esse conceito de ir da entrada até a camada final de propagação direta (forward propagation).\n",
    "\n",
    "Observe que adicionamos zeros para preenchimento para corresponder ao tamanho do tweet mais longo. Uma rede neural na configuração que você pode ver acima **só pode processar um tweet por vez**. Para tornar o treinamento mais eficiente (mais rápido), você deseja processar muitos tweets **em paralelo**. Você consegue isso juntando muitos tweets em uma matriz e depois passando essa matriz (em vez de tweets individuais) pela rede neural. Então a rede neural pode realizar seus cálculos em todos os tweets ao mesmo tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3125c3a",
   "metadata": {},
   "source": [
    "## Dense Layers and ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd91e779",
   "metadata": {},
   "source": [
    "As **camadas densas**, também conhecidas como camadas totalmente conectadas (Fully Connected Layers), são um tipo de camada em redes neurais onde **cada neurônio de uma camada está conectado a todos os neurônios da camada seguinte**. Essas conexões são ponderadas e ajustadas durante o processo de treinamento para aprender a mapear entradas para saídas desejadas. A camada Densa é o cálculo do produto interno entre um conjunto de pesos treináveis (matriz de pesos) e um vetor de entrada. Elas funcionam da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/dense_layer.png\">\n",
    "\n",
    "1. **Entrada:** Recebe um vetor de ativação da camada anterior.\n",
    "2. **Pesos e Bias:** Cada conexão tem um peso associado, e cada neurônio tem um valor de bias (viés).\n",
    "3. **Produto Ponto:** A camada calcula o produto ponto entre os vetores de entrada e os pesos, e soma o bias.\n",
    "4. **Função de Ativação:** O resultado é passado por uma função de ativação, que introduz não-linearidade no modelo, permitindo a aprendizagem de relações complexas.\n",
    "\n",
    "A **função de ativação ReLU** é uma das mais populares em redes neurais modernas devido à sua simplicidade e eficácia. ReLU é definida como:\n",
    "\n",
    "$$ f(x) = \\max(0, x) $$\n",
    "\n",
    "**Características:**\n",
    "1. **Linearidade por Partes:** ReLU mantém a linearidade para valores positivos, mas define todos os valores negativos como zero.\n",
    "2. **Não-Linearidade:** Introduz não-linearidade no modelo, essencial para aprender representações complexas.\n",
    "3. **Eficiente Computacionalmente:** Computacionalmente simples e eficiente de calcular.\n",
    "4. **Problema do Neurônio Morto:** Uma desvantagem potencial é que, se muitos neurônios saírem da faixa ativa (produzirem sempre zero), podem \"morrer\" e parar de aprender.\n",
    "\n",
    "<img src=\"./imgs/relu.png\">\n",
    "\n",
    "A caixa laranja na imagem acima mostra a camada densa. Uma camada de ativação é o conjunto de nós azuis mostrados com a caixa laranja na imagem abaixo. Concretamente, uma das camadas de ativação mais comumente utilizadas é a unidade linear retificada (ReLU).\n",
    "\n",
    "**Uso em Camadas Densas:**\n",
    "- **Aprimoramento da Expressividade:** A aplicação da ReLU em camadas densas permite que a rede aprenda representações mais ricas e capture relações não lineares nos dados.\n",
    "- **Mitigação do Desvanecimento de Gradiente:** ReLU ajuda a mitigar o problema do desvanecimento de gradiente, permitindo que gradientes maiores fluam durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df9bc81",
   "metadata": {},
   "source": [
    "## Embedding and Mean Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d672ad51",
   "metadata": {},
   "source": [
    "Usando uma camada de embedding, podemos aprender os embeddings para cada palavra do vocabulário da seguinte maneira:\n",
    "\n",
    "<img src=\"./imgs/embedding_layer.png\">\n",
    "\n",
    "A camada de embedding é uma camada especial utilizada em redes neurais para transformar palavras ou tokens em vetores densos de dimensão fixa, onde cada vetor captura informações semânticas sobre a palavra. Este processo é fundamental no processamento de linguagem natural (NLP) e permite que o modelo aprenda representações de palavras que refletem suas relações semânticas. Ele funciona da seguinte forma:\n",
    "\n",
    "1. **Entrada**: Recebe uma sequência de índices, onde cada índice corresponde a uma palavra ou token no vocabulário.\n",
    "2. **Lookup**: Cada índice é mapeado para um vetor denso, geralmente inicializado aleatoriamente e ajustado durante o treinamento.\n",
    "3. **Saída**: Produz uma matriz onde cada linha é o vetor de embedding correspondente a uma palavra na sequência de entrada.\n",
    "\n",
    "Essa camada possui os seguintes benefícios:\n",
    "\n",
    "1. **Dimensionalidade Reduzida**: Transforma palavras em vetores densos de menor dimensão, facilitando o processamento.\n",
    "2. **Captura de Semântica**: As palavras com significados semelhantes tendem a ter vetores próximos no espaço de embedding.\n",
    "3. **Treinável**: Os vetores de embedding são ajustados durante o treinamento para otimizar a tarefa específica, como classificação de texto ou tradução.\n",
    "\n",
    "A camada média (mean layer) permite tirar a média dos embeddings. Elas são usadas para **agregar informações** ao longo de uma sequência, calculando a média dos vetores de embedding ou das ativações ao longo da sequência. Esta técnica é simples e pode ser eficaz para **capturar uma representação global do contexto** ou do conteúdo de um texto. O vetor resultante da camada de média pode ser passado para camadas densas ou outras camadas de rede neural para realizar a classificação de sentimento, determinando se a frase expressa um sentimento positivo, negativo ou neutro. Fuciona da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/mean_layer.png\">\n",
    "\n",
    "1. **Entrada**: Recebe uma matriz de vetores de embedding ou ativações de uma camada anterior, onde cada linha corresponde a uma palavra ou token na sequência.\n",
    "2. **Cálculo da Média**: Calcula a média ao longo de uma dimensão específica (geralmente ao longo da dimensão da sequência).\n",
    "3. **Saída**: Produz um único vetor que representa a média das ativações ou embeddings ao longo da sequência.\n",
    "\n",
    "Possui os seguintes benefícios:\n",
    "\n",
    "1. **Simplicidade**: Fácil de implementar e computacionalmente eficiente.\n",
    "2. **Redução de Dimensionalidade**: Reduz a sequência de vetores a um único vetor, simplificando o processamento subsequente.\n",
    "3. **Representação Global**: Fornece uma representação global da sequência, capturando informações de todas as palavras ou tokens.\n",
    "\n",
    "Esta camada não possui nenhum parâmetro treinável."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe74e28",
   "metadata": {},
   "source": [
    "## Traditional Language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402cd33",
   "metadata": {},
   "source": [
    "Os modelos de idiomas tradicionais utilizam probabilidades para ajudar a identificar qual frase provavelmente ocorrerá.\n",
    "\n",
    "<img src=\"./imgs/trad_models1.png\">\n",
    "\n",
    "No exemplo acima, a segunda frase é a que provavelmente ocorrerá, pois tem a maior probabilidade de acontecer. Para calcular as probabilidades, você pode fazer o seguinte:\n",
    "\n",
    "<img src=\"./imgs/trad_models2.png\">\n",
    "\n",
    "Grandes gramas de N capturam dependências entre palavras distantes e **precisam de muito espaço e RAM**. Portanto, recorremos ao uso de diferentes tipos de alternativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f4e68a",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8583368",
   "metadata": {},
   "source": [
    "A frase seguinte é um exemplo de que os modelos probabilísticos não tem um bom desempenho, a exemplo, os modelos n-gram.\n",
    "\n",
    "<img src=\"./imgs/rnn1.png\">\n",
    "\n",
    "\n",
    "Um n-grama (trigrama) só olharia para \"did not\" e **tentaria concluir a frase a partir daí**, não veria o contexto nem as palavras anteriores. Como resultado, o modelo não poderá ver o início da frase \"I called her but she\". Provavelmente a palavra mais provável é \"have\" depois do \"did not\". Os RNNs nos ajudam a resolver esse problema, sendo capazes de **rastrear dependências muito mais longe uma da outra**. À medida que o RNN passa por um corpus de texto, ele capta algumas informações da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/rnn2.png\">\n",
    "\n",
    "Observe que, à medida que você alimenta mais informações no modelo, **a retenção da palavra anterior fica mais fraca, mas ainda está lá**. Olhe para o retângulo laranja acima e veja como ele se torna menor ao percorrer o texto. Isso mostra que seu modelo é capaz de capturar dependências e se lembra de uma palavra anterior, embora esteja no início de uma frase ou parágrafo. Outra vantagem dos RRNs é que muitos computações compartilham parâmetros. Ou seja, ao contrário dos modelos tradicionais de N-gramas, onde cada estado (ou cada grupo de palavras) tem seus próprios parâmetros independentes, as RNNs usam um conjunto fixo de parâmetros ao longo de toda a sequência de entrada.\n",
    "\n",
    "1. **Modelos Tradicionais de N-gramas**:\n",
    "   - Em modelos de N-gramas, os parâmetros são específicos para cada combinação de palavras de um tamanho fixo (N). Isso significa que o modelo precisa armazenar informações separadas para cada possível N-grama.\n",
    "   - Isso pode levar a um grande consumo de espaço e memória, especialmente com um vocabulário grande, pois a quantidade de N-gramas únicos pode ser enorme.\n",
    "\n",
    "2. **Redes Neurais Recorrentes (RNNs)**:\n",
    "   - As RNNs, por outro lado, processam a sequência de texto de forma iterativa, palavra por palavra, ou token por token.\n",
    "   - Uma RNN usa os mesmos parâmetros (pesos e bias) em cada passo da sequência. Esses parâmetros são aplicados repetidamente enquanto a RNN percorre a sequência de entrada.\n",
    "   - O compartilhamento de parâmetros ocorre porque a RNN aplica a mesma função de atualização (com os mesmos pesos) para processar cada palavra na sequência. Essa função leva em consideração tanto a palavra atual quanto o estado oculto anterior (que contém informações acumuladas das palavras anteriores).\n",
    "   \n",
    "**Benefícios do Compartilhamento de Parâmetros**\n",
    "\n",
    "- **Eficiência de Memória**: Como as RNNs não precisam armazenar parâmetros separados para cada combinação de palavras, elas são muito mais eficientes em termos de memória.\n",
    "- **Generalização**: O compartilhamento de parâmetros ajuda o modelo a generalizar melhor, pois ele aprende uma representação mais compacta e reutilizável das dependências na sequência de entrada.\n",
    "\n",
    "Imagine que você tenha uma sequência de palavras: \\[w_1, w_2, w_3, \\ldots, w_n\\]. \n",
    "\n",
    "- Em um N-grama de trigramas, você teria parâmetros específicos para cada trigrama possível, como (w_1, w_2, w_3), (w_2, w_3, w_4), etc.\n",
    "- Em uma RNN, você tem um conjunto fixo de parâmetros \\( \\theta \\) que são usados em cada passo para atualizar o estado oculto \\( h_t \\) com base na palavra atual \\( w_t \\) e o estado oculto anterior \\( h_{t-1} \\).\n",
    "\n",
    "Esse compartilhamento de parâmetros é uma das razões pelas quais as RNNs são eficazes em capturar dependências de longo prazo e são uma escolha popular para tarefas de processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c43931",
   "metadata": {},
   "source": [
    "## Applications of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf5c14e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T21:40:47.389758Z",
     "start_time": "2024-07-02T21:40:47.383693Z"
    }
   },
   "source": [
    "Redes Neurais Recorrentes (RNNs, do inglês Recurrent Neural Networks) são uma classe de redes neurais particularmente eficazes no processamento de dados sequenciais, como texto, áudio e séries temporais. Em NLP (Processamento de Linguagem Natural), as RNNs têm várias aplicações importantes devido à sua capacidade de capturar dependências temporais e contextuais em sequências de dados.\n",
    "\n",
    "| **Task**                        | **LSTM** | **GRU** | **BiRNN** |\n",
    "|----------------------------------|:--------:|:-------:|:---------:|\n",
    "| Modelagem de Linguagem           |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Tradução Automática              |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Análise de Sentimento            |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Reconhecimento de Entidades Nomeadas (NER) | ✔️ | ✔️ | ✔️ |\n",
    "| Classificação de Texto           |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Sumarização de Texto             |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Geração de Texto                 |    ✔️    |   ✔️    |    ✔️     |\n",
    "\n",
    "Arquiteturas Comuns de RNNs em NLP:\n",
    "- **LSTM:** Long Short-Term Memory é frequentemente usado para todas as tarefas listadas devido à sua capacidade de capturar dependências de longo prazo e gerenciar o problema do desvanecimento de gradiente.\n",
    "- **GRU:** Gated Recurrent Units são uma alternativa mais simples e computacionalmente eficiente às LSTMs, mas ainda muito eficazes para a maioria das tarefas de NLP.\n",
    "- **BiRNN:** Bidirectional RNNs processam a sequência de texto em ambas as direções, o que pode ser vantajoso para tarefas que dependem do contexto completo da sequência.\n",
    "\n",
    "Cada tipo de RNN pode ser usado para resolver essas tarefas de maneira eficaz, dependendo das necessidades específicas do problema e dos recursos computacionais disponíveis.\n",
    "\n",
    "Existem muitas maneiras de implementar um modelo RNN:\n",
    "\n",
    "- one to one: dadas algumas pontuações de um campeonato, você pode prever o vencedor.\n",
    "- one to many: dada uma imagem, você pode prever qual será a legenda.\n",
    "- many to one: Dado um tweet, você pode prever o sentimento desse tweet.\n",
    "- many to many: dada uma frase em inglês, você pode traduzi -la para seu equivalente alemão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec2a5de",
   "metadata": {},
   "source": [
    "## Math in Simple RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1013ca",
   "metadata": {},
   "source": [
    "Para entender como funciona uma RNN simples (vanilla RNN), vamos explorar a matemática por trás dela usando uma representação gráfica e equações.\n",
    "\n",
    "### Conceitos e Notação\n",
    "\n",
    "1. **Estado Oculto $ h^{<t>} $:**\n",
    "   - Representa a memória da RNN no tempo $ t $.\n",
    "   - Atualizado em cada passo de tempo com base no estado oculto anterior $ h^{<t-1>} $ e na entrada atual $ x^{<t>} $.\n",
    "\n",
    "2. **Função de Ativação $ g $:**\n",
    "   - Pode ser uma função não-linear como a tangente hiperbólica ($\\tanh$) ou ReLU (Rectified Linear Unit).\n",
    "\n",
    "3. **Pesos e Biases:**\n",
    "   - $ W_{hh} $: Pesos que conectam o estado oculto anterior ao novo estado oculto.\n",
    "   - $ W_{hx} $: Pesos que conectam a entrada atual ao novo estado oculto.\n",
    "   - $ W_{yh} $: Pesos que conectam o estado oculto à saída.\n",
    "   - $ b_h $: Bias do estado oculto.\n",
    "   - $ b_y $: Bias da saída.\n",
    "\n",
    "### Atualização do Estado Oculto\n",
    "\n",
    "A fórmula geral para atualizar o estado oculto em um vanilla RNN é:\n",
    "$$ h^{<t>} = g(W_h [h^{<t-1>}, x^{<t>}] + b_h) $$\n",
    "\n",
    "Esta fórmula pode ser detalhada em componentes individuais:\n",
    "$$ h^{<t>} = g(W_{hh} h^{<t-1>} + W_{hx} x^{<t>} + b_h) $$\n",
    "\n",
    "Aqui:\n",
    "- $ W_{hh} h^{<t-1>} $: Multiplicação dos pesos $ W_{hh} $ pelo estado oculto anterior $ h^{<t-1>} $.\n",
    "- $ W_{hx} x^{<t>} $: Multiplicação dos pesos $ W_{hx} $ pela entrada atual $ x^{<t>} $.\n",
    "- $ b_h $: Adição do bias $ b_h $.\n",
    "\n",
    "### Concatenando Entradas e Estados Ocultos\n",
    "\n",
    "Para simplificar a notação, podemos concatenar o estado oculto anterior e a entrada atual em um único vetor:\n",
    "$$ h^{<t>} = g(W_h [h^{<t-1>}, x^{<t>}] + b_h) $$\n",
    "\n",
    "Isso pode ser reescrito como:\n",
    "$$ h^{<t>} = g(W_{hh} h^{<t-1>} \\oplus W_{hx} x^{<t>} + b_h) $$\n",
    "\n",
    "Aqui, $\\oplus$ indica a concatenação dos produtos $ W_{hh} h^{<t-1>} $ e $ W_{hx} x^{<t>} $.\n",
    "\n",
    "<img src=\"./imgs/rnn3.png\">\n",
    "\n",
    "### Previsão em Cada Passo de Tempo\n",
    "\n",
    "Para gerar uma previsão ou saída em cada passo de tempo, usamos o estado oculto atualizado:\n",
    "$$ \\hat{y}^{<t>} = g(W_{yh} h^{<t>} + b_y) $$\n",
    "\n",
    "Aqui:\n",
    "- $ W_{yh} h^{<t>} $: Multiplicação dos pesos $ W_{yh} $ pelo estado oculto atual $ h^{<t>} $.\n",
    "- $ b_y $: Adição do bias $ b_y $.\n",
    "\n",
    "### Treinamento\n",
    "\n",
    "Durante o treinamento da RNN, ajustamos os seguintes parâmetros:\n",
    "- $ W_{hh} $: Pesos conectando estados ocultos.\n",
    "- $ W_{hx} $: Pesos conectando entradas às unidades ocultas.\n",
    "- $ W_{yh} $: Pesos conectando estados ocultos às saídas.\n",
    "- $ b_h $: Bias das unidades ocultas.\n",
    "- $ b_y $: Bias das saídas.\n",
    "\n",
    "<img src=\"./imgs/rnn4.png\">\n",
    "\n",
    "### Visualização do Modelo\n",
    "\n",
    "Aqui está uma visualização simplificada do modelo vanilla RNN:\n",
    "\n",
    "```\n",
    "x^{<1>} ---> [ Unidade RNN ] ---> h^{<1>}\n",
    "           |                |\n",
    "x^{<2>} ---> [ Unidade RNN ] ---> h^{<2>}\n",
    "           |                |\n",
    "x^{<3>} ---> [ Unidade RNN ] ---> h^{<3>}\n",
    "            ...\n",
    "```\n",
    "\n",
    "Para cada passo de tempo $ t $:\n",
    "- A entrada $ x^{<t>} $ e o estado oculto anterior $ h^{<t-1>} $ são usados para calcular o novo estado oculto $ h^{<t>} $.\n",
    "- O novo estado oculto $ h^{<t>} $ é usado para gerar a saída $ \\hat{y}^{<t>} $.\n",
    "\n",
    "Este processo continua ao longo de toda a sequência, permitindo que a RNN capture dependências temporais nos dados.\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- **Estado Oculto:** Memória que armazena informações sobre a sequência até o ponto atual.\n",
    "- **Função de Ativação:** Introduz não-linearidade, permitindo que a RNN capture relações complexas.\n",
    "- **Pesos e Biases:** Parâmetros treináveis que conectam entradas, estados ocultos e saídas.\n",
    "- **Atualização Recorrente:** O estado oculto é atualizado a cada passo de tempo com base na entrada atual e no estado oculto anterior.\n",
    "- **Previsão:** As saídas são geradas com base no estado oculto atualizado em cada passo de tempo.\n",
    "\n",
    "As vanilla RNNs são poderosas para capturar dependências em dados sequenciais, mas têm limitações, como o desvanecimento de gradiente, que são abordadas em variantes mais avançadas como LSTMs e GRUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c5195d",
   "metadata": {},
   "source": [
    "Uma Rede Neural Recorrente (RNN) \"vanilla\" é uma arquitetura básica de RNN que processa sequências de dados, um elemento por vez, mantendo um estado oculto que armazena informações sobre elementos anteriores da sequência. Isso permite que o modelo capture dependências temporais ou sequenciais nos dados. Vamos explorar o funcionamento de um vanilla RNN de forma aprofundada e intuitiva.\n",
    "\n",
    "### Estrutura de uma Vanilla RNN\n",
    "\n",
    "1. **Entrada:** \n",
    "   - Uma sequência de dados, como uma frase, onde cada palavra é convertida em uma representação numérica, por exemplo, um vetor de embedding.\n",
    "\n",
    "2. **Estado Oculto (Hidden State):**\n",
    "   - Um vetor que armazena informações sobre a sequência até o ponto atual. Este vetor é atualizado em cada passo da sequência.\n",
    "\n",
    "3. **Unidade Recurrente:**\n",
    "   - A função que define como o estado oculto é atualizado em cada passo da sequência.\n",
    "\n",
    "4. **Saída:**\n",
    "   - Uma previsão ou transformação do estado oculto, que pode ser usada para diversas tarefas, como classificação de texto ou previsão da próxima palavra na sequência.\n",
    "\n",
    "### Funcionamento Passo a Passo\n",
    "\n",
    "#### Passo 1: Inicialização\n",
    "- O estado oculto inicial $\\mathbf{h}_0$ é geralmente um vetor de zeros ou pode ser inicializado aleatoriamente.\n",
    "\n",
    "#### Passo 2: Processamento da Sequência\n",
    "Para cada elemento $x_t$ na sequência de entrada ($x_1, x_2, \\ldots, x_T$):\n",
    "1. **Entrada Atual ($x_t$):** O vetor de entrada no tempo $t$.\n",
    "2. **Estado Oculto Anterior ($\\mathbf{h}_{t-1}$):** O estado oculto do passo anterior.\n",
    "\n",
    "3. **Atualização do Estado Oculto:**\n",
    "   - O estado oculto atual $\\mathbf{h}_t$ é calculado usando a unidade recurrente.\n",
    "   - A fórmula típica é:\n",
    "     $$\n",
    "     \\mathbf{h}_t = \\tanh(\\mathbf{W}_{xh} \\cdot x_t + \\mathbf{W}_{hh} \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_h)\n",
    "     $$\n",
    "     - $\\mathbf{W}_{xh}$: Matriz de pesos que conecta a entrada ao estado oculto.\n",
    "     - $\\mathbf{W}_{hh}$: Matriz de pesos que conecta o estado oculto anterior ao novo estado oculto.\n",
    "     - $\\mathbf{b}_h$: Vetor de bias.\n",
    "     - $\\tanh$: Função de ativação não-linear (tangente hiperbólica) que ajuda a manter os valores do estado oculto dentro de um intervalo fixo.\n",
    "\n",
    "4. **Saída do Passo Atual:**\n",
    "   - A saída do modelo no tempo $t$ ($y_t$) pode ser calculada a partir do estado oculto atual.\n",
    "     $$\n",
    "     y_t = \\mathbf{W}_{hy} \\cdot \\mathbf{h}_t + \\mathbf{b}_y\n",
    "     $$\n",
    "     - $\\mathbf{W}_{hy}$: Matriz de pesos que conecta o estado oculto à saída.\n",
    "     - $\\mathbf{b}_y$: Vetor de bias para a saída.\n",
    "\n",
    "#### Passo 3: Processamento da Sequência Completa\n",
    "- Este processo é repetido para cada elemento da sequência, atualizando o estado oculto em cada passo e gerando uma saída correspondente.\n",
    "\n",
    "### Intuição sobre o Estado Oculto\n",
    "- O estado oculto pode ser visto como a \"memória\" da RNN. Ele carrega informações sobre elementos anteriores da sequência e influencia como os próximos elementos são processados.\n",
    "- Em cada passo, o estado oculto é uma combinação linear da entrada atual e do estado oculto anterior, transformado por uma função não-linear (tanh). Isso permite que o modelo capture tanto informações novas quanto contextuais.\n",
    "\n",
    "### Limitações das Vanilla RNNs\n",
    "- **Desvanecimento e Explosão do Gradiente:** Durante o treinamento, os gradientes podem se tornar extremamente pequenos (desvanecimento) ou extremamente grandes (explosão), dificultando o aprendizado de dependências de longo prazo.\n",
    "- **Dependências de Longo Prazo:** Vanilla RNNs podem ter dificuldades para capturar dependências que ocorrem em grandes intervalos de tempo dentro da sequência.\n",
    "\n",
    "### Conclusão\n",
    "Vanilla RNNs são modelos poderosos para processar dados sequenciais, mantendo uma memória dinâmica da sequência através do estado oculto. No entanto, suas limitações levaram ao desenvolvimento de variantes mais sofisticadas, como LSTMs e GRUs, que abordam problemas como o desvanecimento do gradiente e melhoram a capacidade de capturar dependências de longo prazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14135b7d",
   "metadata": {},
   "source": [
    "## Cost Function for RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae5cabb",
   "metadata": {},
   "source": [
    "A função de custo utilizada nas RNNs é a **cross entropy loss**\n",
    "\n",
    "<img src=\"./imgs/cross_entropy_loss.png\">\n",
    "\n",
    "### Entropia Cruzada\n",
    "\n",
    "A perda por entropia cruzada mede a diferença entre a distribuição de probabilidade verdadeira (ou alvo) e a distribuição de probabilidade prevista pelo modelo. Para cada classe $ j $ em uma tarefa de classificação com $ K $ classes, a entropia cruzada é calculada como:\n",
    "\n",
    "$$ \\text{Loss} = - \\sum_{j=1}^{K} y_j \\log(\\hat{y}_j) $$\n",
    "\n",
    "Aqui:\n",
    "- $ y_j $ é o valor real da classe $ j $ (1 se a classe é a correta, 0 caso contrário).\n",
    "- $ \\hat{y}_j $ é a probabilidade prevista pelo modelo para a classe $ j $.\n",
    "\n",
    "### Função de Custo em RNNs\n",
    "\n",
    "Quando se trabalha com RNNs, estamos lidando com sequências de dados, e a previsão é feita em cada passo de tempo $ t $. Portanto, precisamos calcular a perda para cada passo de tempo e depois somá-las para obter a perda total da sequência.\n",
    "\n",
    "A fórmula geral para a função de custo $ J $ ao longo de $ T $ passos de tempo é:\n",
    "\n",
    "$$ J = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{j=1}^{K} y_j^{<t>} \\log(\\hat{y}_j^{<t>}) $$\n",
    "\n",
    "Aqui:\n",
    "- $ T $ é o número total de passos de tempo na sequência.\n",
    "- $ K $ é o número total de classes.\n",
    "- $ y_j^{<t>} $ é o valor real para a classe $ j $ no tempo $ t $.\n",
    "- $ \\hat{y}_j^{<t>} $ é a probabilidade prevista pelo modelo para a classe $ j $ no tempo $ t $.\n",
    "\n",
    "### Intuição\n",
    "\n",
    "1. **Soma sobre as Classes:** Para cada passo de tempo $ t $, calculamos a entropia cruzada para todas as classes $ j $. Isso mede o quão bem o modelo previu a distribuição de probabilidade para o passo de tempo específico.\n",
    "\n",
    "2. **Soma sobre os Passos de Tempo:** Depois, somamos a perda de todos os passos de tempo. Isso nos dá a perda total para toda a sequência.\n",
    "\n",
    "3. **Média ao Longo do Tempo:** Dividimos pela quantidade total de passos de tempo $ T $ para obter a média da perda por passo de tempo. Isso é importante porque nos dá uma noção da performance do modelo em média, ao longo de toda a sequência.\n",
    "\n",
    "### Visualização\n",
    "\n",
    "Imagine que temos uma sequência com 3 passos de tempo (T = 3) e 4 classes (K = 4). Para cada passo de tempo $ t $, temos as previsões do modelo $\\hat{y}^{<t>}$ e os valores reais $ y^{<t>} $.\n",
    "\n",
    "1. Para cada $ t $ (passo de tempo):\n",
    "   - Calculamos a entropia cruzada entre $\\hat{y}^{<t>}$ e $ y^{<t>} $.\n",
    "   - Fazemos isso somando $ y_j^{<t>} \\log(\\hat{y}_j^{<t>}) $ para todas as classes $ j $.\n",
    "\n",
    "2. Somamos essas perdas para todos os passos de tempo $ t = 1, 2, 3 $.\n",
    "\n",
    "3. Dividimos a soma total pelo número de passos de tempo $ T $ para obter a média.\n",
    "\n",
    "### Exemplo de Cálculo\n",
    "\n",
    "Suponha que temos as seguintes probabilidades previstas $\\hat{y}$ e valores reais $ y $ para uma sequência de 3 passos de tempo e 2 classes:\n",
    "\n",
    "- Tempo 1: $ \\hat{y}^{<1>} = [0.7, 0.3] $, $ y^{<1>} = [1, 0] $\n",
    "- Tempo 2: $ \\hat{y}^{<2>} = [0.4, 0.6] $, $ y^{<2>} = [0, 1] $\n",
    "- Tempo 3: $ \\hat{y}^{<3>} = [0.9, 0.1] $, $ y^{<3>} = [1, 0] $\n",
    "\n",
    "Para cada passo de tempo, calculamos a entropia cruzada:\n",
    "\n",
    "- Tempo 1: $ \\text{Loss}^{<1>} = -(1 \\log(0.7) + 0 \\log(0.3)) = 0.3567 $\n",
    "- Tempo 2: $ \\text{Loss}^{<2>} = -(0 \\log(0.4) + 1 \\log(0.6)) = 0.5108 $\n",
    "- Tempo 3: $ \\text{Loss}^{<3>} = -(1 \\log(0.9) + 0 \\log(0.1)) = 0.1054 $\n",
    "\n",
    "Somamos as perdas:\n",
    "\n",
    "$$ \\text{Total Loss} = 0.3567 + 0.5108 + 0.1054 = 0.9729 $$\n",
    "\n",
    "Calculamos a média:\n",
    "\n",
    "$$ J = \\frac{\\text{Total Loss}}{T} = \\frac{0.9729}{3} = 0.3243 $$\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- A função de custo em RNNs é a perda por entropia cruzada calculada para cada passo de tempo.\n",
    "- A entropia cruzada mede a diferença entre as distribuições de probabilidade previstas e reais.\n",
    "- A perda total é a soma das perdas em cada passo de tempo, dividida pelo número total de passos, resultando na média da perda por passo de tempo.\n",
    "- Esse processo permite que o modelo aprenda a prever melhor as sequências de dados ao longo do tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231e991d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:02:03.350599Z",
     "start_time": "2024-07-01T22:02:03.264743Z"
    }
   },
   "source": [
    "## Implementation Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed850d31",
   "metadata": {},
   "source": [
    "A função scan (abstração do rnn) é construída da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/rnn_implementation.png\">\n",
    "\n",
    "Observe que isso é basicamente o que um RNN está fazendo. Ele pega o inicializador e retorna uma lista de saídas (ys) e usa o valor atual para obter o próximo y e o próximo valor atual. Esse tipo de abstração permite uma computação muito mais rápida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023fe67",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ae3b44",
   "metadata": {},
   "source": [
    "As **unidades recorrentes com portas (GRUs)** são uma variante dos RNNs que introduzem mecanismos adicionais, chamados de portas, para melhor gerenciar o fluxo de informações ao longo do tempo. As GRUs possuem duas portas principais: a **porta de atualização (update)** e a **porta de relevância (relevance ou reset)**. Estas portas ajudam a **determinar o que deve ser lembrado ou esquecido no estado oculto**, permitindo que o modelo mantenha informações relevantes por longos períodos e melhore a captura de dependências de longo prazo.\n",
    "\n",
    "\n",
    "<img src=\"./imgs/vanilla_rnn_vs_gru.png\">\n",
    "\n",
    "\n",
    "### Estrutura das GRUs\n",
    "\n",
    "1. **Porta de Atualização ($\\Gamma_u$)**:\n",
    "   - Determina quanto do estado oculto anterior deve ser levado para frente.\n",
    "   - Calculada como:\n",
    "     $$\n",
    "     \\Gamma_u = \\sigma(W_u [h^{<t-1>}, x^{<t>}] + b_u)\n",
    "     $$\n",
    "   - Aqui, $\\sigma$ é a função sigmoid que gera valores entre 0 e 1.\n",
    "\n",
    "2. **Porta de Relevância ($\\Gamma_r$)**:\n",
    "   - Decide quanta parte do estado oculto anterior deve ser esquecida (ou reiniciada).\n",
    "   - Calculada como:\n",
    "     $$\n",
    "     \\Gamma_r = \\sigma(W_r [h^{<t-1>}, x^{<t>}] + b_r)\n",
    "     $$\n",
    "   - A função sigmoid também é usada aqui para limitar os valores entre 0 e 1.\n",
    "\n",
    "3. **Novo Estado Candidato ($h'^{<t>}$)**:\n",
    "   - É a nova memória candidata que poderia ser adicionada ao estado oculto.\n",
    "   - Calculado como:\n",
    "     $$\n",
    "     h'^{<t>} = \\tanh(W_h [\\Gamma_r * h^{<t-1>}, x^{<t>}] + b_h)\n",
    "     $$\n",
    "   - Aqui, a função tangente hiperbólica ($\\tanh$) é usada para permitir valores entre -1 e 1, e $\\Gamma_r * h^{<t-1>}$ indica a multiplicação elemento a elemento entre a porta de relevância e o estado oculto anterior.\n",
    "\n",
    "4. **Estado Oculto Atualizado ($h^{<t>}$)**:\n",
    "   - Calculado combinando o estado oculto anterior e o novo estado candidato, ponderados pela porta de atualização.\n",
    "   - A fórmula é:\n",
    "     $$\n",
    "     h^{<t>} = (1 - \\Gamma_u) * h^{<t-1>} + \\Gamma_u * h'^{<t>}\n",
    "     $$\n",
    "   - Isso significa que o novo estado oculto é uma média ponderada entre o estado oculto anterior (controlado por $1 - \\Gamma_u$) e o novo estado candidato (controlado por $\\Gamma_u$).\n",
    "\n",
    "### Intuição\n",
    "\n",
    "1. **Porta de Atualização ($\\Gamma_u$)**:\n",
    "   - Se $\\Gamma_u$ está perto de 1, o modelo atualiza fortemente o estado oculto com o novo estado candidato.\n",
    "   - Se $\\Gamma_u$ está perto de 0, o modelo mantém mais do estado oculto anterior.\n",
    "\n",
    "2. **Porta de Relevância ($\\Gamma_r$)**:\n",
    "   - Se $\\Gamma_r$ está perto de 1, o modelo usa completamente o estado oculto anterior para calcular o novo estado candidato.\n",
    "   - Se $\\Gamma_r$ está perto de 0, o modelo ignora o estado oculto anterior e reinicia (ou reseta) a memória.\n",
    "\n",
    "### Vantagens das GRUs\n",
    "\n",
    "1. **Captura de Dependências Longas**: As GRUs ajudam a resolver o problema do desvanecimento do gradiente, permitindo que informações importantes sejam mantidas por longos períodos.\n",
    "2. **Eficiência Computacional**: As GRUs têm menos parâmetros comparados às LSTMs, o que pode resultar em treinamento mais rápido e inferência mais eficiente.\n",
    "3. **Simplicidade**: Embora sejam mais complexas que as vanilla RNNs, as GRUs são mais simples que as LSTMs, pois não possuem uma porta de saída.\n",
    "\n",
    "### Fórmulas Resumidas\n",
    "\n",
    "1. **Porta de Atualização**:\n",
    "   $$\n",
    "   \\Gamma_u = \\sigma(W_u [h^{<t-1>}, x^{<t>}] + b_u)\n",
    "   $$\n",
    "\n",
    "2. **Porta de Relevância**:\n",
    "   $$\n",
    "   \\Gamma_r = \\sigma(W_r [h^{<t-1>}, x^{<t>}] + b_r)\n",
    "   $$\n",
    "\n",
    "3. **Novo Estado Candidato**:\n",
    "   $$\n",
    "   h'^{<t>} = \\tanh(W_h [\\Gamma_r * h^{<t-1>}, x^{<t>}] + b_h)\n",
    "   $$\n",
    "\n",
    "4. **Estado Oculto Atualizado**:\n",
    "   $$\n",
    "   h^{<t>} = (1 - \\Gamma_u) * h^{<t-1>} + \\Gamma_u * h'^{<t>}\n",
    "   $$\n",
    "\n",
    "As GRUs são uma poderosa variante das RNNs, projetadas para capturar dependências de longo prazo de maneira mais eficaz, utilizando mecanismos de porta para controlar o fluxo de informações. Elas equilibram simplicidade e eficiência computacional, tornando-as uma escolha popular para muitas tarefas de NLP e séries temporais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517206c8",
   "metadata": {},
   "source": [
    "## Deep and Bi-directional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270b1882",
   "metadata": {},
   "source": [
    "**Redes Neurais Recorrentes Bidirecionais (Bi-directional RNNs, ou BRNNs)** são uma extensão das RNNs tradicionais que permitem que a informação flua em ambas as direções: do passado para o futuro e do futuro para o passado. Essa arquitetura é particularmente útil em tarefas de processamento de linguagem natural (NLP) onde o contexto futuro pode ser tão importante quanto o contexto passado para a compreensão da sequência atual.\n",
    "\n",
    "<img src=\"./imgs/birnn1.png\">\n",
    "\n",
    "### Estrutura das BRNNs\n",
    "\n",
    "A principal diferença entre uma RNN tradicional e uma BRNN é que a BRNN possui duas redes recorrentes:\n",
    "1. **Forward RNN**: Processa a sequência da esquerda para a direita.\n",
    "2. **Backward RNN**: Processa a sequência da direita para a esquerda.\n",
    "\n",
    "#### Funcionamento\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   - A RNN direta (forward) processa a sequência de entrada $ x_1, x_2, ..., x_T $ e produz uma sequência de estados ocultos $ \\overrightarrow{h_1}, \\overrightarrow{h_2}, ..., \\overrightarrow{h_T} $.\n",
    "\n",
    "2. **Backward Pass**:\n",
    "   - A RNN reversa (backward) processa a sequência de entrada na ordem inversa $ x_T, x_{T-1}, ..., x_1 $ e produz uma sequência de estados ocultos $ \\overleftarrow{h_T}, \\overleftarrow{h_{T-1}}, ..., \\overleftarrow{h_1} $.\n",
    "\n",
    "3. **Combinação**: Em cada passo de tempo $ t $, os estados ocultos das duas redes são combinados. A combinação pode ser feita de várias maneiras, como **concatenação ou soma**. Normalmente, usa-se a concatenação:\n",
    "     $$\n",
    "     h_t = [\\overrightarrow{h_t}, \\overleftarrow{h_t}]\n",
    "     $$\n",
    "\n",
    "4. **Previsão**: A saída final em cada passo de tempo $ y_t $ é gerada usando a combinação dos estados ocultos bidirecionais:\n",
    "     $$\n",
    "     y_t = g(W_y h_t + b_y)\n",
    "     $$\n",
    "     \n",
    "     ou\n",
    "     \n",
    "     $$\n",
    "     y_t = g(W_y [\\overrightarrow{h_t}, \\overleftarrow{h_t}] + b_y)\n",
    "     $$\n",
    "     \n",
    "   - Onde $ g $ é uma função de ativação apropriada (por exemplo, softmax para tarefas de classificação).\n",
    "\n",
    "### Intuição\n",
    "\n",
    "A ideia principal por trás das BRNNs é fornecer ao modelo acesso ao contexto completo em torno de cada ponto da sequência. Por exemplo, ao processar uma palavra em uma frase, uma BRNN pode considerar tanto as palavras anteriores quanto as palavras subsequentes, o que pode ser crucial para desambiguação e melhor compreensão do contexto.\n",
    "\n",
    "### Fórmulas e Arquitetura\n",
    "\n",
    "Vamos detalhar as fórmulas e a arquitetura de uma BRNN.\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   $$\n",
    "   \\overrightarrow{h_t} = g(W_x x_t + W_{\\overrightarrow{h}} \\overrightarrow{h_{t-1}} + b_{\\overrightarrow{h}})\n",
    "   $$\n",
    "\n",
    "2. **Backward Pass**:\n",
    "   $$\n",
    "   \\overleftarrow{h_t} = g(W_x x_t + W_{\\overleftarrow{h}} \\overleftarrow{h_{t+1}} + b_{\\overleftarrow{h}})\n",
    "   $$\n",
    "\n",
    "3. **Combinação dos Estados Ocultos**:\n",
    "   $$\n",
    "   h_t = [\\overrightarrow{h_t}, \\overleftarrow{h_t}]\n",
    "   $$\n",
    "\n",
    "4. **Previsão da Saída**:\n",
    "   $$\n",
    "   y_t = g(W_y h_t + b_y)\n",
    "   $$\n",
    "\n",
    "### Vantagens das BRNNs\n",
    "\n",
    "1. **Acesso Completo ao Contexto**: A capacidade de considerar tanto o contexto anterior quanto o futuro permite que as BRNNs façam previsões mais informadas e precisas.\n",
    "\n",
    "2. **Melhor Desempenho em Tarefas de NLP**: BRNNs são especialmente eficazes em tarefas onde o contexto completo é crucial, como tradução automática, reconhecimento de fala e análise de sentimentos.\n",
    "\n",
    "### Exemplos de Aplicações\n",
    "\n",
    "1. **Tradução Automática**: Considerar palavras futuras pode ajudar a determinar a melhor tradução para palavras ambíguas.\n",
    "\n",
    "2. **Reconhecimento de Fala**: O contexto futuro pode ajudar a desambiguar palavras que soam semelhantes.\n",
    "\n",
    "3. **Análise de Sentimentos**: A compreensão do sentimento de uma frase pode depender de palavras tanto no início quanto no final da frase.\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- **BRNNs** são uma extensão das RNNs que processam a entrada em duas direções: forward e backward.\n",
    "- **Combinação dos Estados Ocultos**: Os estados ocultos das direções forward e backward são combinados para fornecer um contexto mais completo.\n",
    "- **Vantagens**: Acesso ao contexto completo ao redor de cada ponto da sequência, resultando em previsões mais precisas em muitas tarefas de NLP.\n",
    "\n",
    "As BRNNs representam uma melhoria significativa em relação às RNNs unidirecionais, especialmente em cenários onde o contexto completo da sequência é crítico para a tarefa em questão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a39181",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T21:44:55.399940Z",
     "start_time": "2024-07-04T21:44:55.394695Z"
    }
   },
   "source": [
    "**Redes Neurais Recorrentes Profundas (Deep RNNs)** são uma extensão das RNNs tradicionais que aumentam a profundidade da rede, **empilhando várias camadas** de unidades recorrentes umas sobre as outras. Isso permite que o modelo capture representações mais complexas e aprenda características hierárquicas dos dados sequenciais.\n",
    "\n",
    "A estrutura de uma Deep RNN consiste em várias camadas recorrentes (RNNs, LSTMs, GRUs, etc.) **empilhadas verticalmente**. Cada camada da rede **recebe a saída da camada anterior** como entrada, processa essa entrada e passa a saída para a próxima camada.\n",
    "\n",
    "<img src=\"./imgs/birnn2.png\">\n",
    "\n",
    "#### Funcionamento\n",
    "\n",
    "1. **Primeira Camada Recorrente**:\n",
    "   - Recebe a sequência de entrada original.\n",
    "   - Processa a entrada sequencialmente para produzir uma sequência de estados ocultos.\n",
    "\n",
    "2. **Camadas Recorrentes Subsequentes**:\n",
    "   - Cada camada seguinte recebe a sequência de estados ocultos da camada anterior como entrada.\n",
    "   - Processa essa entrada sequencialmente para produzir uma nova sequência de estados ocultos.\n",
    "   - Isso continua até a camada final.\n",
    "\n",
    "3. **Camada de Saída**:\n",
    "   - A camada de saída recebe a sequência de estados ocultos da última camada recorrente.\n",
    "   - Gera a previsão ou a saída desejada para cada passo de tempo.\n",
    "\n",
    "### Fórmulas e Arquitetura\n",
    "\n",
    "Vamos considerar uma Deep RNN com duas camadas de unidades recorrentes para simplificação:\n",
    "\n",
    "1. **Primeira Camada Recorrente**:\n",
    "   - Para cada passo de tempo $ t $:\n",
    "     $$\n",
    "     h^{(1)}_t = g(W^{(1)}_x x_t + W^{(1)}_h h^{(1)}_{t-1} + b^{(1)})\n",
    "     $$\n",
    "   - Onde $ h^{(1)}_t $ é o estado oculto da primeira camada no passo de tempo $ t $.\n",
    "\n",
    "2. **Segunda Camada Recorrente**:\n",
    "   - Recebe $ h^{(1)} $ como entrada:\n",
    "     $$\n",
    "     h^{(2)}_t = g(W^{(2)}_{h^{(1)}} h^{(1)}_t + W^{(2)}_h h^{(2)}_{t-1} + b^{(2)})\n",
    "     $$\n",
    "   - Onde $ h^{(2)}_t $ é o estado oculto da segunda camada no passo de tempo $ t $.\n",
    "\n",
    "3. **Camada de Saída**:\n",
    "   - Recebe $ h^{(2)}_t $ e gera a saída $ y_t $:\n",
    "     $$\n",
    "     y_t = g(W_y h^{(2)}_t + b_y)\n",
    "     $$\n",
    "\n",
    "### Intuição\n",
    "\n",
    "- **Primeira Camada**: Extrai características básicas da sequência de entrada.\n",
    "- **Camadas Intermediárias**: Capturam representações mais abstratas e complexas da sequência, permitindo que a rede aprenda dependências de longo alcance e padrões hierárquicos.\n",
    "- **Camada de Saída**: Combina todas as características aprendidas para produzir a previsão final.\n",
    "\n",
    "### Vantagens das Deep RNNs\n",
    "\n",
    "1. **Captura de Padrões Complexos**: A profundidade adicional permite que a rede capture padrões mais complexos e hierárquicos nos dados.\n",
    "2. **Aprendizado de Representações Hierárquicas**: Cada camada pode aprender diferentes níveis de abstração, melhorando a capacidade do modelo de generalizar.\n",
    "3. **Melhor Desempenho em Tarefas Complexas**: Deep RNNs são mais eficazes em tarefas que exigem a captura de dependências de longo prazo e a compreensão de contextos complexos.\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- **Deep RNNs** são RNNs com várias camadas empilhadas, permitindo a captura de representações mais complexas e hierárquicas.\n",
    "- **Funcionamento**: Cada camada recorrente processa a saída da camada anterior, passando a sequência de estados ocultos para a próxima camada.\n",
    "- **Vantagens**: Captura de padrões complexos, aprendizado de representações hierárquicas e melhor desempenho em tarefas complexas.\n",
    "\n",
    "Deep RNNs são uma poderosa extensão das RNNs tradicionais, oferecendo melhorias significativas em termos de capacidade de modelagem e desempenho em tarefas de processamento de linguagem natural e outras aplicações sequenciais complexas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e45a8b",
   "metadata": {},
   "source": [
    "## Calculating Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee37c0f5",
   "metadata": {},
   "source": [
    "A perplexidade é uma métrica comum usada para avaliar modelos de linguagem, especialmente em tarefas de modelagem de linguagem e previsão de sequência. Ela mede quão bem um modelo de linguagem prevê uma sequência de palavras. Uma perplexidade mais baixa indica que o modelo está melhor em prever a sequência.\n",
    "\n",
    "Perplexidade pode ser entendida como a medida de **\"surpresa\"** que o modelo experimenta ao prever a próxima palavra em uma sequência. Em outras palavras, **é uma medida de quão incerto o modelo está sobre a próxima palavra**.\n",
    "\n",
    "Para calcular a perplexidade, usamos a probabilidade logarítmica da sequência de teste. Aqui estão os passos detalhados:\n",
    "\n",
    "1. **Probabilidade Logarítmica**:\n",
    "   - Suponha que temos uma sequência de palavras $ w_1, w_2, ..., w_N $.\n",
    "   - A probabilidade do modelo prever a sequência é $ P(w_1, w_2, ..., w_N) $.\n",
    "\n",
    "2. **Log Probabilidade Média**:\n",
    "   - A probabilidade logarítmica média é calculada como:\n",
    "     $$\n",
    "     \\frac{1}{N} \\sum_{t=1}^N \\log P(w_t | w_1, w_2, ..., w_{t-1})\n",
    "     $$\n",
    "   - Aqui, $ P(w_t | w_1, w_2, ..., w_{t-1}) $ é a probabilidade que o modelo atribui à palavra $ w_t $ dado o histórico $ w_1, w_2, ..., w_{t-1} $.\n",
    "\n",
    "3. **Perplexidade**:\n",
    "   - A perplexidade é então definida como a exponenciação negativa da log probabilidade média:\n",
    "     $$\n",
    "     \\text{Perplexity} = \\exp \\left( - \\frac{1}{N} \\sum_{t=1}^N \\log P(w_t | w_1, w_2, ..., w_{t-1}) \\right)\n",
    "     $$\n",
    "   - Alternativamente, pode-se calcular a perplexidade diretamente da probabilidade da sequência:\n",
    "     $$\n",
    "     \\text{Perplexity} = P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}}\n",
    "     $$\n",
    "\n",
    "Vamos ilustrar o cálculo da perplexidade com um exemplo simples:\n",
    "\n",
    "- Suponha que temos uma sequência de três palavras $ w_1, w_2, w_3 $.\n",
    "- As probabilidades previstas pelo modelo são:\n",
    "  - $ P(w_1) = 0.1 $\n",
    "  - $ P(w_2 | w_1) = 0.4 $\n",
    "  - $ P(w_3 | w_1, w_2) = 0.3 $\n",
    "\n",
    "1. **Probabilidade da sequência**:\n",
    "   $$\n",
    "   P(w_1, w_2, w_3) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_1, w_2) = 0.1 \\times 0.4 \\times 0.3 = 0.012\n",
    "   $$\n",
    "\n",
    "2. **Log Probabilidade Média**:\n",
    "   $$\n",
    "   \\frac{1}{3} \\left( \\log 0.1 + \\log 0.4 + \\log 0.3 \\right) = \\frac{1}{3} \\left( -2.3026 + -0.9163 + -1.2040 \\right) = \\frac{1}{3} \\left( -4.4229 \\right) = -1.4743\n",
    "   $$\n",
    "\n",
    "3. **Perplexidade**:\n",
    "   $$\n",
    "   \\text{Perplexity} = \\exp(-1.4743) \\approx 4.37\n",
    "   $$\n",
    "\n",
    "A métrica deve ser interpreta como:\n",
    "\n",
    "- **Perplexidade Alta**: Indica que o modelo tem baixa confiança em suas previsões (é \"muito surpreso\").\n",
    "- **Perplexidade Baixa**: Indica que o modelo está mais confiante em suas previsões (é \"menos surpreso\").\n",
    "\n",
    "Por fim:\n",
    "\n",
    "- **Perplexidade** mede a qualidade de um modelo de linguagem.\n",
    "- É calculada usando a probabilidade logarítmica da sequência de teste.\n",
    "- Uma perplexidade mais baixa indica um modelo de linguagem mais preciso.\n",
    "\n",
    "A perplexidade é uma métrica crucial para avaliar modelos de linguagem, ajudando a comparar diferentes modelos e a melhorar suas previsões."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a639817",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abc5535",
   "metadata": {},
   "source": [
    "## RNNs and Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0ebf56",
   "metadata": {},
   "source": [
    "### Vanishing e Exploding Gradients em RNNs\n",
    "\n",
    "Redes Neurais Recorrentes (RNNs) são particularmente suscetíveis aos problemas de vanishing (desaparecimento) e exploding (explosão) de gradientes devido à natureza recursiva de suas operações durante o treinamento. Esses problemas afetam a capacidade da rede de aprender dependências de longo prazo.\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "O problema de vanishing gradients ocorre quando os gradientes das funções de perda em relação aos pesos da rede se tornam extremamente pequenos. Isso impede a atualização efetiva dos pesos durante o treinamento, resultando em um aprendizado muito lento ou em uma paralisação completa do aprendizado.\n",
    "\n",
    "Durante o backpropagation através do tempo (BPTT), os gradientes são propagados de volta através de muitas camadas (uma por cada passo de tempo). Se as derivadas das funções de ativação ou dos pesos são menores que 1, os gradientes podem diminuir exponencialmente a cada passo de tempo:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} \\approx \\left( \\prod_{t=1}^T \\frac{\\partial h_t}{\\partial h_{t-1}} \\right) \\frac{\\partial L}{\\partial h_T}\n",
    "$$\n",
    "\n",
    "Se $ \\frac{\\partial h_t}{\\partial h_{t-1}} $ é menor que 1, **os gradientes multiplicados sucessivamente tornam-se muito pequenos**.\n",
    "\n",
    "Consequências:\n",
    "- **Dependências de Longo Prazo**: A rede tem dificuldade em aprender dependências de longo prazo, pois as informações dos passos de tempo anteriores são \"esquecidas\" rapidamente.\n",
    "- **Treinamento Ineficaz**: O aprendizado se torna muito lento ou estagna.\n",
    "\n",
    "### Exploding Gradients\n",
    "\n",
    "O problema de exploding gradients ocorre quando os gradientes se tornam extremamente grandes, causando atualizações instáveis e grandes nos pesos da rede. Isso pode resultar em números muito grandes ou NaNs durante o treinamento.\n",
    "\n",
    "Se as derivadas das funções de ativação ou dos pesos são maiores que 1, os gradientes podem aumentar exponencialmente a cada passo de tempo:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} \\approx \\left( \\prod_{t=1}^T \\frac{\\partial h_t}{\\partial h_{t-1}} \\right) \\frac{\\partial L}{\\partial h_T}\n",
    "$$\n",
    "\n",
    "Se $ \\frac{\\partial h_t}{\\partial h_{t-1}} $ é maior que 1, **os gradientes multiplicados sucessivamente tornam-se muito grandes**.\n",
    "\n",
    "Consequências:\n",
    "- **Instabilidade no Treinamento**: Pesos muito grandes podem causar oscilações na função de perda, dificultando a convergência.\n",
    "- **Numéricos NaNs**: Gradientes muito grandes podem causar overflow, resultando em NaNs e falha no treinamento.\n",
    "\n",
    "### Técnicas para Mitigar Vanishing e Exploding Gradients\n",
    "\n",
    "1. **Inicialização Adequada dos Pesos**: Usar técnicas de inicialização como Xavier ou He que são projetadas para manter os gradientes em uma faixa adequada.\n",
    "\n",
    "2. **Funções de Ativação Adequadas**: Usar funções de ativação como ReLU, que ajudam a mitigar o problema de vanishing gradients em comparação com funções como sigmoid ou tanh.\n",
    "\n",
    "3. **Normalização dos Gradientes**: Gradient Clipping, ou seja, limitar o valor máximo dos gradientes durante o backpropagation para evitar exploding gradients:\n",
    "  $$\n",
    "  \\text{if } ||\\nabla L|| > \\text{threshold} \\text{ then } \\nabla L = \\frac{\\nabla L}{||\\nabla L||} \\times \\text{threshold}\n",
    "  $$\n",
    "\n",
    "4. **Arquiteturas Recorrentes Avançadas**: **LSTM (Long Short-Term Memory)** e **GRU (Gated Recurrent Unit)**: Essas arquiteturas incluem mecanismos internos de controle de fluxo de informações, ajudando a manter os gradientes estáveis e preservando informações de longo prazo.\n",
    "\n",
    "Imagine que você está treinando um RNN para prever a próxima palavra em uma frase. Se o gradiente desaparece, **o modelo se torna incapaz de aprender** a influência de palavras anteriores (por exemplo, lembrar que \"não\" precede \"gosta\" para prever \"não gosta\"). Se o gradiente explode, o modelo fica instável e suas previsões se tornam erráticas.\n",
    "\n",
    "Resumo:\n",
    "- **Vanishing Gradients**: Gradientes diminuem exponencialmente, dificultando o aprendizado de dependências de longo prazo.\n",
    "- **Exploding Gradients**: Gradientes aumentam exponencialmente, causando instabilidade e problemas numéricos no treinamento.\n",
    "- **Mitigação**: Inicialização adequada dos pesos, uso de funções de ativação adequadas, normalização dos gradientes e uso de arquiteturas avançadas como LSTM e GRU.\n",
    "\n",
    "Entender e lidar com vanishing e exploding gradients é crucial para treinar RNNs de maneira eficaz e alcançar bom desempenho em tarefas de modelagem de linguagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316d4ac4",
   "metadata": {},
   "source": [
    "## LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e40e67",
   "metadata": {},
   "source": [
    "LSTM ou Long Short-Term Memory é uma arquitetura de rede neural recorrente (RNN) especialmente **projetada para superar os problemas de vanishing e exploding gradients** que podem ocorrer durante o treinamento de redes recorrentes tradicionais. LSTMs são eficazes na captura de dependências de longo prazo em dados sequenciais.\n",
    "\n",
    "A estrutura básica de uma célula LSTM é mais complexa do que uma célula RNN tradicional. Cada célula LSTM possui uma série de \"portões\" que controlam o fluxo de informações. Esses portões são:\n",
    "\n",
    "1. **Portão de Esquecimento (Forget Gate)**: Decide quais informações da célula anterior devem ser esquecidas.\n",
    "2. **Portão de Entrada (Input Gate)**: Decide quais novas informações serão armazenadas na célula.\n",
    "3. **Portão de Saída (Output Gate)**: Decide quais informações da célula serão usadas para a saída.\n",
    "\n",
    "Componentes Principais:\n",
    "- **Estado da Célula ($ C_t $)**: Armazena informações a longo prazo.\n",
    "- **Estado Oculto ($ h_t $)**: Armazena informações para o próximo passo de tempo.\n",
    "\n",
    "As aplicações desse modelo incluem:\n",
    "- Next character prediction\n",
    "- Chatbots\n",
    "- Music Compositon\n",
    "- Image Captioning\n",
    "- Speech Recognition\n",
    "\n",
    "### Portões do LSTM\n",
    "\n",
    "<img src=\"./imgs/lstm_gates.png\">\n",
    "\n",
    "\n",
    "#### 1. Portão de Esquecimento\n",
    "Decide quais partes do estado da célula devem ser esquecidas:\n",
    "$$\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "Onde $ \\sigma $ é a função sigmoide, $ W_f $ são os pesos do portão de esquecimento, $ h_{t-1} $ é o estado oculto anterior, $ x_t $ é a entrada atual e $ b_f $ é o viés.\n",
    "\n",
    "#### 2. Portão de Entrada\n",
    "Decide quais novas informações serão armazenadas no estado da célula:\n",
    "$$\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "Onde $ i_t $ é o vetor de atualização da célula e $ \\tilde{C}_t $ é o vetor de candidatos a novas informações.\n",
    "\n",
    "#### 3. Atualização do Estado da Célula\n",
    "Atualiza o estado da célula combinando o estado anterior modificado pelo portão de esquecimento e o novo candidato modificado pelo portão de entrada:\n",
    "$$\n",
    "C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "$$\n",
    "\n",
    "#### 4. Portão de Saída\n",
    "Decide quais informações do estado da célula serão usadas para a saída:\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "$$\n",
    "h_t = o_t \\cdot \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "### Resumo das Operações\n",
    "\n",
    "1. **Esquecimento**: Parte do estado da célula anterior é esquecida.\n",
    "2. **Atualização**: Novas informações relevantes são adicionadas ao estado da célula.\n",
    "3. **Saída**: O estado da célula atualizado é usado para produzir o estado oculto atual, que também pode ser usado como saída.\n",
    "\n",
    "### Diagrama da Célula LSTM\n",
    "\n",
    "Um diagrama típico de uma célula LSTM mostra a interação entre esses portões e o fluxo de informações. Cada portão é uma camada neural com uma função de ativação específica (sigmoide para $ f_t $, $ i_t $, $ o_t $ e tanh para $ \\tilde{C}_t $ e $ h_t $).\n",
    "\n",
    "### Intuição por Trás das LSTMs\n",
    "\n",
    "- **Portão de Esquecimento**: Permite que a célula decida quais informações antigas são relevantes o suficiente para serem mantidas.\n",
    "- **Portão de Entrada**: Permite que a célula decida quais novas informações devem ser adicionadas ao estado.\n",
    "- **Portão de Saída**: Permite que a célula decida quais informações do estado devem ser usadas para a saída atual.\n",
    "\n",
    "### Vantagens das LSTMs\n",
    "\n",
    "- **Aprendizagem de Dependências de Longo Prazo**: Graças à estrutura dos portões, LSTMs podem capturar dependências de longo prazo de maneira mais eficaz do que RNNs tradicionais.\n",
    "- **Controle Fino do Fluxo de Informação**: Os portões fornecem um mecanismo para controlar o fluxo de informações dentro da célula, mitigando os problemas de vanishing e exploding gradients.\n",
    "\n",
    "### Aplicações das LSTMs\n",
    "\n",
    "- **Processamento de Linguagem Natural (NLP)**: Tradução automática, geração de texto, análise de sentimentos.\n",
    "- **Reconhecimento de Fala**: Transcrição de fala para texto.\n",
    "- **Séries Temporais**: Previsão de séries temporais, como preços de ações e dados meteorológicos.\n",
    "- **Visão Computacional**: Descrição de imagens e vídeos.\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- **LSTM**: Uma arquitetura de rede neural recorrente que usa portões para controlar o fluxo de informações, permitindo a captura de dependências de longo prazo.\n",
    "- **Componentes Principais**: Portões de esquecimento, entrada e saída, estado da célula e estado oculto.\n",
    "- **Vantagens**: Mitiga problemas de vanishing e exploding gradients, captura dependências de longo prazo, controle fino do fluxo de informações.\n",
    "\n",
    "As LSTMs são uma ferramenta poderosa para modelagem de dados sequenciais, proporcionando melhorias significativas em diversas aplicações práticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6967ee",
   "metadata": {},
   "source": [
    "Ler [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f8f325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-05T21:23:55.381090Z",
     "start_time": "2024-07-05T21:23:55.377669Z"
    }
   },
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34920279",
   "metadata": {},
   "source": [
    "A tarefa de Reconhecimento de Entidades Nomeadas (NER) envolva a extração e localização de entidades pré-definidas no texto. Isso permite que encontremos termos que representam lugares, organizações, nomes, datas, etc. \n",
    "\n",
    "<img src=\"./imgs/intro_ner.png\">\n",
    "\n",
    "Os sistemas baseados em NER são utilizados para aumentar a eficiência em pesquisa, motores de recomendação, atendimento ao cliente, negociações automáticas (trading), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8c006",
   "metadata": {},
   "source": [
    "## Training NERs: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdc36db",
   "metadata": {},
   "source": [
    "O processamento de dados é uma das tarefas mais importantes no treinamento de algoritmos de IA. Para NER, você deve:\n",
    "\n",
    "- Converta palavras e classes de entidades em arrays:\n",
    "- Bloco com tokens: Defina o comprimento da sequência para um determinado número e use o token <PAD> para preencher espaços vazios\n",
    "- Crie um gerador de dados:\n",
    "\n",
    "Depois de fazer isso, você pode atribuir um número a cada classe e um número a cada palavra.\n",
    "\n",
    "<img src=\"./imgs/train_ner1.png\">\n",
    "\n",
    "Treinando um sistema NER:\n",
    "- Crie um tensor para cada entrada e seu número correspondente\n",
    "- Coloque-os em lote ==> 64, 128, 256, 512 ...\n",
    "- Alimente-o em uma unidade LSTM\n",
    "- Passe a saída por uma camada densa\n",
    "- Prever usando um log softmax sobre classes K\n",
    "\n",
    "Aqui está um exemplo da arquitetura:\n",
    "    \n",
    "<img src=\"./imgs/train_ner2.png\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2af8b",
   "metadata": {},
   "source": [
    "### Treinamento de um Modelo BERT para Named Entity Recognition (NER)\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) é um modelo de linguagem pré-treinado que alcançou resultados de ponta em várias tarefas de NLP, incluindo Named Entity Recognition (NER). BERT é particularmente poderoso devido à sua capacidade de considerar o contexto bidirecional, o que é crucial para a tarefa de NER.\n",
    "\n",
    "### Etapas para Treinar um Modelo BERT para NER\n",
    "\n",
    "#### 1. Preparação dos Dados\n",
    "\n",
    "1. **Coleta de Dados**:\n",
    "   - Utilize um corpus anotado para NER, como CoNLL-2003 ou OntoNotes.\n",
    "   - Cada token deve ser anotado com sua respectiva etiqueta (ex: B-PER, I-PER, B-LOC, O).\n",
    "\n",
    "2. **Pré-processamento**:\n",
    "   - Tokenize o texto usando o tokenizer BERT, que divide o texto em subpalavras (WordPiece).\n",
    "   - Alinhe as etiquetas com os tokens BERT. Se um token é dividido em subtokens, a etiqueta do primeiro token é replicada para todos os subtokens.\n",
    "\n",
    "#### 2. Configuração do Ambiente\n",
    "\n",
    "- **Biblioteca**: Utilize a biblioteca `transformers` da Hugging Face, que fornece uma implementação do BERT e utilitários para tarefas de NLP.\n",
    "- **Framework**: PyTorch ou TensorFlow (a seguir, um exemplo com PyTorch).\n",
    "\n",
    "#### 3. Carregamento e Configuração do Modelo\n",
    "\n",
    "1. **Modelo Pré-treinado**:\n",
    "   - Carregue um modelo BERT pré-treinado (`bert-base-cased` ou `bert-base-uncased`).\n",
    "\n",
    "2. **Camada de Classificação**:\n",
    "   - Adicione uma camada linear no topo do BERT para classificar as etiquetas NER.\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "\n",
    "# Carregue o tokenizer e o modelo BERT pré-treinado\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels)\n",
    "```\n",
    "\n",
    "#### 4. Preparação dos Dados para o Modelo\n",
    "\n",
    "1. **Tokenização**:\n",
    "   - Tokenize o texto e as etiquetas.\n",
    "\n",
    "2. **Criação de Tensores**:\n",
    "   - Converta as sequências tokenizadas e as etiquetas em tensores PyTorch.\n",
    "\n",
    "```python\n",
    "def tokenize_and_align_labels(texts, labels, tokenizer, label_map):\n",
    "    tokenized_inputs = tokenizer(texts, truncation=True, is_split_into_words=True, padding=True)\n",
    "    labels_aligned = []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = [-100 if word_id is None else label_map[label[word_id]] for word_id in word_ids]\n",
    "        labels_aligned.append(aligned_labels)\n",
    "\n",
    "    return tokenized_inputs, labels_aligned\n",
    "\n",
    "# Supondo que texts e labels sejam listas de sentenças e suas respectivas etiquetas\n",
    "tokenized_inputs, labels_aligned = tokenize_and_align_labels(texts, labels, tokenizer, label_map)\n",
    "\n",
    "# Converta para tensores\n",
    "input_ids = torch.tensor(tokenized_inputs['input_ids'])\n",
    "attention_masks = torch.tensor(tokenized_inputs['attention_mask'])\n",
    "labels = torch.tensor(labels_aligned)\n",
    "```\n",
    "\n",
    "#### 5. Treinamento do Modelo\n",
    "\n",
    "1. **Definição da Função de Perda e Otimizador**:\n",
    "   - Use `CrossEntropyLoss` ignorando a etiqueta `-100`.\n",
    "   - Utilize AdamW como otimizador, que é adequado para modelos de linguagem.\n",
    "\n",
    "2. **Treinamento**:\n",
    "   - Realize o treinamento do modelo por várias épocas, alimentando o modelo com minibatches dos dados.\n",
    "\n",
    "```python\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Crie DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=16)\n",
    "\n",
    "# Defina a função de perda e otimizador\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "# Treinamento\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        b_input_ids, b_attention_masks, b_labels = batch\n",
    "        model.zero_grad()\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_masks, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "```\n",
    "\n",
    "#### 6. Avaliação e Ajuste do Modelo\n",
    "\n",
    "1. **Avaliação**:\n",
    "   - Utilize um conjunto de validação para avaliar a precisão, revocação e F1-score do modelo.\n",
    "\n",
    "2. **Ajustes Finais**:\n",
    "   - Ajuste os hiperparâmetros (taxa de aprendizado, tamanho do batch, número de épocas) com base nos resultados da avaliação.\n",
    "\n",
    "#### 7. Inferência\n",
    "\n",
    "Após o treinamento, o modelo pode ser utilizado para fazer previsões em novos dados.\n",
    "\n",
    "```python\n",
    "# Modo de avaliação\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_attention_masks = batch\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_masks)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "```\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- **Preparação dos Dados**: Coleta, anotação e tokenização dos dados.\n",
    "- **Carregamento do Modelo**: Uso de um modelo BERT pré-treinado com uma camada de classificação adicional.\n",
    "- **Preparação dos Dados para Treinamento**: Alinhamento das etiquetas e criação de tensores.\n",
    "- **Treinamento**: Definição da função de perda e otimizador, e treinamento do modelo.\n",
    "- **Avaliação**: Uso de métricas de desempenho para ajustar o modelo.\n",
    "- **Inferência**: Utilização do modelo treinado para fazer previsões em novos dados.\n",
    "\n",
    "O uso de BERT para NER oferece resultados robustos devido à sua capacidade de entender o contexto bidirecional, o que é essencial para identificar e classificar entidades nomeadas de forma precisa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f173481",
   "metadata": {},
   "source": [
    "## Computing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc79ef56",
   "metadata": {},
   "source": [
    "### Computando a Acurácia de um Modelo NER\n",
    "\n",
    "A acurácia de um modelo de reconhecimento de entidades nomeadas (NER) pode ser avaliada através de várias métricas, incluindo precisão, revocação e F1-score. Para computar a acurácia especificamente, você pode considerar a proporção de tokens corretamente classificados em relação ao total de tokens.\n",
    "\n",
    "#### Passos para Computar a Acurácia de um Modelo NER\n",
    "\n",
    "1. **Preparação dos Dados de Teste**: Utilize um conjunto de dados de teste anotado que não foi usado durante o treinamento do modelo.\n",
    "2. **Inferência**: Faça previsões no conjunto de dados de teste usando o modelo treinado.\n",
    "3. **Comparação de Etiquetas**: Compare as etiquetas preditas com as etiquetas verdadeiras.\n",
    "4. **Cálculo da Acurácia**: Calcule a acurácia como a proporção de tokens corretamente classificados.\n",
    "\n",
    "### Implementação em PyTorch Usando BERT\n",
    "\n",
    "#### 1. Preparação dos Dados de Teste\n",
    "\n",
    "```python\n",
    "# Supondo que texts e labels sejam listas de sentenças e suas respectivas etiquetas para o conjunto de teste\n",
    "tokenized_inputs, labels_aligned = tokenize_and_align_labels(test_texts, test_labels, tokenizer, label_map)\n",
    "\n",
    "# Converta para tensores\n",
    "test_input_ids = torch.tensor(tokenized_inputs['input_ids'])\n",
    "test_attention_masks = torch.tensor(tokenized_inputs['attention_mask'])\n",
    "test_labels = torch.tensor(labels_aligned)\n",
    "```\n",
    "\n",
    "#### 2. Inferência\n",
    "\n",
    "```python\n",
    "# Crie DataLoader para o conjunto de teste\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Modo de avaliação\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_attention_masks, b_labels = batch\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_masks)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        # Remova os índices de padding para comparação\n",
    "        active_logits = predictions.view(-1)\n",
    "        active_labels = b_labels.view(-1)\n",
    "        active_attention_masks = b_attention_masks.view(-1)\n",
    "        \n",
    "        active_logits = active_logits[active_attention_masks == 1]\n",
    "        active_labels = active_labels[active_attention_masks == 1]\n",
    "\n",
    "        all_predictions.extend(active_logits.cpu().numpy())\n",
    "        all_true_labels.extend(active_labels.cpu().numpy())\n",
    "```\n",
    "\n",
    "#### 3. Comparação de Etiquetas\n",
    "\n",
    "```python\n",
    "# Converta os arrays de numpy para listas para facilitar a manipulação\n",
    "all_predictions = list(all_predictions)\n",
    "all_true_labels = list(all_true_labels)\n",
    "```\n",
    "\n",
    "#### 4. Cálculo da Acurácia\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "print(f\"Acurácia do modelo NER: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- **Preparação dos Dados de Teste**: Tokenização e conversão para tensores.\n",
    "- **Inferência**: Uso do modelo para fazer previsões no conjunto de teste.\n",
    "- **Comparação de Etiquetas**: Comparação das etiquetas preditas com as etiquetas verdadeiras, ignorando os tokens de padding.\n",
    "- **Cálculo da Acurácia**: Proporção de tokens corretamente classificados sobre o total de tokens.\n",
    "\n",
    "### Exemplo Completo de Computação da Acurácia\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Função para tokenizar e alinhar etiquetas\n",
    "def tokenize_and_align_labels(texts, labels, tokenizer, label_map):\n",
    "    tokenized_inputs = tokenizer(texts, truncation=True, is_split_into_words=True, padding=True)\n",
    "    labels_aligned = []\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        aligned_labels = [-100 if word_id is None else label_map[label[word_id]] for word_id in word_ids]\n",
    "        labels_aligned.append(aligned_labels)\n",
    "\n",
    "    return tokenized_inputs, labels_aligned\n",
    "\n",
    "# Carregar o tokenizer e o modelo BERT pré-treinado\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-cased', num_labels=num_labels)\n",
    "\n",
    "# Supondo que test_texts e test_labels sejam listas de sentenças e suas respectivas etiquetas para o conjunto de teste\n",
    "tokenized_inputs, labels_aligned = tokenize_and_align_labels(test_texts, test_labels, tokenizer, label_map)\n",
    "\n",
    "# Converta para tensores\n",
    "test_input_ids = torch.tensor(tokenized_inputs['input_ids'])\n",
    "test_attention_masks = torch.tensor(tokenized_inputs['attention_mask'])\n",
    "test_labels = torch.tensor(labels_aligned)\n",
    "\n",
    "# Crie DataLoader para o conjunto de teste\n",
    "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Modo de avaliação\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        b_input_ids, b_attention_masks, b_labels = batch\n",
    "        outputs = model(b_input_ids, attention_mask=b_attention_masks)\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "\n",
    "        # Remova os índices de padding para comparação\n",
    "        active_logits = predictions.view(-1)\n",
    "        active_labels = b_labels.view(-1)\n",
    "        active_attention_masks = b_attention_masks.view(-1)\n",
    "        \n",
    "        active_logits = active_logits[active_attention_masks == 1]\n",
    "        active_labels = active_labels[active_attention_masks == 1]\n",
    "\n",
    "        all_predictions.extend(active_logits.cpu().numpy())\n",
    "        all_true_labels.extend(active_labels.cpu().numpy())\n",
    "\n",
    "# Cálculo da acurácia\n",
    "accuracy = accuracy_score(all_true_labels, all_predictions)\n",
    "print(f\"Acurácia do modelo NER: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "Este código cobre todo o processo de preparação dos dados, inferência e cálculo da acurácia para um modelo BERT treinado para a tarefa de NER."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b799a",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee377d",
   "metadata": {},
   "source": [
    "## Siamese Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233779e9",
   "metadata": {},
   "source": [
    "Redes Siamesas são uma arquitetura de redes neurais que consistem em duas ou mais redes gêmeas, ou idênticas, que compartilham os mesmos pesos e parâmetros. Essas redes são usadas para aprender representações que permitem comparar pares de entradas. \n",
    "\n",
    "Observe que no primeiro exemplo abaixo, as duas frases significam a mesma coisa, mas possuem palavras completamente diferentes. Já no segundo caso, as duas frases significam coisas completamente diferentes, mas têm palavras muito semelhantes. Enquanto que os algoritmos de classificação aprendem o que torna um input o que ele é, as **redes siamesas** aprendem o que torna **dois** inputs o que eles são.\n",
    "\n",
    "<img src=\"./imgs/phrases_ex.png\">\n",
    "\n",
    "\n",
    "### Estrutura das Redes Siamesas\n",
    "\n",
    "A estrutura básica de uma rede siamesa inclui:\n",
    "\n",
    "1. **Redes Idênticas**: As redes idênticas têm a mesma arquitetura e compartilham os mesmos pesos. Isso significa que as mesmas transformações são aplicadas em ambas as entradas.\n",
    "\n",
    "2. **Camada de Comparação**: Após passar pelas redes idênticas, as representações geradas são comparadas usando uma função de distância, como a distância Euclidiana ou Cosine.\n",
    "\n",
    "3. **Função de Perda**: A função de perda é projetada para minimizar a distância entre representações de entradas semelhantes e maximizar a distância entre representações de entradas diferentes.\n",
    "\n",
    "### Aplicações das Redes Siamesas\n",
    "\n",
    "1. **Reconhecimento Facial**: Comparar imagens de rostos para verificar se são da mesma pessoa.\n",
    "\n",
    "2. **Verificação de Assinaturas**: Comparar assinaturas manuscritas para verificar sua autenticidade.\n",
    "\n",
    "3. **Detecção de Plágio**: Comparar documentos para detectar similaridade de conteúdo.\n",
    "\n",
    "4. **Correspondência de Documentos**: Comparar documentos para encontrar correspondências ou duplicatas.\n",
    "\n",
    "### Funcionamento das Redes Siamesas\n",
    "\n",
    "1. **Entrada de Pares**: Dois exemplos de assinatura (A e B) são dados como entrada ao sistema.\n",
    "\n",
    "2. **Processamento pelas Redes Gêmeas**: As duas assinaturas passam por duas redes idênticas que compartilham os mesmos pesos. Cada rede transforma a entrada em uma representação vetorial (embedding).\n",
    "\n",
    "3. **Cálculo da Distância**: As representações vetoriais das duas assinaturas são comparadas usando uma função de distância, como a distância Euclidiana.\n",
    "\n",
    "4. **Função de Perda**: A função de perda é configurada para minimizar a distância entre representações de assinaturas da mesma pessoa e maximizar a distância entre representações de assinaturas de pessoas diferentes.\n",
    "\n",
    "### Exemplo de Implementação\n",
    "\n",
    "Vamos considerar um exemplo simples usando PyTorch para ilustrar uma rede siamesa.\n",
    "\n",
    "#### 1. Definição da Rede Neural\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        # Primeira camada convolucional (entrada com 1 canal, saída com 64 canais, kernel 10x10)\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=10)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=7)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=4)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=4)\n",
    "        # Primeira camada totalmente conectada (entrada com 256*6*6 neurônios, saída com 4096 neurônios)\n",
    "        self.fc1 = nn.Linear(256*6*6, 4096)\n",
    "        # Segunda camada totalmente conectada (entrada com 4096 neurônios, saída com 1 neurônio)\n",
    "        self.fc2 = nn.Linear(4096, 1)\n",
    "    \n",
    "    def forward_once(self, x):\n",
    "        # Passa a entrada pela primeira camada convolucional seguida por ReLU e max pooling\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        # Achata a saída para um vetor\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        # Passa a entrada pela primeira camada totalmente conectada seguida por ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "    \n",
    "    def forward(self, input1, input2):\n",
    "        # Passa os dois inputs pelas redes idênticas\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "```\n",
    "\n",
    "#### 2. Função de Distância e Perda\n",
    "\n",
    "```python\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                          (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss\n",
    "```\n",
    "\n",
    "#### 3. Treinamento da Rede Siamesa\n",
    "\n",
    "```python\n",
    "# Exemplo de treinamento\n",
    "model = SiameseNetwork()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Suponha que `train_loader` seja um DataLoader que fornece pares de imagens e seus rótulos\n",
    "for epoch in range(num_epochs):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        img0, img1, label = data\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img0, img1)\n",
    "        loss = criterion(output1, output2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "Em resumo\n",
    "\n",
    "- **Arquitetura**: Redes Siamesas consistem em duas redes idênticas que compartilham os mesmos pesos e são usadas para aprender representações comparáveis.\n",
    "- **Aplicações**: São usadas em várias tarefas de comparação e verificação, como reconhecimento facial, verificação de assinaturas e detecção de plágio.\n",
    "- **Funcionamento**: As entradas são processadas pelas redes gêmeas, comparadas usando uma função de distância, e treinadas para minimizar uma função de perda específica.\n",
    "\n",
    "Redes Siamesas são poderosas para tarefas que envolvem comparação e verificação, aproveitando a capacidade de aprender representações discriminativas através de treinamento supervisionado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d5ed88",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf24ab5",
   "metadata": {},
   "source": [
    "A arquitetura do modelo de uma rede siamesa típica poderia ser a seguinte:\n",
    "\n",
    "<img src=\"./imgs/siamese_architecture.png\">\n",
    "\n",
    "Essas duas sub-redes são redes irmãs que se unem para produzir uma pontuação de similaridade. Nem todas as redes siamesas serão projetadas para conter LSTMs. Uma coisa a lembrar é que as sub-redes compartilham parâmetros idênticos. Isso significa que você só precisa treinar **um** conjunto de pesos e não dois. As sub-redes idênticas compartilham os mesmos pesos, ou seja, **as mesmas camadas** como as convolucionais, totalmente conectadas, etc., **são aplicadas a ambas as entradas**. Isso significa que **as atualizações de pesos durante o treinamento são aplicadas igualmente a ambas as sub-redes**.\n",
    "\n",
    "A saída de cada sub-rede é um vetor. Podemos então executar a saída por meio de uma **função de similaridade de cosseno** para obter a pontuação de similaridade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c58d02",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47d813e",
   "metadata": {},
   "source": [
    "Podemos computar a função de custo com o **triplet loss** para redes siamesas. O triplet utiliza busca e três exemplos: Anchor, Positive e Negative. Os pesos do modelo devem ser ajustados de uma forma que o Anchor e o Positivo tenham similaridade de cosseno próximo de 1, enquanto que o Anchor e o Negative devem ter a similaridade próximo de -1. **Minimizar a diferença entre A e N e A e P é equivalente a maximizar a similaridade entre A e P, enquanto minimiza A e P**, já que quanto maior similaridade de A e P e quanto menor a similaridade de A e N, mais próximo de zero essa diferença será. A equação que se busca minimizar é a seguinte:\n",
    "\n",
    "<img src=\"./imgs/triplet1.png\">\n",
    "\n",
    "Note que se o $cos(A, P) = 1$ e $cos(A, N) = -1$, então a equação é menor que zero. Mas se o cos(A, P) desvia de 1 e cos(A, N) desvia de -1, podemos acabar com um custo que é maior que zero.\n",
    "\n",
    "<img src=\"./imgs/triplet2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe90a052",
   "metadata": {},
   "source": [
    "A triplet loss é uma função de perda usada para treinar modelos de aprendizado de máquina, especialmente em tarefas de reconhecimento facial, verificação de identidade, e outras tarefas de comparação. Ela funciona selecionando três exemplos (um triplet) a cada passo do treinamento: um âncora (anchor), um exemplo positivo (positive) e um exemplo negativo (negative). O objetivo é fazer com que a distância entre a âncora e o exemplo positivo seja menor do que a distância entre a âncora e o exemplo negativo, por pelo menos uma margem especificada.\n",
    "\n",
    "Componentes do Triplet Loss:\n",
    "\n",
    "1. **Anchor (A)**: Um exemplo de entrada.\n",
    "2. **Positive (P)**: Um exemplo que é similar ao anchor (da mesma classe).\n",
    "3. **Negative (N)**: Um exemplo que é dissimilar ao anchor (de uma classe diferente).\n",
    "\n",
    "A função de perda triplet é definida como:\n",
    "\n",
    "$$ \\text{loss} = \\max(0, \\| f(A) - f(P) \\|^2 - \\| f(A) - f(N) \\|^2 + \\text{margin}) $$\n",
    "\n",
    "Onde:\n",
    "- $ f(\\cdot) $ representa a função de transformação (ou embeddagem) do modelo.\n",
    "- $ \\| f(A) - f(P) \\|^2 $ é a distância entre o embedding da âncora e o embedding do exemplo positivo.\n",
    "- $ \\| f(A) - f(N) \\|^2 $ é a distância entre o embedding da âncora e o embedding do exemplo negativo.\n",
    "- $ \\text{margin} $ é um valor que define o quanto a distância entre a âncora e o exemplo negativo deve ser maior que a distância entre a âncora e o exemplo positivo.\n",
    "\n",
    "O objetivo da triplet loss é garantir que:\n",
    "\n",
    "- As representações (embeddings) de exemplos da mesma classe estejam próximas no espaço de embeddings.\n",
    "- As representações de exemplos de classes diferentes estejam distantes no espaço de embeddings.\n",
    "\n",
    "A margem é um hiperparâmetro que define o quanto a distância entre a âncora e o exemplo negativo deve ser maior do que a distância entre a âncora e o exemplo positivo. Isso ajuda a criar um \"espaço\" entre classes diferentes. Como Funciona:\n",
    "\n",
    "1. **Forward Pass**: Passe o anchor, o positive, e o negative pela rede para obter seus embeddings.\n",
    "2. **Cálculo das Distâncias**: Calcule a distância entre o embedding do anchor e do positive. Calcule a distância entre o embedding do anchor e do negative.\n",
    "3. **Cálculo da Perda**: Calcule a triplet loss usando a fórmula acima. A perda será zero se a distância entre o anchor e o negative for maior que a distância entre o anchor e o positive por pelo menos a margem.\n",
    "4. **Backward Pass e Atualização dos Pesos**: Calcule os gradientes e atualize os pesos da rede.\n",
    "\n",
    "Exemplo de Implementação em PyTorch\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        positive_distance = F.pairwise_distance(anchor, positive)\n",
    "        negative_distance = F.pairwise_distance(anchor, negative)\n",
    "        loss = torch.mean(torch.relu(positive_distance - negative_distance + self.margin))\n",
    "        return loss\n",
    "\n",
    "# Exemplo de uso\n",
    "anchor = torch.randn(10, 128)   # 10 exemplos, dimensão do embedding 128\n",
    "positive = torch.randn(10, 128) # 10 exemplos, dimensão do embedding 128\n",
    "negative = torch.randn(10, 128) # 10 exemplos, dimensão do embedding 128\n",
    "\n",
    "criterion = TripletLoss(margin=1.0)\n",
    "loss = criterion(anchor, positive, negative)\n",
    "\n",
    "print(\"Triplet Loss:\", loss.item())\n",
    "```\n",
    "\n",
    "A triplet loss é uma poderosa função de perda que ajuda a treinar modelos para aprender embeddings discriminativos, garantindo que exemplos da mesma classe estejam próximos uns dos outros no espaço de embeddings e exemplos de classes diferentes estejam distantes. Isso é especialmente útil em tarefas de verificação e identificação, como reconhecimento facial e verificação de assinaturas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e23b8ac",
   "metadata": {},
   "source": [
    "## Triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1650125",
   "metadata": {},
   "source": [
    "A seleção de hard triplets durante o treinamento com triplet loss ajuda a focar o modelo nos exemplos mais desafiadores. Isso força o modelo a ajustar seus pesos de forma mais precisa, resultando em uma melhor capacidade de distinguir entre exemplos positivos e negativos, mesmo quando as similaridades são muito próximas. Essa abordagem otimiza o treinamento, permitindo que o modelo aprenda de forma mais robusta e eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c728af30",
   "metadata": {},
   "source": [
    "## Computing The Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da8318",
   "metadata": {},
   "source": [
    "Para computar o custo, podemos preparar o batch da seguinte forma\n",
    "\n",
    "<img src=\"./imgs/computing_cost1.png\">\n",
    "\n",
    "Note que cada exemplo da esquerda tem um exemplo similar a direita (significam o mesmo), mas nenhum em nenhuma das colunas os exemplos são similares entre si. Podemos calcular a matriz de similaridade entre cada par possível nas colunas esquerda e direita.\n",
    "\n",
    "<img src=\"./imgs/computing_cost2.png\">\n",
    "\n",
    "A linha diagonal corresponde a pontuações de sentenças semelhantes (normalmente deveriam ser positivas). As fora-diagonais correspondem às pontuações de cosseno entre a **âncora e os exemplos negativos**.\n",
    "\n",
    "Agora que temos a matrix scores de similaridade de cossenos, que é o produto de duas matrizes, podemos seguir com a computação do custo.\n",
    "\n",
    "<img src=\"./imgs/computing_cost3.png\">\n",
    "\n",
    "- O **mean_neg** é simplesmente a média dos valores **off-diagonal** de cada linha (sem o valor diagonal). \n",
    "- Já o **closest_neg** é o valor off-diagonal mais alto (mas menor que) o valor da diagonal.\n",
    "\n",
    "$$ \\text{Cost} = \\max(- cos(A, P) + cos(A, N) + \\alpha, 0)$$\n",
    "\n",
    "Agora, teremos dois custos:\n",
    "\n",
    "$$ \\text{Cost1} = \\max(- cos(A, P) + \\text{mean_neg} + \\alpha, 0)$$\n",
    "\n",
    "$$ \\text{Cost2} = \\max(- cos(A, P) + \\text{closest_neg} + \\alpha, 0)$$\n",
    "\n",
    "O custo total é definido como **Cost1 + Cost2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8024203",
   "metadata": {},
   "source": [
    "## One Shot Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f67a51",
   "metadata": {},
   "source": [
    "One Shot Learning é uma técnica de aprendizado de máquina que visa ensinar um modelo a reconhecer ou classificar novos objetos a partir de apenas um ou poucos exemplos. Isso contrasta com as abordagens tradicionais de aprendizado de máquina, que geralmente requerem grandes quantidades de dados de treinamento para alcançar um bom desempenho. O One Shot Learning é particularmente útil em situações onde **a coleta de muitos exemplos de treinamento é difícil, custosa ou impraticável**.\n",
    "\n",
    "Imagine que voce trabalha em um banco e precisa verificar a assinatura em documentos. Poderíamos construir um modelo para classificar as assinaturas com K possíveis assinaturas como outputs, ou poderíamos simplesmente classificar que as duas assinaturas (real e input) são diferentes. Em vez de treinar novamente seu modelo para cada assinatura, você pode **simplesmente aprender uma pontuação de similaridade** da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/osl1.png\">\n",
    "<img src=\"./imgs/osl2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f5ca5",
   "metadata": {},
   "source": [
    "## Training / Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4417941e",
   "metadata": {},
   "source": [
    "(implementa usando o quora duplicate dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3854b1",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Sequence Models, disponível em https://www.coursera.org/learn/sequence-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b42dd8",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
