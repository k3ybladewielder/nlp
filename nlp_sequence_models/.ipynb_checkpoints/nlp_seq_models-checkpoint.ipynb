{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4df9530d",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Probabilistic Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Sequence Models da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c6507e",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98f45e6",
   "metadata": {},
   "source": [
    "## Neural Networks for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f568f07",
   "metadata": {},
   "source": [
    "Modelos mais simples são um ótimo baseline mas podem não capturar bem a o sentimento de uma frase. Nesses casos, podemos utilizar redes neurais.\n",
    "\n",
    "As redes neurais (Neural Networks) são um tipo de modelo de aprendizado de máquina inspirado na estrutura e no funcionamento do cérebro humano. Elas consistem em camadas de unidades interconectadas (neurônios) que processam informações e aprendem a realizar tarefas através de exemplos. As principais componentes das redes neurais são:\n",
    "\n",
    "1. **Camada de Entrada (Input Layer):** Recebe os dados brutos.\n",
    "2. **Camadas Ocultas (Hidden Layers):** Realizam processamento intermediário. Cada neurônio em uma camada oculta está conectado a neurônios da camada anterior e da camada seguinte, e aplica uma função de ativação para transformar a entrada.\n",
    "3. **Camada de Saída (Output Layer):** Produz o resultado final, como uma classificação ou previsão.\n",
    "\n",
    "A análise de sentimento envolve a **classificação de textos**, como comentários ou tweets, para determinar se expressam sentimentos positivos, negativos ou neutros. As redes neurais, especialmente as arquiteturas avançadas como LSTM (Long Short-Term Memory) e GRU (Gated Recurrent Units), são amplamente usadas nessa tarefa devido à sua capacidade de **capturar dependências de longo prazo em sequências de texto**.\n",
    "\n",
    "O processo de análise de sentimento com redes neurais segue os seguintes passos:\n",
    "\n",
    "1. **Pré-processamento:** O texto bruto é limpo e transformado em uma **representação numérica**, como vetores de palavras (Word Embeddings) ou **sequências de índices de palavras**.\n",
    "2. **Arquitetura do Modelo:**\n",
    "   - **Embedding Layer:** Transforma as palavras em vetores densos que capturam seus significados.\n",
    "   - **Camadas Recurrentes (LSTM/GRU):** Processam a sequência de vetores, mantendo informações contextuais ao longo do tempo.\n",
    "   - **Camadas Densas (Fully Connected):** Agregam informações das camadas anteriores e produzem a probabilidade de cada classe de sentimento (positivo, negativo, neutro).\n",
    "3. **Treinamento:** O modelo é treinado em um conjunto de dados rotulados, ajustando os pesos das conexões entre neurônios para minimizar o erro na classificação.\n",
    "4. **Avaliação e Ajuste:** O desempenho do modelo é avaliado em dados de validação, e ajustes são feitos para melhorar a precisão.\n",
    "5. **Predição:** O modelo treinado é usado para analisar novos textos e prever o sentimento.\n",
    "\n",
    "As redes neurais permitem uma análise de sentimento mais robusta, capturando nuances e contextos que métodos mais simples podem perder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedc525f",
   "metadata": {},
   "source": [
    "Os dados de entrada como já mencionados são **representações numéricas** de palavras, como as ilustradas a seguir\n",
    "\n",
    "<img src=\"./imgs/word_representation.png\">\n",
    "\n",
    "<img src=\"./imgs/forward_propagation.png\">\n",
    "\n",
    "Observe que a rede acima possui três camadas (input layer, hidden layer e output layer) e três saídas (ex. positivo, negativo e neutro). Para ir de uma camada para outra você pode usar uma matriz $W^{i}$ para propagar para a próxima camada. Portanto, chamamos esse conceito de ir da entrada até a camada final de propagação direta (forward propagation).\n",
    "\n",
    "Observe que adicionamos zeros para preenchimento para corresponder ao tamanho do tweet mais longo. Uma rede neural na configuração que você pode ver acima **só pode processar um tweet por vez**. Para tornar o treinamento mais eficiente (mais rápido), você deseja processar muitos tweets **em paralelo**. Você consegue isso juntando muitos tweets em uma matriz e depois passando essa matriz (em vez de tweets individuais) pela rede neural. Então a rede neural pode realizar seus cálculos em todos os tweets ao mesmo tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e426a2",
   "metadata": {},
   "source": [
    "## Dense Layers and ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a66e55",
   "metadata": {},
   "source": [
    "As **camadas densas**, também conhecidas como camadas totalmente conectadas (Fully Connected Layers), são um tipo de camada em redes neurais onde **cada neurônio de uma camada está conectado a todos os neurônios da camada seguinte**. Essas conexões são ponderadas e ajustadas durante o processo de treinamento para aprender a mapear entradas para saídas desejadas. A camada Densa é o cálculo do produto interno entre um conjunto de pesos treináveis (matriz de pesos) e um vetor de entrada. Elas funcionam da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/dense_layer.png\">\n",
    "\n",
    "1. **Entrada:** Recebe um vetor de ativação da camada anterior.\n",
    "2. **Pesos e Bias:** Cada conexão tem um peso associado, e cada neurônio tem um valor de bias (viés).\n",
    "3. **Produto Ponto:** A camada calcula o produto ponto entre os vetores de entrada e os pesos, e soma o bias.\n",
    "4. **Função de Ativação:** O resultado é passado por uma função de ativação, que introduz não-linearidade no modelo, permitindo a aprendizagem de relações complexas.\n",
    "\n",
    "\n",
    "### Função de Ativação ReLU (Rectified Linear Unit)\n",
    "\n",
    "A **função de ativação ReLU** é uma das mais populares em redes neurais modernas devido à sua simplicidade e eficácia. ReLU é definida como:\n",
    "\n",
    "$$ f(x) = \\max(0, x) $$\n",
    "\n",
    "**Características:**\n",
    "1. **Linearidade por Partes:** ReLU mantém a linearidade para valores positivos, mas define todos os valores negativos como zero.\n",
    "2. **Não-Linearidade:** Introduz não-linearidade no modelo, essencial para aprender representações complexas.\n",
    "3. **Eficiente Computacionalmente:** Computacionalmente simples e eficiente de calcular.\n",
    "4. **Problema do Neurônio Morto:** Uma desvantagem potencial é que, se muitos neurônios saírem da faixa ativa (produzirem sempre zero), podem \"morrer\" e parar de aprender.\n",
    "\n",
    "<img src=\"./imgs/relu.png\">\n",
    "\n",
    "A caixa laranja na imagem acima mostra a camada densa. Uma camada de ativação é o conjunto de nós azuis mostrados com a caixa laranja na imagem abaixo. Concretamente, uma das camadas de ativação mais comumente utilizadas é a unidade linear retificada (ReLU).\n",
    "\n",
    "**Uso em Camadas Densas:**\n",
    "- **Aprimoramento da Expressividade:** A aplicação da ReLU em camadas densas permite que a rede aprenda representações mais ricas e capture relações não lineares nos dados.\n",
    "- **Mitigação do Desvanecimento de Gradiente:** ReLU ajuda a mitigar o problema do desvanecimento de gradiente, permitindo que gradientes maiores fluam durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d10ac8",
   "metadata": {},
   "source": [
    "## Embedding and Mean Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849795c7",
   "metadata": {},
   "source": [
    "Usando uma camada de embedding, podemos aprender os embeddings para cada palavra do vocabulário da seguinte maneira:\n",
    "\n",
    "<img src=\"./imgs/embedding_layer.png\">\n",
    "\n",
    "A camada de embedding é uma camada especial utilizada em redes neurais para transformar palavras ou tokens em vetores densos de dimensão fixa, onde cada vetor captura informações semânticas sobre a palavra. Este processo é fundamental no processamento de linguagem natural (NLP) e permite que o modelo aprenda representações de palavras que refletem suas relações semânticas. Ele funciona da seguinte forma:\n",
    "\n",
    "1. **Entrada**: Recebe uma sequência de índices, onde cada índice corresponde a uma palavra ou token no vocabulário.\n",
    "2. **Lookup**: Cada índice é mapeado para um vetor denso, geralmente inicializado aleatoriamente e ajustado durante o treinamento.\n",
    "3. **Saída**: Produz uma matriz onde cada linha é o vetor de embedding correspondente a uma palavra na sequência de entrada.\n",
    "\n",
    "Essa camada possui os seguintes benefícios:\n",
    "\n",
    "1. **Dimensionalidade Reduzida**: Transforma palavras em vetores densos de menor dimensão, facilitando o processamento.\n",
    "2. **Captura de Semântica**: As palavras com significados semelhantes tendem a ter vetores próximos no espaço de embedding.\n",
    "3. **Treinável**: Os vetores de embedding são ajustados durante o treinamento para otimizar a tarefa específica, como classificação de texto ou tradução.\n",
    "\n",
    "A camada média (mean layer) permite tirar a média dos embeddings. Elas são usadas para **agregar informações** ao longo de uma sequência, calculando a média dos vetores de embedding ou das ativações ao longo da sequência. Esta técnica é simples e pode ser eficaz para **capturar uma representação global do contexto** ou do conteúdo de um texto. O vetor resultante da camada de média pode ser passado para camadas densas ou outras camadas de rede neural para realizar a classificação de sentimento, determinando se a frase expressa um sentimento positivo, negativo ou neutro. Fuciona da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/mean_layer.png\">\n",
    "\n",
    "1. **Entrada**: Recebe uma matriz de vetores de embedding ou ativações de uma camada anterior, onde cada linha corresponde a uma palavra ou token na sequência.\n",
    "2. **Cálculo da Média**: Calcula a média ao longo de uma dimensão específica (geralmente ao longo da dimensão da sequência).\n",
    "3. **Saída**: Produz um único vetor que representa a média das ativações ou embeddings ao longo da sequência.\n",
    "\n",
    "Possui os seguintes benefícios:\n",
    "\n",
    "1. **Simplicidade**: Fácil de implementar e computacionalmente eficiente.\n",
    "2. **Redução de Dimensionalidade**: Reduz a sequência de vetores a um único vetor, simplificando o processamento subsequente.\n",
    "3. **Representação Global**: Fornece uma representação global da sequência, capturando informações de todas as palavras ou tokens.\n",
    "\n",
    "Esta camada não possui nenhum parâmetro treinável."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176dd683",
   "metadata": {},
   "source": [
    "## Introduction to TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d0b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Explique e descreva brevemente o que são Neural Networks e como são usadas em análise de sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84387a3e",
   "metadata": {},
   "source": [
    "## Traditional Language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fef822",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f261de",
   "metadata": {},
   "source": [
    "## Applications of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b836e",
   "metadata": {},
   "source": [
    "## Math in Simple RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5e41dc",
   "metadata": {},
   "source": [
    "## Hidden State Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71f1d27",
   "metadata": {},
   "source": [
    "## Cost Function for RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b20c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:02:03.350599Z",
     "start_time": "2024-07-01T22:02:03.264743Z"
    }
   },
   "source": [
    "## Implementation Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab6962",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ddea7f",
   "metadata": {},
   "source": [
    "## Vanilla RNNs, GRUs and the scan function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692f2e3",
   "metadata": {},
   "source": [
    "## Deep and Bi-directional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc1bf3",
   "metadata": {},
   "source": [
    "## Calculating Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd43977",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdda21c",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbbe583",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b7b0e",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Classification and Vector Spaces, disponível em https://www.coursera.org/learn/probabilistic-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bad1ecc",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
