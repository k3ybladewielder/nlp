{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3a76511",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Probabilistic Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Sequence Models da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3c7774",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ef4d17",
   "metadata": {},
   "source": [
    "## Neural Networks for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09157558",
   "metadata": {},
   "source": [
    "Modelos mais simples são um ótimo baseline mas podem não capturar bem a o sentimento de uma frase. Nesses casos, podemos utilizar redes neurais.\n",
    "\n",
    "As redes neurais (Neural Networks) são um tipo de modelo de aprendizado de máquina inspirado na estrutura e no funcionamento do cérebro humano. Elas consistem em camadas de unidades interconectadas (neurônios) que processam informações e aprendem a realizar tarefas através de exemplos. As principais componentes das redes neurais são:\n",
    "\n",
    "1. **Camada de Entrada (Input Layer):** Recebe os dados brutos.\n",
    "2. **Camadas Ocultas (Hidden Layers):** Realizam processamento intermediário. Cada neurônio em uma camada oculta está conectado a neurônios da camada anterior e da camada seguinte, e aplica uma função de ativação para transformar a entrada.\n",
    "3. **Camada de Saída (Output Layer):** Produz o resultado final, como uma classificação ou previsão.\n",
    "\n",
    "A análise de sentimento envolve a **classificação de textos**, como comentários ou tweets, para determinar se expressam sentimentos positivos, negativos ou neutros. As redes neurais, especialmente as arquiteturas avançadas como LSTM (Long Short-Term Memory) e GRU (Gated Recurrent Units), são amplamente usadas nessa tarefa devido à sua capacidade de **capturar dependências de longo prazo em sequências de texto**.\n",
    "\n",
    "O processo de análise de sentimento com redes neurais segue os seguintes passos:\n",
    "\n",
    "1. **Pré-processamento:** O texto bruto é limpo e transformado em uma **representação numérica**, como vetores de palavras (Word Embeddings) ou **sequências de índices de palavras**.\n",
    "2. **Arquitetura do Modelo:**\n",
    "   - **Embedding Layer:** Transforma as palavras em vetores densos que capturam seus significados.\n",
    "   - **Camadas Recurrentes (LSTM/GRU):** Processam a sequência de vetores, mantendo informações contextuais ao longo do tempo.\n",
    "   - **Camadas Densas (Fully Connected):** Agregam informações das camadas anteriores e produzem a probabilidade de cada classe de sentimento (positivo, negativo, neutro).\n",
    "3. **Treinamento:** O modelo é treinado em um conjunto de dados rotulados, ajustando os pesos das conexões entre neurônios para minimizar o erro na classificação.\n",
    "4. **Avaliação e Ajuste:** O desempenho do modelo é avaliado em dados de validação, e ajustes são feitos para melhorar a precisão.\n",
    "5. **Predição:** O modelo treinado é usado para analisar novos textos e prever o sentimento.\n",
    "\n",
    "As redes neurais permitem uma análise de sentimento mais robusta, capturando nuances e contextos que métodos mais simples podem perder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc5e10f",
   "metadata": {},
   "source": [
    "Os dados de entrada como já mencionados são **representações numéricas** de palavras, como as ilustradas a seguir\n",
    "\n",
    "<img src=\"./imgs/word_representation.png\">\n",
    "\n",
    "<img src=\"./imgs/forward_propagation.png\">\n",
    "\n",
    "Observe que a rede acima possui três camadas (input layer, hidden layer e output layer) e três saídas (ex. positivo, negativo e neutro). Para ir de uma camada para outra você pode usar uma matriz $W^{i}$ para propagar para a próxima camada. Portanto, chamamos esse conceito de ir da entrada até a camada final de propagação direta (forward propagation).\n",
    "\n",
    "Observe que adicionamos zeros para preenchimento para corresponder ao tamanho do tweet mais longo. Uma rede neural na configuração que você pode ver acima **só pode processar um tweet por vez**. Para tornar o treinamento mais eficiente (mais rápido), você deseja processar muitos tweets **em paralelo**. Você consegue isso juntando muitos tweets em uma matriz e depois passando essa matriz (em vez de tweets individuais) pela rede neural. Então a rede neural pode realizar seus cálculos em todos os tweets ao mesmo tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c62c828",
   "metadata": {},
   "source": [
    "## Dense Layers and ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338d1d74",
   "metadata": {},
   "source": [
    "As **camadas densas**, também conhecidas como camadas totalmente conectadas (Fully Connected Layers), são um tipo de camada em redes neurais onde **cada neurônio de uma camada está conectado a todos os neurônios da camada seguinte**. Essas conexões são ponderadas e ajustadas durante o processo de treinamento para aprender a mapear entradas para saídas desejadas. A camada Densa é o cálculo do produto interno entre um conjunto de pesos treináveis (matriz de pesos) e um vetor de entrada. Elas funcionam da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/dense_layer.png\">\n",
    "\n",
    "1. **Entrada:** Recebe um vetor de ativação da camada anterior.\n",
    "2. **Pesos e Bias:** Cada conexão tem um peso associado, e cada neurônio tem um valor de bias (viés).\n",
    "3. **Produto Ponto:** A camada calcula o produto ponto entre os vetores de entrada e os pesos, e soma o bias.\n",
    "4. **Função de Ativação:** O resultado é passado por uma função de ativação, que introduz não-linearidade no modelo, permitindo a aprendizagem de relações complexas.\n",
    "\n",
    "A **função de ativação ReLU** é uma das mais populares em redes neurais modernas devido à sua simplicidade e eficácia. ReLU é definida como:\n",
    "\n",
    "$$ f(x) = \\max(0, x) $$\n",
    "\n",
    "**Características:**\n",
    "1. **Linearidade por Partes:** ReLU mantém a linearidade para valores positivos, mas define todos os valores negativos como zero.\n",
    "2. **Não-Linearidade:** Introduz não-linearidade no modelo, essencial para aprender representações complexas.\n",
    "3. **Eficiente Computacionalmente:** Computacionalmente simples e eficiente de calcular.\n",
    "4. **Problema do Neurônio Morto:** Uma desvantagem potencial é que, se muitos neurônios saírem da faixa ativa (produzirem sempre zero), podem \"morrer\" e parar de aprender.\n",
    "\n",
    "<img src=\"./imgs/relu.png\">\n",
    "\n",
    "A caixa laranja na imagem acima mostra a camada densa. Uma camada de ativação é o conjunto de nós azuis mostrados com a caixa laranja na imagem abaixo. Concretamente, uma das camadas de ativação mais comumente utilizadas é a unidade linear retificada (ReLU).\n",
    "\n",
    "**Uso em Camadas Densas:**\n",
    "- **Aprimoramento da Expressividade:** A aplicação da ReLU em camadas densas permite que a rede aprenda representações mais ricas e capture relações não lineares nos dados.\n",
    "- **Mitigação do Desvanecimento de Gradiente:** ReLU ajuda a mitigar o problema do desvanecimento de gradiente, permitindo que gradientes maiores fluam durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b258a158",
   "metadata": {},
   "source": [
    "## Embedding and Mean Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c0c775",
   "metadata": {},
   "source": [
    "Usando uma camada de embedding, podemos aprender os embeddings para cada palavra do vocabulário da seguinte maneira:\n",
    "\n",
    "<img src=\"./imgs/embedding_layer.png\">\n",
    "\n",
    "A camada de embedding é uma camada especial utilizada em redes neurais para transformar palavras ou tokens em vetores densos de dimensão fixa, onde cada vetor captura informações semânticas sobre a palavra. Este processo é fundamental no processamento de linguagem natural (NLP) e permite que o modelo aprenda representações de palavras que refletem suas relações semânticas. Ele funciona da seguinte forma:\n",
    "\n",
    "1. **Entrada**: Recebe uma sequência de índices, onde cada índice corresponde a uma palavra ou token no vocabulário.\n",
    "2. **Lookup**: Cada índice é mapeado para um vetor denso, geralmente inicializado aleatoriamente e ajustado durante o treinamento.\n",
    "3. **Saída**: Produz uma matriz onde cada linha é o vetor de embedding correspondente a uma palavra na sequência de entrada.\n",
    "\n",
    "Essa camada possui os seguintes benefícios:\n",
    "\n",
    "1. **Dimensionalidade Reduzida**: Transforma palavras em vetores densos de menor dimensão, facilitando o processamento.\n",
    "2. **Captura de Semântica**: As palavras com significados semelhantes tendem a ter vetores próximos no espaço de embedding.\n",
    "3. **Treinável**: Os vetores de embedding são ajustados durante o treinamento para otimizar a tarefa específica, como classificação de texto ou tradução.\n",
    "\n",
    "A camada média (mean layer) permite tirar a média dos embeddings. Elas são usadas para **agregar informações** ao longo de uma sequência, calculando a média dos vetores de embedding ou das ativações ao longo da sequência. Esta técnica é simples e pode ser eficaz para **capturar uma representação global do contexto** ou do conteúdo de um texto. O vetor resultante da camada de média pode ser passado para camadas densas ou outras camadas de rede neural para realizar a classificação de sentimento, determinando se a frase expressa um sentimento positivo, negativo ou neutro. Fuciona da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/mean_layer.png\">\n",
    "\n",
    "1. **Entrada**: Recebe uma matriz de vetores de embedding ou ativações de uma camada anterior, onde cada linha corresponde a uma palavra ou token na sequência.\n",
    "2. **Cálculo da Média**: Calcula a média ao longo de uma dimensão específica (geralmente ao longo da dimensão da sequência).\n",
    "3. **Saída**: Produz um único vetor que representa a média das ativações ou embeddings ao longo da sequência.\n",
    "\n",
    "Possui os seguintes benefícios:\n",
    "\n",
    "1. **Simplicidade**: Fácil de implementar e computacionalmente eficiente.\n",
    "2. **Redução de Dimensionalidade**: Reduz a sequência de vetores a um único vetor, simplificando o processamento subsequente.\n",
    "3. **Representação Global**: Fornece uma representação global da sequência, capturando informações de todas as palavras ou tokens.\n",
    "\n",
    "Esta camada não possui nenhum parâmetro treinável."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58942fd1",
   "metadata": {},
   "source": [
    "## Traditional Language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f41c95f",
   "metadata": {},
   "source": [
    "Os modelos de idiomas tradicionais utilizam probabilidades para ajudar a identificar qual frase provavelmente ocorrerá.\n",
    "\n",
    "<img src=\"./imgs/trad_models1.png\">\n",
    "\n",
    "No exemplo acima, a segunda frase é a que provavelmente ocorrerá, pois tem a maior probabilidade de acontecer. Para calcular as probabilidades, você pode fazer o seguinte:\n",
    "\n",
    "<img src=\"./imgs/trad_models2.png\">\n",
    "\n",
    "Grandes gramas de N capturam dependências entre palavras distantes e **precisam de muito espaço e RAM**. Portanto, recorremos ao uso de diferentes tipos de alternativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490718b",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670884d",
   "metadata": {},
   "source": [
    "A frase seguinte é um exemplo de que os modelos probabilísticos não tem um bom desempenho, a exemplo, os modelos n-gram.\n",
    "\n",
    "<img src=\"./imgs/rnn1.png\">\n",
    "\n",
    "\n",
    "Um n-grama (trigrama) só olharia para \"did not\" e **tentaria concluir a frase a partir daí**, não veria o contexto nem as palavras anteriores. Como resultado, o modelo não poderá ver o início da frase \"I called her but she\". Provavelmente a palavra mais provável é \"have\" depois do \"did not\". Os RNNs nos ajudam a resolver esse problema, sendo capazes de **rastrear dependências muito mais longe uma da outra**. À medida que o RNN passa por um corpus de texto, ele capta algumas informações da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/rnn2.png\">\n",
    "\n",
    "Observe que, à medida que você alimenta mais informações no modelo, **a retenção da palavra anterior fica mais fraca, mas ainda está lá**. Olhe para o retângulo laranja acima e veja como ele se torna menor ao percorrer o texto. Isso mostra que seu modelo é capaz de capturar dependências e se lembra de uma palavra anterior, embora esteja no início de uma frase ou parágrafo. Outra vantagem dos RRNs é que muitos computações compartilham parâmetros. Ou seja, ao contrário dos modelos tradicionais de N-gramas, onde cada estado (ou cada grupo de palavras) tem seus próprios parâmetros independentes, as RNNs usam um conjunto fixo de parâmetros ao longo de toda a sequência de entrada.\n",
    "\n",
    "1. **Modelos Tradicionais de N-gramas**:\n",
    "   - Em modelos de N-gramas, os parâmetros são específicos para cada combinação de palavras de um tamanho fixo (N). Isso significa que o modelo precisa armazenar informações separadas para cada possível N-grama.\n",
    "   - Isso pode levar a um grande consumo de espaço e memória, especialmente com um vocabulário grande, pois a quantidade de N-gramas únicos pode ser enorme.\n",
    "\n",
    "2. **Redes Neurais Recorrentes (RNNs)**:\n",
    "   - As RNNs, por outro lado, processam a sequência de texto de forma iterativa, palavra por palavra, ou token por token.\n",
    "   - Uma RNN usa os mesmos parâmetros (pesos e bias) em cada passo da sequência. Esses parâmetros são aplicados repetidamente enquanto a RNN percorre a sequência de entrada.\n",
    "   - O compartilhamento de parâmetros ocorre porque a RNN aplica a mesma função de atualização (com os mesmos pesos) para processar cada palavra na sequência. Essa função leva em consideração tanto a palavra atual quanto o estado oculto anterior (que contém informações acumuladas das palavras anteriores).\n",
    "   \n",
    "**Benefícios do Compartilhamento de Parâmetros**\n",
    "\n",
    "- **Eficiência de Memória**: Como as RNNs não precisam armazenar parâmetros separados para cada combinação de palavras, elas são muito mais eficientes em termos de memória.\n",
    "- **Generalização**: O compartilhamento de parâmetros ajuda o modelo a generalizar melhor, pois ele aprende uma representação mais compacta e reutilizável das dependências na sequência de entrada.\n",
    "\n",
    "Imagine que você tenha uma sequência de palavras: \\[w_1, w_2, w_3, \\ldots, w_n\\]. \n",
    "\n",
    "- Em um N-grama de trigramas, você teria parâmetros específicos para cada trigrama possível, como (w_1, w_2, w_3), (w_2, w_3, w_4), etc.\n",
    "- Em uma RNN, você tem um conjunto fixo de parâmetros \\( \\theta \\) que são usados em cada passo para atualizar o estado oculto \\( h_t \\) com base na palavra atual \\( w_t \\) e o estado oculto anterior \\( h_{t-1} \\).\n",
    "\n",
    "Esse compartilhamento de parâmetros é uma das razões pelas quais as RNNs são eficazes em capturar dependências de longo prazo e são uma escolha popular para tarefas de processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a654ef",
   "metadata": {},
   "source": [
    "## Applications of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f32266",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T21:40:47.389758Z",
     "start_time": "2024-07-02T21:40:47.383693Z"
    }
   },
   "source": [
    "Redes Neurais Recorrentes (RNNs, do inglês Recurrent Neural Networks) são uma classe de redes neurais particularmente eficazes no processamento de dados sequenciais, como texto, áudio e séries temporais. Em NLP (Processamento de Linguagem Natural), as RNNs têm várias aplicações importantes devido à sua capacidade de capturar dependências temporais e contextuais em sequências de dados.\n",
    "\n",
    "| **Task**                        | **LSTM** | **GRU** | **BiRNN** |\n",
    "|----------------------------------|:--------:|:-------:|:---------:|\n",
    "| Modelagem de Linguagem           |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Tradução Automática              |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Análise de Sentimento            |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Reconhecimento de Entidades Nomeadas (NER) | ✔️ | ✔️ | ✔️ |\n",
    "| Classificação de Texto           |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Sumarização de Texto             |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Geração de Texto                 |    ✔️    |   ✔️    |    ✔️     |\n",
    "\n",
    "Arquiteturas Comuns de RNNs em NLP:\n",
    "- **LSTM:** Long Short-Term Memory é frequentemente usado para todas as tarefas listadas devido à sua capacidade de capturar dependências de longo prazo e gerenciar o problema do desvanecimento de gradiente.\n",
    "- **GRU:** Gated Recurrent Units são uma alternativa mais simples e computacionalmente eficiente às LSTMs, mas ainda muito eficazes para a maioria das tarefas de NLP.\n",
    "- **BiRNN:** Bidirectional RNNs processam a sequência de texto em ambas as direções, o que pode ser vantajoso para tarefas que dependem do contexto completo da sequência.\n",
    "\n",
    "Cada tipo de RNN pode ser usado para resolver essas tarefas de maneira eficaz, dependendo das necessidades específicas do problema e dos recursos computacionais disponíveis.\n",
    "\n",
    "Existem muitas maneiras de implementar um modelo RNN:\n",
    "\n",
    "- one to one: dadas algumas pontuações de um campeonato, você pode prever o vencedor.\n",
    "- one to many: dada uma imagem, você pode prever qual será a legenda.\n",
    "- many to one: Dado um tweet, você pode prever o sentimento desse tweet.\n",
    "- many to many: dada uma frase em inglês, você pode traduzi -la para seu equivalente alemão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b51d3e",
   "metadata": {},
   "source": [
    "## Math in Simple RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c71ac69",
   "metadata": {},
   "source": [
    "## Hidden State Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d735c",
   "metadata": {},
   "source": [
    "## Cost Function for RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc71bf0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:02:03.350599Z",
     "start_time": "2024-07-01T22:02:03.264743Z"
    }
   },
   "source": [
    "## Implementation Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e195f1",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91061a5",
   "metadata": {},
   "source": [
    "## Vanilla RNNs, GRUs and the scan function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096f6e3",
   "metadata": {},
   "source": [
    "## Deep and Bi-directional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56bebda",
   "metadata": {},
   "source": [
    "## Calculating Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fe19d6",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07c70b8",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d7f7e",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c80d5d3",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Classification and Vector Spaces, disponível em https://www.coursera.org/learn/probabilistic-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d13bb7",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
