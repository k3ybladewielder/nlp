{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e513a38d",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Sequence Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Sequence Models da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8196ba",
   "metadata": {},
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159b6178",
   "metadata": {},
   "source": [
    "## Neural Networks for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c38c4",
   "metadata": {},
   "source": [
    "Modelos mais simples são um ótimo baseline mas podem não capturar bem a o sentimento de uma frase. Nesses casos, podemos utilizar redes neurais.\n",
    "\n",
    "As redes neurais (Neural Networks) são um tipo de modelo de aprendizado de máquina inspirado na estrutura e no funcionamento do cérebro humano. Elas consistem em camadas de unidades interconectadas (neurônios) que processam informações e aprendem a realizar tarefas através de exemplos. As principais componentes das redes neurais são:\n",
    "\n",
    "1. **Camada de Entrada (Input Layer):** Recebe os dados brutos.\n",
    "2. **Camadas Ocultas (Hidden Layers):** Realizam processamento intermediário. Cada neurônio em uma camada oculta está conectado a neurônios da camada anterior e da camada seguinte, e aplica uma função de ativação para transformar a entrada.\n",
    "3. **Camada de Saída (Output Layer):** Produz o resultado final, como uma classificação ou previsão.\n",
    "\n",
    "A análise de sentimento envolve a **classificação de textos**, como comentários ou tweets, para determinar se expressam sentimentos positivos, negativos ou neutros. As redes neurais, especialmente as arquiteturas avançadas como LSTM (Long Short-Term Memory) e GRU (Gated Recurrent Units), são amplamente usadas nessa tarefa devido à sua capacidade de **capturar dependências de longo prazo em sequências de texto**.\n",
    "\n",
    "O processo de análise de sentimento com redes neurais segue os seguintes passos:\n",
    "\n",
    "1. **Pré-processamento:** O texto bruto é limpo e transformado em uma **representação numérica**, como vetores de palavras (Word Embeddings) ou **sequências de índices de palavras**.\n",
    "2. **Arquitetura do Modelo:**\n",
    "   - **Embedding Layer:** Transforma as palavras em vetores densos que capturam seus significados.\n",
    "   - **Camadas Recurrentes (LSTM/GRU):** Processam a sequência de vetores, mantendo informações contextuais ao longo do tempo.\n",
    "   - **Camadas Densas (Fully Connected):** Agregam informações das camadas anteriores e produzem a probabilidade de cada classe de sentimento (positivo, negativo, neutro).\n",
    "3. **Treinamento:** O modelo é treinado em um conjunto de dados rotulados, ajustando os pesos das conexões entre neurônios para minimizar o erro na classificação.\n",
    "4. **Avaliação e Ajuste:** O desempenho do modelo é avaliado em dados de validação, e ajustes são feitos para melhorar a precisão.\n",
    "5. **Predição:** O modelo treinado é usado para analisar novos textos e prever o sentimento.\n",
    "\n",
    "As redes neurais permitem uma análise de sentimento mais robusta, capturando nuances e contextos que métodos mais simples podem perder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c814407b",
   "metadata": {},
   "source": [
    "Os dados de entrada como já mencionados são **representações numéricas** de palavras, como as ilustradas a seguir\n",
    "\n",
    "<img src=\"./imgs/word_representation.png\">\n",
    "\n",
    "<img src=\"./imgs/forward_propagation.png\">\n",
    "\n",
    "Observe que a rede acima possui três camadas (input layer, hidden layer e output layer) e três saídas (ex. positivo, negativo e neutro). Para ir de uma camada para outra você pode usar uma matriz $W^{i}$ para propagar para a próxima camada. Portanto, chamamos esse conceito de ir da entrada até a camada final de propagação direta (forward propagation).\n",
    "\n",
    "Observe que adicionamos zeros para preenchimento para corresponder ao tamanho do tweet mais longo. Uma rede neural na configuração que você pode ver acima **só pode processar um tweet por vez**. Para tornar o treinamento mais eficiente (mais rápido), você deseja processar muitos tweets **em paralelo**. Você consegue isso juntando muitos tweets em uma matriz e depois passando essa matriz (em vez de tweets individuais) pela rede neural. Então a rede neural pode realizar seus cálculos em todos os tweets ao mesmo tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7929b2",
   "metadata": {},
   "source": [
    "## Dense Layers and ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4594008a",
   "metadata": {},
   "source": [
    "As **camadas densas**, também conhecidas como camadas totalmente conectadas (Fully Connected Layers), são um tipo de camada em redes neurais onde **cada neurônio de uma camada está conectado a todos os neurônios da camada seguinte**. Essas conexões são ponderadas e ajustadas durante o processo de treinamento para aprender a mapear entradas para saídas desejadas. A camada Densa é o cálculo do produto interno entre um conjunto de pesos treináveis (matriz de pesos) e um vetor de entrada. Elas funcionam da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/dense_layer.png\">\n",
    "\n",
    "1. **Entrada:** Recebe um vetor de ativação da camada anterior.\n",
    "2. **Pesos e Bias:** Cada conexão tem um peso associado, e cada neurônio tem um valor de bias (viés).\n",
    "3. **Produto Ponto:** A camada calcula o produto ponto entre os vetores de entrada e os pesos, e soma o bias.\n",
    "4. **Função de Ativação:** O resultado é passado por uma função de ativação, que introduz não-linearidade no modelo, permitindo a aprendizagem de relações complexas.\n",
    "\n",
    "A **função de ativação ReLU** é uma das mais populares em redes neurais modernas devido à sua simplicidade e eficácia. ReLU é definida como:\n",
    "\n",
    "$$ f(x) = \\max(0, x) $$\n",
    "\n",
    "**Características:**\n",
    "1. **Linearidade por Partes:** ReLU mantém a linearidade para valores positivos, mas define todos os valores negativos como zero.\n",
    "2. **Não-Linearidade:** Introduz não-linearidade no modelo, essencial para aprender representações complexas.\n",
    "3. **Eficiente Computacionalmente:** Computacionalmente simples e eficiente de calcular.\n",
    "4. **Problema do Neurônio Morto:** Uma desvantagem potencial é que, se muitos neurônios saírem da faixa ativa (produzirem sempre zero), podem \"morrer\" e parar de aprender.\n",
    "\n",
    "<img src=\"./imgs/relu.png\">\n",
    "\n",
    "A caixa laranja na imagem acima mostra a camada densa. Uma camada de ativação é o conjunto de nós azuis mostrados com a caixa laranja na imagem abaixo. Concretamente, uma das camadas de ativação mais comumente utilizadas é a unidade linear retificada (ReLU).\n",
    "\n",
    "**Uso em Camadas Densas:**\n",
    "- **Aprimoramento da Expressividade:** A aplicação da ReLU em camadas densas permite que a rede aprenda representações mais ricas e capture relações não lineares nos dados.\n",
    "- **Mitigação do Desvanecimento de Gradiente:** ReLU ajuda a mitigar o problema do desvanecimento de gradiente, permitindo que gradientes maiores fluam durante o treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294fac7c",
   "metadata": {},
   "source": [
    "## Embedding and Mean Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1db5e",
   "metadata": {},
   "source": [
    "Usando uma camada de embedding, podemos aprender os embeddings para cada palavra do vocabulário da seguinte maneira:\n",
    "\n",
    "<img src=\"./imgs/embedding_layer.png\">\n",
    "\n",
    "A camada de embedding é uma camada especial utilizada em redes neurais para transformar palavras ou tokens em vetores densos de dimensão fixa, onde cada vetor captura informações semânticas sobre a palavra. Este processo é fundamental no processamento de linguagem natural (NLP) e permite que o modelo aprenda representações de palavras que refletem suas relações semânticas. Ele funciona da seguinte forma:\n",
    "\n",
    "1. **Entrada**: Recebe uma sequência de índices, onde cada índice corresponde a uma palavra ou token no vocabulário.\n",
    "2. **Lookup**: Cada índice é mapeado para um vetor denso, geralmente inicializado aleatoriamente e ajustado durante o treinamento.\n",
    "3. **Saída**: Produz uma matriz onde cada linha é o vetor de embedding correspondente a uma palavra na sequência de entrada.\n",
    "\n",
    "Essa camada possui os seguintes benefícios:\n",
    "\n",
    "1. **Dimensionalidade Reduzida**: Transforma palavras em vetores densos de menor dimensão, facilitando o processamento.\n",
    "2. **Captura de Semântica**: As palavras com significados semelhantes tendem a ter vetores próximos no espaço de embedding.\n",
    "3. **Treinável**: Os vetores de embedding são ajustados durante o treinamento para otimizar a tarefa específica, como classificação de texto ou tradução.\n",
    "\n",
    "A camada média (mean layer) permite tirar a média dos embeddings. Elas são usadas para **agregar informações** ao longo de uma sequência, calculando a média dos vetores de embedding ou das ativações ao longo da sequência. Esta técnica é simples e pode ser eficaz para **capturar uma representação global do contexto** ou do conteúdo de um texto. O vetor resultante da camada de média pode ser passado para camadas densas ou outras camadas de rede neural para realizar a classificação de sentimento, determinando se a frase expressa um sentimento positivo, negativo ou neutro. Fuciona da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/mean_layer.png\">\n",
    "\n",
    "1. **Entrada**: Recebe uma matriz de vetores de embedding ou ativações de uma camada anterior, onde cada linha corresponde a uma palavra ou token na sequência.\n",
    "2. **Cálculo da Média**: Calcula a média ao longo de uma dimensão específica (geralmente ao longo da dimensão da sequência).\n",
    "3. **Saída**: Produz um único vetor que representa a média das ativações ou embeddings ao longo da sequência.\n",
    "\n",
    "Possui os seguintes benefícios:\n",
    "\n",
    "1. **Simplicidade**: Fácil de implementar e computacionalmente eficiente.\n",
    "2. **Redução de Dimensionalidade**: Reduz a sequência de vetores a um único vetor, simplificando o processamento subsequente.\n",
    "3. **Representação Global**: Fornece uma representação global da sequência, capturando informações de todas as palavras ou tokens.\n",
    "\n",
    "Esta camada não possui nenhum parâmetro treinável."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b87733",
   "metadata": {},
   "source": [
    "## Traditional Language models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49789122",
   "metadata": {},
   "source": [
    "Os modelos de idiomas tradicionais utilizam probabilidades para ajudar a identificar qual frase provavelmente ocorrerá.\n",
    "\n",
    "<img src=\"./imgs/trad_models1.png\">\n",
    "\n",
    "No exemplo acima, a segunda frase é a que provavelmente ocorrerá, pois tem a maior probabilidade de acontecer. Para calcular as probabilidades, você pode fazer o seguinte:\n",
    "\n",
    "<img src=\"./imgs/trad_models2.png\">\n",
    "\n",
    "Grandes gramas de N capturam dependências entre palavras distantes e **precisam de muito espaço e RAM**. Portanto, recorremos ao uso de diferentes tipos de alternativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8895eb7a",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef887abc",
   "metadata": {},
   "source": [
    "A frase seguinte é um exemplo de que os modelos probabilísticos não tem um bom desempenho, a exemplo, os modelos n-gram.\n",
    "\n",
    "<img src=\"./imgs/rnn1.png\">\n",
    "\n",
    "\n",
    "Um n-grama (trigrama) só olharia para \"did not\" e **tentaria concluir a frase a partir daí**, não veria o contexto nem as palavras anteriores. Como resultado, o modelo não poderá ver o início da frase \"I called her but she\". Provavelmente a palavra mais provável é \"have\" depois do \"did not\". Os RNNs nos ajudam a resolver esse problema, sendo capazes de **rastrear dependências muito mais longe uma da outra**. À medida que o RNN passa por um corpus de texto, ele capta algumas informações da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/rnn2.png\">\n",
    "\n",
    "Observe que, à medida que você alimenta mais informações no modelo, **a retenção da palavra anterior fica mais fraca, mas ainda está lá**. Olhe para o retângulo laranja acima e veja como ele se torna menor ao percorrer o texto. Isso mostra que seu modelo é capaz de capturar dependências e se lembra de uma palavra anterior, embora esteja no início de uma frase ou parágrafo. Outra vantagem dos RRNs é que muitos computações compartilham parâmetros. Ou seja, ao contrário dos modelos tradicionais de N-gramas, onde cada estado (ou cada grupo de palavras) tem seus próprios parâmetros independentes, as RNNs usam um conjunto fixo de parâmetros ao longo de toda a sequência de entrada.\n",
    "\n",
    "1. **Modelos Tradicionais de N-gramas**:\n",
    "   - Em modelos de N-gramas, os parâmetros são específicos para cada combinação de palavras de um tamanho fixo (N). Isso significa que o modelo precisa armazenar informações separadas para cada possível N-grama.\n",
    "   - Isso pode levar a um grande consumo de espaço e memória, especialmente com um vocabulário grande, pois a quantidade de N-gramas únicos pode ser enorme.\n",
    "\n",
    "2. **Redes Neurais Recorrentes (RNNs)**:\n",
    "   - As RNNs, por outro lado, processam a sequência de texto de forma iterativa, palavra por palavra, ou token por token.\n",
    "   - Uma RNN usa os mesmos parâmetros (pesos e bias) em cada passo da sequência. Esses parâmetros são aplicados repetidamente enquanto a RNN percorre a sequência de entrada.\n",
    "   - O compartilhamento de parâmetros ocorre porque a RNN aplica a mesma função de atualização (com os mesmos pesos) para processar cada palavra na sequência. Essa função leva em consideração tanto a palavra atual quanto o estado oculto anterior (que contém informações acumuladas das palavras anteriores).\n",
    "   \n",
    "**Benefícios do Compartilhamento de Parâmetros**\n",
    "\n",
    "- **Eficiência de Memória**: Como as RNNs não precisam armazenar parâmetros separados para cada combinação de palavras, elas são muito mais eficientes em termos de memória.\n",
    "- **Generalização**: O compartilhamento de parâmetros ajuda o modelo a generalizar melhor, pois ele aprende uma representação mais compacta e reutilizável das dependências na sequência de entrada.\n",
    "\n",
    "Imagine que você tenha uma sequência de palavras: \\[w_1, w_2, w_3, \\ldots, w_n\\]. \n",
    "\n",
    "- Em um N-grama de trigramas, você teria parâmetros específicos para cada trigrama possível, como (w_1, w_2, w_3), (w_2, w_3, w_4), etc.\n",
    "- Em uma RNN, você tem um conjunto fixo de parâmetros \\( \\theta \\) que são usados em cada passo para atualizar o estado oculto \\( h_t \\) com base na palavra atual \\( w_t \\) e o estado oculto anterior \\( h_{t-1} \\).\n",
    "\n",
    "Esse compartilhamento de parâmetros é uma das razões pelas quais as RNNs são eficazes em capturar dependências de longo prazo e são uma escolha popular para tarefas de processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e455d11",
   "metadata": {},
   "source": [
    "## Applications of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b21da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-02T21:40:47.389758Z",
     "start_time": "2024-07-02T21:40:47.383693Z"
    }
   },
   "source": [
    "Redes Neurais Recorrentes (RNNs, do inglês Recurrent Neural Networks) são uma classe de redes neurais particularmente eficazes no processamento de dados sequenciais, como texto, áudio e séries temporais. Em NLP (Processamento de Linguagem Natural), as RNNs têm várias aplicações importantes devido à sua capacidade de capturar dependências temporais e contextuais em sequências de dados.\n",
    "\n",
    "| **Task**                        | **LSTM** | **GRU** | **BiRNN** |\n",
    "|----------------------------------|:--------:|:-------:|:---------:|\n",
    "| Modelagem de Linguagem           |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Tradução Automática              |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Análise de Sentimento            |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Reconhecimento de Entidades Nomeadas (NER) | ✔️ | ✔️ | ✔️ |\n",
    "| Classificação de Texto           |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Sumarização de Texto             |    ✔️    |   ✔️    |    ✔️     |\n",
    "| Geração de Texto                 |    ✔️    |   ✔️    |    ✔️     |\n",
    "\n",
    "Arquiteturas Comuns de RNNs em NLP:\n",
    "- **LSTM:** Long Short-Term Memory é frequentemente usado para todas as tarefas listadas devido à sua capacidade de capturar dependências de longo prazo e gerenciar o problema do desvanecimento de gradiente.\n",
    "- **GRU:** Gated Recurrent Units são uma alternativa mais simples e computacionalmente eficiente às LSTMs, mas ainda muito eficazes para a maioria das tarefas de NLP.\n",
    "- **BiRNN:** Bidirectional RNNs processam a sequência de texto em ambas as direções, o que pode ser vantajoso para tarefas que dependem do contexto completo da sequência.\n",
    "\n",
    "Cada tipo de RNN pode ser usado para resolver essas tarefas de maneira eficaz, dependendo das necessidades específicas do problema e dos recursos computacionais disponíveis.\n",
    "\n",
    "Existem muitas maneiras de implementar um modelo RNN:\n",
    "\n",
    "- one to one: dadas algumas pontuações de um campeonato, você pode prever o vencedor.\n",
    "- one to many: dada uma imagem, você pode prever qual será a legenda.\n",
    "- many to one: Dado um tweet, você pode prever o sentimento desse tweet.\n",
    "- many to many: dada uma frase em inglês, você pode traduzi -la para seu equivalente alemão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d33fe2",
   "metadata": {},
   "source": [
    "## Math in Simple RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4422eb",
   "metadata": {},
   "source": [
    "Para entender como funciona uma RNN simples (vanilla RNN), vamos explorar a matemática por trás dela usando uma representação gráfica e equações.\n",
    "\n",
    "### Conceitos e Notação\n",
    "\n",
    "1. **Estado Oculto $ h^{<t>} $:**\n",
    "   - Representa a memória da RNN no tempo $ t $.\n",
    "   - Atualizado em cada passo de tempo com base no estado oculto anterior $ h^{<t-1>} $ e na entrada atual $ x^{<t>} $.\n",
    "\n",
    "2. **Função de Ativação $ g $:**\n",
    "   - Pode ser uma função não-linear como a tangente hiperbólica ($\\tanh$) ou ReLU (Rectified Linear Unit).\n",
    "\n",
    "3. **Pesos e Biases:**\n",
    "   - $ W_{hh} $: Pesos que conectam o estado oculto anterior ao novo estado oculto.\n",
    "   - $ W_{hx} $: Pesos que conectam a entrada atual ao novo estado oculto.\n",
    "   - $ W_{yh} $: Pesos que conectam o estado oculto à saída.\n",
    "   - $ b_h $: Bias do estado oculto.\n",
    "   - $ b_y $: Bias da saída.\n",
    "\n",
    "### Atualização do Estado Oculto\n",
    "\n",
    "A fórmula geral para atualizar o estado oculto em um vanilla RNN é:\n",
    "$$ h^{<t>} = g(W_h [h^{<t-1>}, x^{<t>}] + b_h) $$\n",
    "\n",
    "Esta fórmula pode ser detalhada em componentes individuais:\n",
    "$$ h^{<t>} = g(W_{hh} h^{<t-1>} + W_{hx} x^{<t>} + b_h) $$\n",
    "\n",
    "Aqui:\n",
    "- $ W_{hh} h^{<t-1>} $: Multiplicação dos pesos $ W_{hh} $ pelo estado oculto anterior $ h^{<t-1>} $.\n",
    "- $ W_{hx} x^{<t>} $: Multiplicação dos pesos $ W_{hx} $ pela entrada atual $ x^{<t>} $.\n",
    "- $ b_h $: Adição do bias $ b_h $.\n",
    "\n",
    "### Concatenando Entradas e Estados Ocultos\n",
    "\n",
    "Para simplificar a notação, podemos concatenar o estado oculto anterior e a entrada atual em um único vetor:\n",
    "$$ h^{<t>} = g(W_h [h^{<t-1>}, x^{<t>}] + b_h) $$\n",
    "\n",
    "Isso pode ser reescrito como:\n",
    "$$ h^{<t>} = g(W_{hh} h^{<t-1>} \\oplus W_{hx} x^{<t>} + b_h) $$\n",
    "\n",
    "Aqui, $\\oplus$ indica a concatenação dos produtos $ W_{hh} h^{<t-1>} $ e $ W_{hx} x^{<t>} $.\n",
    "\n",
    "<img src=\"./imgs/rnn3.png\">\n",
    "\n",
    "### Previsão em Cada Passo de Tempo\n",
    "\n",
    "Para gerar uma previsão ou saída em cada passo de tempo, usamos o estado oculto atualizado:\n",
    "$$ \\hat{y}^{<t>} = g(W_{yh} h^{<t>} + b_y) $$\n",
    "\n",
    "Aqui:\n",
    "- $ W_{yh} h^{<t>} $: Multiplicação dos pesos $ W_{yh} $ pelo estado oculto atual $ h^{<t>} $.\n",
    "- $ b_y $: Adição do bias $ b_y $.\n",
    "\n",
    "### Treinamento\n",
    "\n",
    "Durante o treinamento da RNN, ajustamos os seguintes parâmetros:\n",
    "- $ W_{hh} $: Pesos conectando estados ocultos.\n",
    "- $ W_{hx} $: Pesos conectando entradas às unidades ocultas.\n",
    "- $ W_{yh} $: Pesos conectando estados ocultos às saídas.\n",
    "- $ b_h $: Bias das unidades ocultas.\n",
    "- $ b_y $: Bias das saídas.\n",
    "\n",
    "<img src=\"./imgs/rnn4.png\">\n",
    "\n",
    "### Visualização do Modelo\n",
    "\n",
    "Aqui está uma visualização simplificada do modelo vanilla RNN:\n",
    "\n",
    "```\n",
    "x^{<1>} ---> [ Unidade RNN ] ---> h^{<1>}\n",
    "           |                |\n",
    "x^{<2>} ---> [ Unidade RNN ] ---> h^{<2>}\n",
    "           |                |\n",
    "x^{<3>} ---> [ Unidade RNN ] ---> h^{<3>}\n",
    "            ...\n",
    "```\n",
    "\n",
    "Para cada passo de tempo $ t $:\n",
    "- A entrada $ x^{<t>} $ e o estado oculto anterior $ h^{<t-1>} $ são usados para calcular o novo estado oculto $ h^{<t>} $.\n",
    "- O novo estado oculto $ h^{<t>} $ é usado para gerar a saída $ \\hat{y}^{<t>} $.\n",
    "\n",
    "Este processo continua ao longo de toda a sequência, permitindo que a RNN capture dependências temporais nos dados.\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- **Estado Oculto:** Memória que armazena informações sobre a sequência até o ponto atual.\n",
    "- **Função de Ativação:** Introduz não-linearidade, permitindo que a RNN capture relações complexas.\n",
    "- **Pesos e Biases:** Parâmetros treináveis que conectam entradas, estados ocultos e saídas.\n",
    "- **Atualização Recorrente:** O estado oculto é atualizado a cada passo de tempo com base na entrada atual e no estado oculto anterior.\n",
    "- **Previsão:** As saídas são geradas com base no estado oculto atualizado em cada passo de tempo.\n",
    "\n",
    "As vanilla RNNs são poderosas para capturar dependências em dados sequenciais, mas têm limitações, como o desvanecimento de gradiente, que são abordadas em variantes mais avançadas como LSTMs e GRUs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb5109f",
   "metadata": {},
   "source": [
    "Uma Rede Neural Recorrente (RNN) \"vanilla\" é uma arquitetura básica de RNN que processa sequências de dados, um elemento por vez, mantendo um estado oculto que armazena informações sobre elementos anteriores da sequência. Isso permite que o modelo capture dependências temporais ou sequenciais nos dados. Vamos explorar o funcionamento de um vanilla RNN de forma aprofundada e intuitiva.\n",
    "\n",
    "### Estrutura de uma Vanilla RNN\n",
    "\n",
    "1. **Entrada:** \n",
    "   - Uma sequência de dados, como uma frase, onde cada palavra é convertida em uma representação numérica, por exemplo, um vetor de embedding.\n",
    "\n",
    "2. **Estado Oculto (Hidden State):**\n",
    "   - Um vetor que armazena informações sobre a sequência até o ponto atual. Este vetor é atualizado em cada passo da sequência.\n",
    "\n",
    "3. **Unidade Recurrente:**\n",
    "   - A função que define como o estado oculto é atualizado em cada passo da sequência.\n",
    "\n",
    "4. **Saída:**\n",
    "   - Uma previsão ou transformação do estado oculto, que pode ser usada para diversas tarefas, como classificação de texto ou previsão da próxima palavra na sequência.\n",
    "\n",
    "### Funcionamento Passo a Passo\n",
    "\n",
    "#### Passo 1: Inicialização\n",
    "- O estado oculto inicial $\\mathbf{h}_0$ é geralmente um vetor de zeros ou pode ser inicializado aleatoriamente.\n",
    "\n",
    "#### Passo 2: Processamento da Sequência\n",
    "Para cada elemento $x_t$ na sequência de entrada ($x_1, x_2, \\ldots, x_T$):\n",
    "1. **Entrada Atual ($x_t$):** O vetor de entrada no tempo $t$.\n",
    "2. **Estado Oculto Anterior ($\\mathbf{h}_{t-1}$):** O estado oculto do passo anterior.\n",
    "\n",
    "3. **Atualização do Estado Oculto:**\n",
    "   - O estado oculto atual $\\mathbf{h}_t$ é calculado usando a unidade recurrente.\n",
    "   - A fórmula típica é:\n",
    "     $$\n",
    "     \\mathbf{h}_t = \\tanh(\\mathbf{W}_{xh} \\cdot x_t + \\mathbf{W}_{hh} \\cdot \\mathbf{h}_{t-1} + \\mathbf{b}_h)\n",
    "     $$\n",
    "     - $\\mathbf{W}_{xh}$: Matriz de pesos que conecta a entrada ao estado oculto.\n",
    "     - $\\mathbf{W}_{hh}$: Matriz de pesos que conecta o estado oculto anterior ao novo estado oculto.\n",
    "     - $\\mathbf{b}_h$: Vetor de bias.\n",
    "     - $\\tanh$: Função de ativação não-linear (tangente hiperbólica) que ajuda a manter os valores do estado oculto dentro de um intervalo fixo.\n",
    "\n",
    "4. **Saída do Passo Atual:**\n",
    "   - A saída do modelo no tempo $t$ ($y_t$) pode ser calculada a partir do estado oculto atual.\n",
    "     $$\n",
    "     y_t = \\mathbf{W}_{hy} \\cdot \\mathbf{h}_t + \\mathbf{b}_y\n",
    "     $$\n",
    "     - $\\mathbf{W}_{hy}$: Matriz de pesos que conecta o estado oculto à saída.\n",
    "     - $\\mathbf{b}_y$: Vetor de bias para a saída.\n",
    "\n",
    "#### Passo 3: Processamento da Sequência Completa\n",
    "- Este processo é repetido para cada elemento da sequência, atualizando o estado oculto em cada passo e gerando uma saída correspondente.\n",
    "\n",
    "### Intuição sobre o Estado Oculto\n",
    "- O estado oculto pode ser visto como a \"memória\" da RNN. Ele carrega informações sobre elementos anteriores da sequência e influencia como os próximos elementos são processados.\n",
    "- Em cada passo, o estado oculto é uma combinação linear da entrada atual e do estado oculto anterior, transformado por uma função não-linear (tanh). Isso permite que o modelo capture tanto informações novas quanto contextuais.\n",
    "\n",
    "### Limitações das Vanilla RNNs\n",
    "- **Desvanecimento e Explosão do Gradiente:** Durante o treinamento, os gradientes podem se tornar extremamente pequenos (desvanecimento) ou extremamente grandes (explosão), dificultando o aprendizado de dependências de longo prazo.\n",
    "- **Dependências de Longo Prazo:** Vanilla RNNs podem ter dificuldades para capturar dependências que ocorrem em grandes intervalos de tempo dentro da sequência.\n",
    "\n",
    "### Conclusão\n",
    "Vanilla RNNs são modelos poderosos para processar dados sequenciais, mantendo uma memória dinâmica da sequência através do estado oculto. No entanto, suas limitações levaram ao desenvolvimento de variantes mais sofisticadas, como LSTMs e GRUs, que abordam problemas como o desvanecimento do gradiente e melhoram a capacidade de capturar dependências de longo prazo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56c4121",
   "metadata": {},
   "source": [
    "## Cost Function for RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e660e0",
   "metadata": {},
   "source": [
    "A função de custo utilizada nas RNNs é a **cross entropy loss**\n",
    "\n",
    "<img src=\"./imgs/cross_entropy_loss.png\">\n",
    "\n",
    "### Entropia Cruzada\n",
    "\n",
    "A perda por entropia cruzada mede a diferença entre a distribuição de probabilidade verdadeira (ou alvo) e a distribuição de probabilidade prevista pelo modelo. Para cada classe $ j $ em uma tarefa de classificação com $ K $ classes, a entropia cruzada é calculada como:\n",
    "\n",
    "$$ \\text{Loss} = - \\sum_{j=1}^{K} y_j \\log(\\hat{y}_j) $$\n",
    "\n",
    "Aqui:\n",
    "- $ y_j $ é o valor real da classe $ j $ (1 se a classe é a correta, 0 caso contrário).\n",
    "- $ \\hat{y}_j $ é a probabilidade prevista pelo modelo para a classe $ j $.\n",
    "\n",
    "### Função de Custo em RNNs\n",
    "\n",
    "Quando se trabalha com RNNs, estamos lidando com sequências de dados, e a previsão é feita em cada passo de tempo $ t $. Portanto, precisamos calcular a perda para cada passo de tempo e depois somá-las para obter a perda total da sequência.\n",
    "\n",
    "A fórmula geral para a função de custo $ J $ ao longo de $ T $ passos de tempo é:\n",
    "\n",
    "$$ J = -\\frac{1}{T} \\sum_{t=1}^{T} \\sum_{j=1}^{K} y_j^{<t>} \\log(\\hat{y}_j^{<t>}) $$\n",
    "\n",
    "Aqui:\n",
    "- $ T $ é o número total de passos de tempo na sequência.\n",
    "- $ K $ é o número total de classes.\n",
    "- $ y_j^{<t>} $ é o valor real para a classe $ j $ no tempo $ t $.\n",
    "- $ \\hat{y}_j^{<t>} $ é a probabilidade prevista pelo modelo para a classe $ j $ no tempo $ t $.\n",
    "\n",
    "### Intuição\n",
    "\n",
    "1. **Soma sobre as Classes:** Para cada passo de tempo $ t $, calculamos a entropia cruzada para todas as classes $ j $. Isso mede o quão bem o modelo previu a distribuição de probabilidade para o passo de tempo específico.\n",
    "\n",
    "2. **Soma sobre os Passos de Tempo:** Depois, somamos a perda de todos os passos de tempo. Isso nos dá a perda total para toda a sequência.\n",
    "\n",
    "3. **Média ao Longo do Tempo:** Dividimos pela quantidade total de passos de tempo $ T $ para obter a média da perda por passo de tempo. Isso é importante porque nos dá uma noção da performance do modelo em média, ao longo de toda a sequência.\n",
    "\n",
    "### Visualização\n",
    "\n",
    "Imagine que temos uma sequência com 3 passos de tempo (T = 3) e 4 classes (K = 4). Para cada passo de tempo $ t $, temos as previsões do modelo $\\hat{y}^{<t>}$ e os valores reais $ y^{<t>} $.\n",
    "\n",
    "1. Para cada $ t $ (passo de tempo):\n",
    "   - Calculamos a entropia cruzada entre $\\hat{y}^{<t>}$ e $ y^{<t>} $.\n",
    "   - Fazemos isso somando $ y_j^{<t>} \\log(\\hat{y}_j^{<t>}) $ para todas as classes $ j $.\n",
    "\n",
    "2. Somamos essas perdas para todos os passos de tempo $ t = 1, 2, 3 $.\n",
    "\n",
    "3. Dividimos a soma total pelo número de passos de tempo $ T $ para obter a média.\n",
    "\n",
    "### Exemplo de Cálculo\n",
    "\n",
    "Suponha que temos as seguintes probabilidades previstas $\\hat{y}$ e valores reais $ y $ para uma sequência de 3 passos de tempo e 2 classes:\n",
    "\n",
    "- Tempo 1: $ \\hat{y}^{<1>} = [0.7, 0.3] $, $ y^{<1>} = [1, 0] $\n",
    "- Tempo 2: $ \\hat{y}^{<2>} = [0.4, 0.6] $, $ y^{<2>} = [0, 1] $\n",
    "- Tempo 3: $ \\hat{y}^{<3>} = [0.9, 0.1] $, $ y^{<3>} = [1, 0] $\n",
    "\n",
    "Para cada passo de tempo, calculamos a entropia cruzada:\n",
    "\n",
    "- Tempo 1: $ \\text{Loss}^{<1>} = -(1 \\log(0.7) + 0 \\log(0.3)) = 0.3567 $\n",
    "- Tempo 2: $ \\text{Loss}^{<2>} = -(0 \\log(0.4) + 1 \\log(0.6)) = 0.5108 $\n",
    "- Tempo 3: $ \\text{Loss}^{<3>} = -(1 \\log(0.9) + 0 \\log(0.1)) = 0.1054 $\n",
    "\n",
    "Somamos as perdas:\n",
    "\n",
    "$$ \\text{Total Loss} = 0.3567 + 0.5108 + 0.1054 = 0.9729 $$\n",
    "\n",
    "Calculamos a média:\n",
    "\n",
    "$$ J = \\frac{\\text{Total Loss}}{T} = \\frac{0.9729}{3} = 0.3243 $$\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- A função de custo em RNNs é a perda por entropia cruzada calculada para cada passo de tempo.\n",
    "- A entropia cruzada mede a diferença entre as distribuições de probabilidade previstas e reais.\n",
    "- A perda total é a soma das perdas em cada passo de tempo, dividida pelo número total de passos, resultando na média da perda por passo de tempo.\n",
    "- Esse processo permite que o modelo aprenda a prever melhor as sequências de dados ao longo do tempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80eaf30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-01T22:02:03.350599Z",
     "start_time": "2024-07-01T22:02:03.264743Z"
    }
   },
   "source": [
    "## Implementation Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08001f57",
   "metadata": {},
   "source": [
    "A função scan (abstração do rnn) é construída da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/rnn_implementation.png\">\n",
    "\n",
    "Observe que isso é basicamente o que um RNN está fazendo. Ele pega o inicializador e retorna uma lista de saídas (ys) e usa o valor atual para obter o próximo y e o próximo valor atual. Esse tipo de abstração permite uma computação muito mais rápida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b42a1",
   "metadata": {},
   "source": [
    "## Gated Recurrent Units"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e92c45",
   "metadata": {},
   "source": [
    "As **unidades recorrentes com portas (GRUs)** são uma variante dos RNNs que introduzem mecanismos adicionais, chamados de portas, para melhor gerenciar o fluxo de informações ao longo do tempo. As GRUs possuem duas portas principais: a **porta de atualização (update)** e a **porta de relevância (relevance ou reset)**. Estas portas ajudam a **determinar o que deve ser lembrado ou esquecido no estado oculto**, permitindo que o modelo mantenha informações relevantes por longos períodos e melhore a captura de dependências de longo prazo.\n",
    "\n",
    "\n",
    "<img src=\"./imgs/vanilla_rnn_vs_gru.png\">\n",
    "\n",
    "\n",
    "### Estrutura das GRUs\n",
    "\n",
    "1. **Porta de Atualização ($\\Gamma_u$)**:\n",
    "   - Determina quanto do estado oculto anterior deve ser levado para frente.\n",
    "   - Calculada como:\n",
    "     $$\n",
    "     \\Gamma_u = \\sigma(W_u [h^{<t-1>}, x^{<t>}] + b_u)\n",
    "     $$\n",
    "   - Aqui, $\\sigma$ é a função sigmoid que gera valores entre 0 e 1.\n",
    "\n",
    "2. **Porta de Relevância ($\\Gamma_r$)**:\n",
    "   - Decide quanta parte do estado oculto anterior deve ser esquecida (ou reiniciada).\n",
    "   - Calculada como:\n",
    "     $$\n",
    "     \\Gamma_r = \\sigma(W_r [h^{<t-1>}, x^{<t>}] + b_r)\n",
    "     $$\n",
    "   - A função sigmoid também é usada aqui para limitar os valores entre 0 e 1.\n",
    "\n",
    "3. **Novo Estado Candidato ($h'^{<t>}$)**:\n",
    "   - É a nova memória candidata que poderia ser adicionada ao estado oculto.\n",
    "   - Calculado como:\n",
    "     $$\n",
    "     h'^{<t>} = \\tanh(W_h [\\Gamma_r * h^{<t-1>}, x^{<t>}] + b_h)\n",
    "     $$\n",
    "   - Aqui, a função tangente hiperbólica ($\\tanh$) é usada para permitir valores entre -1 e 1, e $\\Gamma_r * h^{<t-1>}$ indica a multiplicação elemento a elemento entre a porta de relevância e o estado oculto anterior.\n",
    "\n",
    "4. **Estado Oculto Atualizado ($h^{<t>}$)**:\n",
    "   - Calculado combinando o estado oculto anterior e o novo estado candidato, ponderados pela porta de atualização.\n",
    "   - A fórmula é:\n",
    "     $$\n",
    "     h^{<t>} = (1 - \\Gamma_u) * h^{<t-1>} + \\Gamma_u * h'^{<t>}\n",
    "     $$\n",
    "   - Isso significa que o novo estado oculto é uma média ponderada entre o estado oculto anterior (controlado por $1 - \\Gamma_u$) e o novo estado candidato (controlado por $\\Gamma_u$).\n",
    "\n",
    "### Intuição\n",
    "\n",
    "1. **Porta de Atualização ($\\Gamma_u$)**:\n",
    "   - Se $\\Gamma_u$ está perto de 1, o modelo atualiza fortemente o estado oculto com o novo estado candidato.\n",
    "   - Se $\\Gamma_u$ está perto de 0, o modelo mantém mais do estado oculto anterior.\n",
    "\n",
    "2. **Porta de Relevância ($\\Gamma_r$)**:\n",
    "   - Se $\\Gamma_r$ está perto de 1, o modelo usa completamente o estado oculto anterior para calcular o novo estado candidato.\n",
    "   - Se $\\Gamma_r$ está perto de 0, o modelo ignora o estado oculto anterior e reinicia (ou reseta) a memória.\n",
    "\n",
    "### Vantagens das GRUs\n",
    "\n",
    "1. **Captura de Dependências Longas**: As GRUs ajudam a resolver o problema do desvanecimento do gradiente, permitindo que informações importantes sejam mantidas por longos períodos.\n",
    "2. **Eficiência Computacional**: As GRUs têm menos parâmetros comparados às LSTMs, o que pode resultar em treinamento mais rápido e inferência mais eficiente.\n",
    "3. **Simplicidade**: Embora sejam mais complexas que as vanilla RNNs, as GRUs são mais simples que as LSTMs, pois não possuem uma porta de saída.\n",
    "\n",
    "### Fórmulas Resumidas\n",
    "\n",
    "1. **Porta de Atualização**:\n",
    "   $$\n",
    "   \\Gamma_u = \\sigma(W_u [h^{<t-1>}, x^{<t>}] + b_u)\n",
    "   $$\n",
    "\n",
    "2. **Porta de Relevância**:\n",
    "   $$\n",
    "   \\Gamma_r = \\sigma(W_r [h^{<t-1>}, x^{<t>}] + b_r)\n",
    "   $$\n",
    "\n",
    "3. **Novo Estado Candidato**:\n",
    "   $$\n",
    "   h'^{<t>} = \\tanh(W_h [\\Gamma_r * h^{<t-1>}, x^{<t>}] + b_h)\n",
    "   $$\n",
    "\n",
    "4. **Estado Oculto Atualizado**:\n",
    "   $$\n",
    "   h^{<t>} = (1 - \\Gamma_u) * h^{<t-1>} + \\Gamma_u * h'^{<t>}\n",
    "   $$\n",
    "\n",
    "As GRUs são uma poderosa variante das RNNs, projetadas para capturar dependências de longo prazo de maneira mais eficaz, utilizando mecanismos de porta para controlar o fluxo de informações. Elas equilibram simplicidade e eficiência computacional, tornando-as uma escolha popular para muitas tarefas de NLP e séries temporais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df275ba",
   "metadata": {},
   "source": [
    "## Deep and Bi-directional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391efb92",
   "metadata": {},
   "source": [
    "**Redes Neurais Recorrentes Bidirecionais (Bi-directional RNNs, ou BRNNs)** são uma extensão das RNNs tradicionais que permitem que a informação flua em ambas as direções: do passado para o futuro e do futuro para o passado. Essa arquitetura é particularmente útil em tarefas de processamento de linguagem natural (NLP) onde o contexto futuro pode ser tão importante quanto o contexto passado para a compreensão da sequência atual.\n",
    "\n",
    "<img src=\"./imgs/birnn1.png\">\n",
    "\n",
    "### Estrutura das BRNNs\n",
    "\n",
    "A principal diferença entre uma RNN tradicional e uma BRNN é que a BRNN possui duas redes recorrentes:\n",
    "1. **Forward RNN**: Processa a sequência da esquerda para a direita.\n",
    "2. **Backward RNN**: Processa a sequência da direita para a esquerda.\n",
    "\n",
    "#### Funcionamento\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   - A RNN direta (forward) processa a sequência de entrada $ x_1, x_2, ..., x_T $ e produz uma sequência de estados ocultos $ \\overrightarrow{h_1}, \\overrightarrow{h_2}, ..., \\overrightarrow{h_T} $.\n",
    "\n",
    "2. **Backward Pass**:\n",
    "   - A RNN reversa (backward) processa a sequência de entrada na ordem inversa $ x_T, x_{T-1}, ..., x_1 $ e produz uma sequência de estados ocultos $ \\overleftarrow{h_T}, \\overleftarrow{h_{T-1}}, ..., \\overleftarrow{h_1} $.\n",
    "\n",
    "3. **Combinação**: Em cada passo de tempo $ t $, os estados ocultos das duas redes são combinados. A combinação pode ser feita de várias maneiras, como **concatenação ou soma**. Normalmente, usa-se a concatenação:\n",
    "     $$\n",
    "     h_t = [\\overrightarrow{h_t}, \\overleftarrow{h_t}]\n",
    "     $$\n",
    "\n",
    "4. **Previsão**: A saída final em cada passo de tempo $ y_t $ é gerada usando a combinação dos estados ocultos bidirecionais:\n",
    "     $$\n",
    "     y_t = g(W_y h_t + b_y)\n",
    "     $$\n",
    "     \n",
    "     ou\n",
    "     \n",
    "     $$\n",
    "     y_t = g(W_y [\\overrightarrow{h_t}, \\overleftarrow{h_t}] + b_y)\n",
    "     $$\n",
    "     \n",
    "   - Onde $ g $ é uma função de ativação apropriada (por exemplo, softmax para tarefas de classificação).\n",
    "\n",
    "### Intuição\n",
    "\n",
    "A ideia principal por trás das BRNNs é fornecer ao modelo acesso ao contexto completo em torno de cada ponto da sequência. Por exemplo, ao processar uma palavra em uma frase, uma BRNN pode considerar tanto as palavras anteriores quanto as palavras subsequentes, o que pode ser crucial para desambiguação e melhor compreensão do contexto.\n",
    "\n",
    "### Fórmulas e Arquitetura\n",
    "\n",
    "Vamos detalhar as fórmulas e a arquitetura de uma BRNN.\n",
    "\n",
    "1. **Forward Pass**:\n",
    "   $$\n",
    "   \\overrightarrow{h_t} = g(W_x x_t + W_{\\overrightarrow{h}} \\overrightarrow{h_{t-1}} + b_{\\overrightarrow{h}})\n",
    "   $$\n",
    "\n",
    "2. **Backward Pass**:\n",
    "   $$\n",
    "   \\overleftarrow{h_t} = g(W_x x_t + W_{\\overleftarrow{h}} \\overleftarrow{h_{t+1}} + b_{\\overleftarrow{h}})\n",
    "   $$\n",
    "\n",
    "3. **Combinação dos Estados Ocultos**:\n",
    "   $$\n",
    "   h_t = [\\overrightarrow{h_t}, \\overleftarrow{h_t}]\n",
    "   $$\n",
    "\n",
    "4. **Previsão da Saída**:\n",
    "   $$\n",
    "   y_t = g(W_y h_t + b_y)\n",
    "   $$\n",
    "\n",
    "### Vantagens das BRNNs\n",
    "\n",
    "1. **Acesso Completo ao Contexto**: A capacidade de considerar tanto o contexto anterior quanto o futuro permite que as BRNNs façam previsões mais informadas e precisas.\n",
    "\n",
    "2. **Melhor Desempenho em Tarefas de NLP**: BRNNs são especialmente eficazes em tarefas onde o contexto completo é crucial, como tradução automática, reconhecimento de fala e análise de sentimentos.\n",
    "\n",
    "### Exemplos de Aplicações\n",
    "\n",
    "1. **Tradução Automática**: Considerar palavras futuras pode ajudar a determinar a melhor tradução para palavras ambíguas.\n",
    "\n",
    "2. **Reconhecimento de Fala**: O contexto futuro pode ajudar a desambiguar palavras que soam semelhantes.\n",
    "\n",
    "3. **Análise de Sentimentos**: A compreensão do sentimento de uma frase pode depender de palavras tanto no início quanto no final da frase.\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- **BRNNs** são uma extensão das RNNs que processam a entrada em duas direções: forward e backward.\n",
    "- **Combinação dos Estados Ocultos**: Os estados ocultos das direções forward e backward são combinados para fornecer um contexto mais completo.\n",
    "- **Vantagens**: Acesso ao contexto completo ao redor de cada ponto da sequência, resultando em previsões mais precisas em muitas tarefas de NLP.\n",
    "\n",
    "As BRNNs representam uma melhoria significativa em relação às RNNs unidirecionais, especialmente em cenários onde o contexto completo da sequência é crítico para a tarefa em questão."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4766848f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T21:44:55.399940Z",
     "start_time": "2024-07-04T21:44:55.394695Z"
    }
   },
   "source": [
    "**Redes Neurais Recorrentes Profundas (Deep RNNs)** são uma extensão das RNNs tradicionais que aumentam a profundidade da rede, **empilhando várias camadas** de unidades recorrentes umas sobre as outras. Isso permite que o modelo capture representações mais complexas e aprenda características hierárquicas dos dados sequenciais.\n",
    "\n",
    "A estrutura de uma Deep RNN consiste em várias camadas recorrentes (RNNs, LSTMs, GRUs, etc.) **empilhadas verticalmente**. Cada camada da rede **recebe a saída da camada anterior** como entrada, processa essa entrada e passa a saída para a próxima camada.\n",
    "\n",
    "<img src=\"./imgs/birnn2.png\">\n",
    "\n",
    "#### Funcionamento\n",
    "\n",
    "1. **Primeira Camada Recorrente**:\n",
    "   - Recebe a sequência de entrada original.\n",
    "   - Processa a entrada sequencialmente para produzir uma sequência de estados ocultos.\n",
    "\n",
    "2. **Camadas Recorrentes Subsequentes**:\n",
    "   - Cada camada seguinte recebe a sequência de estados ocultos da camada anterior como entrada.\n",
    "   - Processa essa entrada sequencialmente para produzir uma nova sequência de estados ocultos.\n",
    "   - Isso continua até a camada final.\n",
    "\n",
    "3. **Camada de Saída**:\n",
    "   - A camada de saída recebe a sequência de estados ocultos da última camada recorrente.\n",
    "   - Gera a previsão ou a saída desejada para cada passo de tempo.\n",
    "\n",
    "### Fórmulas e Arquitetura\n",
    "\n",
    "Vamos considerar uma Deep RNN com duas camadas de unidades recorrentes para simplificação:\n",
    "\n",
    "1. **Primeira Camada Recorrente**:\n",
    "   - Para cada passo de tempo $ t $:\n",
    "     $$\n",
    "     h^{(1)}_t = g(W^{(1)}_x x_t + W^{(1)}_h h^{(1)}_{t-1} + b^{(1)})\n",
    "     $$\n",
    "   - Onde $ h^{(1)}_t $ é o estado oculto da primeira camada no passo de tempo $ t $.\n",
    "\n",
    "2. **Segunda Camada Recorrente**:\n",
    "   - Recebe $ h^{(1)} $ como entrada:\n",
    "     $$\n",
    "     h^{(2)}_t = g(W^{(2)}_{h^{(1)}} h^{(1)}_t + W^{(2)}_h h^{(2)}_{t-1} + b^{(2)})\n",
    "     $$\n",
    "   - Onde $ h^{(2)}_t $ é o estado oculto da segunda camada no passo de tempo $ t $.\n",
    "\n",
    "3. **Camada de Saída**:\n",
    "   - Recebe $ h^{(2)}_t $ e gera a saída $ y_t $:\n",
    "     $$\n",
    "     y_t = g(W_y h^{(2)}_t + b_y)\n",
    "     $$\n",
    "\n",
    "### Intuição\n",
    "\n",
    "- **Primeira Camada**: Extrai características básicas da sequência de entrada.\n",
    "- **Camadas Intermediárias**: Capturam representações mais abstratas e complexas da sequência, permitindo que a rede aprenda dependências de longo alcance e padrões hierárquicos.\n",
    "- **Camada de Saída**: Combina todas as características aprendidas para produzir a previsão final.\n",
    "\n",
    "### Vantagens das Deep RNNs\n",
    "\n",
    "1. **Captura de Padrões Complexos**: A profundidade adicional permite que a rede capture padrões mais complexos e hierárquicos nos dados.\n",
    "2. **Aprendizado de Representações Hierárquicas**: Cada camada pode aprender diferentes níveis de abstração, melhorando a capacidade do modelo de generalizar.\n",
    "3. **Melhor Desempenho em Tarefas Complexas**: Deep RNNs são mais eficazes em tarefas que exigem a captura de dependências de longo prazo e a compreensão de contextos complexos.\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- **Deep RNNs** são RNNs com várias camadas empilhadas, permitindo a captura de representações mais complexas e hierárquicas.\n",
    "- **Funcionamento**: Cada camada recorrente processa a saída da camada anterior, passando a sequência de estados ocultos para a próxima camada.\n",
    "- **Vantagens**: Captura de padrões complexos, aprendizado de representações hierárquicas e melhor desempenho em tarefas complexas.\n",
    "\n",
    "Deep RNNs são uma poderosa extensão das RNNs tradicionais, oferecendo melhorias significativas em termos de capacidade de modelagem e desempenho em tarefas de processamento de linguagem natural e outras aplicações sequenciais complexas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a73282",
   "metadata": {},
   "source": [
    "## Calculating Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5718d2",
   "metadata": {},
   "source": [
    "A perplexidade é uma métrica comum usada para avaliar modelos de linguagem, especialmente em tarefas de modelagem de linguagem e previsão de sequência. Ela mede quão bem um modelo de linguagem prevê uma sequência de palavras. Uma perplexidade mais baixa indica que o modelo está melhor em prever a sequência.\n",
    "\n",
    "Perplexidade pode ser entendida como a medida de **\"surpresa\"** que o modelo experimenta ao prever a próxima palavra em uma sequência. Em outras palavras, **é uma medida de quão incerto o modelo está sobre a próxima palavra**.\n",
    "\n",
    "Para calcular a perplexidade, usamos a probabilidade logarítmica da sequência de teste. Aqui estão os passos detalhados:\n",
    "\n",
    "1. **Probabilidade Logarítmica**:\n",
    "   - Suponha que temos uma sequência de palavras $ w_1, w_2, ..., w_N $.\n",
    "   - A probabilidade do modelo prever a sequência é $ P(w_1, w_2, ..., w_N) $.\n",
    "\n",
    "2. **Log Probabilidade Média**:\n",
    "   - A probabilidade logarítmica média é calculada como:\n",
    "     $$\n",
    "     \\frac{1}{N} \\sum_{t=1}^N \\log P(w_t | w_1, w_2, ..., w_{t-1})\n",
    "     $$\n",
    "   - Aqui, $ P(w_t | w_1, w_2, ..., w_{t-1}) $ é a probabilidade que o modelo atribui à palavra $ w_t $ dado o histórico $ w_1, w_2, ..., w_{t-1} $.\n",
    "\n",
    "3. **Perplexidade**:\n",
    "   - A perplexidade é então definida como a exponenciação negativa da log probabilidade média:\n",
    "     $$\n",
    "     \\text{Perplexity} = \\exp \\left( - \\frac{1}{N} \\sum_{t=1}^N \\log P(w_t | w_1, w_2, ..., w_{t-1}) \\right)\n",
    "     $$\n",
    "   - Alternativamente, pode-se calcular a perplexidade diretamente da probabilidade da sequência:\n",
    "     $$\n",
    "     \\text{Perplexity} = P(w_1, w_2, ..., w_N)^{-\\frac{1}{N}}\n",
    "     $$\n",
    "\n",
    "Vamos ilustrar o cálculo da perplexidade com um exemplo simples:\n",
    "\n",
    "- Suponha que temos uma sequência de três palavras $ w_1, w_2, w_3 $.\n",
    "- As probabilidades previstas pelo modelo são:\n",
    "  - $ P(w_1) = 0.1 $\n",
    "  - $ P(w_2 | w_1) = 0.4 $\n",
    "  - $ P(w_3 | w_1, w_2) = 0.3 $\n",
    "\n",
    "1. **Probabilidade da sequência**:\n",
    "   $$\n",
    "   P(w_1, w_2, w_3) = P(w_1) \\times P(w_2 | w_1) \\times P(w_3 | w_1, w_2) = 0.1 \\times 0.4 \\times 0.3 = 0.012\n",
    "   $$\n",
    "\n",
    "2. **Log Probabilidade Média**:\n",
    "   $$\n",
    "   \\frac{1}{3} \\left( \\log 0.1 + \\log 0.4 + \\log 0.3 \\right) = \\frac{1}{3} \\left( -2.3026 + -0.9163 + -1.2040 \\right) = \\frac{1}{3} \\left( -4.4229 \\right) = -1.4743\n",
    "   $$\n",
    "\n",
    "3. **Perplexidade**:\n",
    "   $$\n",
    "   \\text{Perplexity} = \\exp(-1.4743) \\approx 4.37\n",
    "   $$\n",
    "\n",
    "A métrica deve ser interpreta como:\n",
    "\n",
    "- **Perplexidade Alta**: Indica que o modelo tem baixa confiança em suas previsões (é \"muito surpreso\").\n",
    "- **Perplexidade Baixa**: Indica que o modelo está mais confiante em suas previsões (é \"menos surpreso\").\n",
    "\n",
    "Por fim:\n",
    "\n",
    "- **Perplexidade** mede a qualidade de um modelo de linguagem.\n",
    "- É calculada usando a probabilidade logarítmica da sequência de teste.\n",
    "- Uma perplexidade mais baixa indica um modelo de linguagem mais preciso.\n",
    "\n",
    "A perplexidade é uma métrica crucial para avaliar modelos de linguagem, ajudando a comparar diferentes modelos e a melhorar suas previsões."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fefa1a",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a2ba3",
   "metadata": {},
   "source": [
    "## RNNs and Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb50ed9",
   "metadata": {},
   "source": [
    "### Vanishing e Exploding Gradients em RNNs\n",
    "\n",
    "Redes Neurais Recorrentes (RNNs) são particularmente suscetíveis aos problemas de vanishing (desaparecimento) e exploding (explosão) de gradientes devido à natureza recursiva de suas operações durante o treinamento. Esses problemas afetam a capacidade da rede de aprender dependências de longo prazo.\n",
    "\n",
    "### Vanishing Gradients\n",
    "\n",
    "O problema de vanishing gradients ocorre quando os gradientes das funções de perda em relação aos pesos da rede se tornam extremamente pequenos. Isso impede a atualização efetiva dos pesos durante o treinamento, resultando em um aprendizado muito lento ou em uma paralisação completa do aprendizado.\n",
    "\n",
    "Durante o backpropagation através do tempo (BPTT), os gradientes são propagados de volta através de muitas camadas (uma por cada passo de tempo). Se as derivadas das funções de ativação ou dos pesos são menores que 1, os gradientes podem diminuir exponencialmente a cada passo de tempo:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} \\approx \\left( \\prod_{t=1}^T \\frac{\\partial h_t}{\\partial h_{t-1}} \\right) \\frac{\\partial L}{\\partial h_T}\n",
    "$$\n",
    "\n",
    "Se $ \\frac{\\partial h_t}{\\partial h_{t-1}} $ é menor que 1, **os gradientes multiplicados sucessivamente tornam-se muito pequenos**.\n",
    "\n",
    "Consequências:\n",
    "- **Dependências de Longo Prazo**: A rede tem dificuldade em aprender dependências de longo prazo, pois as informações dos passos de tempo anteriores são \"esquecidas\" rapidamente.\n",
    "- **Treinamento Ineficaz**: O aprendizado se torna muito lento ou estagna.\n",
    "\n",
    "### Exploding Gradients\n",
    "\n",
    "O problema de exploding gradients ocorre quando os gradientes se tornam extremamente grandes, causando atualizações instáveis e grandes nos pesos da rede. Isso pode resultar em números muito grandes ou NaNs durante o treinamento.\n",
    "\n",
    "Se as derivadas das funções de ativação ou dos pesos são maiores que 1, os gradientes podem aumentar exponencialmente a cada passo de tempo:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} \\approx \\left( \\prod_{t=1}^T \\frac{\\partial h_t}{\\partial h_{t-1}} \\right) \\frac{\\partial L}{\\partial h_T}\n",
    "$$\n",
    "\n",
    "Se $ \\frac{\\partial h_t}{\\partial h_{t-1}} $ é maior que 1, **os gradientes multiplicados sucessivamente tornam-se muito grandes**.\n",
    "\n",
    "Consequências:\n",
    "- **Instabilidade no Treinamento**: Pesos muito grandes podem causar oscilações na função de perda, dificultando a convergência.\n",
    "- **Numéricos NaNs**: Gradientes muito grandes podem causar overflow, resultando em NaNs e falha no treinamento.\n",
    "\n",
    "### Técnicas para Mitigar Vanishing e Exploding Gradients\n",
    "\n",
    "1. **Inicialização Adequada dos Pesos**: Usar técnicas de inicialização como Xavier ou He que são projetadas para manter os gradientes em uma faixa adequada.\n",
    "\n",
    "2. **Funções de Ativação Adequadas**: Usar funções de ativação como ReLU, que ajudam a mitigar o problema de vanishing gradients em comparação com funções como sigmoid ou tanh.\n",
    "\n",
    "3. **Normalização dos Gradientes**: Gradient Clipping, ou seja, limitar o valor máximo dos gradientes durante o backpropagation para evitar exploding gradients:\n",
    "  $$\n",
    "  \\text{if } ||\\nabla L|| > \\text{threshold} \\text{ then } \\nabla L = \\frac{\\nabla L}{||\\nabla L||} \\times \\text{threshold}\n",
    "  $$\n",
    "\n",
    "4. **Arquiteturas Recorrentes Avançadas**: **LSTM (Long Short-Term Memory)** e **GRU (Gated Recurrent Unit)**: Essas arquiteturas incluem mecanismos internos de controle de fluxo de informações, ajudando a manter os gradientes estáveis e preservando informações de longo prazo.\n",
    "\n",
    "Imagine que você está treinando um RNN para prever a próxima palavra em uma frase. Se o gradiente desaparece, **o modelo se torna incapaz de aprender** a influência de palavras anteriores (por exemplo, lembrar que \"não\" precede \"gosta\" para prever \"não gosta\"). Se o gradiente explode, o modelo fica instável e suas previsões se tornam erráticas.\n",
    "\n",
    "Resumo:\n",
    "- **Vanishing Gradients**: Gradientes diminuem exponencialmente, dificultando o aprendizado de dependências de longo prazo.\n",
    "- **Exploding Gradients**: Gradientes aumentam exponencialmente, causando instabilidade e problemas numéricos no treinamento.\n",
    "- **Mitigação**: Inicialização adequada dos pesos, uso de funções de ativação adequadas, normalização dos gradientes e uso de arquiteturas avançadas como LSTM e GRU.\n",
    "\n",
    "Entender e lidar com vanishing e exploding gradients é crucial para treinar RNNs de maneira eficaz e alcançar bom desempenho em tarefas de modelagem de linguagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e7bc7c",
   "metadata": {},
   "source": [
    "## Introduction to LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa43a84a",
   "metadata": {},
   "source": [
    "LSTM ou Long Short-Term Memory é uma arquitetura de rede neural recorrente (RNN) especialmente **projetada para superar os problemas de vanishing e exploding gradients** que podem ocorrer durante o treinamento de redes recorrentes tradicionais. LSTMs são eficazes na captura de dependências de longo prazo em dados sequenciais.\n",
    "\n",
    "A estrutura básica de uma célula LSTM é mais complexa do que uma célula RNN tradicional. Cada célula LSTM possui uma série de \"portões\" que controlam o fluxo de informações. Esses portões são:\n",
    "\n",
    "1. **Portão de Esquecimento (Forget Gate)**: Decide quais informações da célula anterior devem ser esquecidas.\n",
    "2. **Portão de Entrada (Input Gate)**: Decide quais novas informações serão armazenadas na célula.\n",
    "3. **Portão de Saída (Output Gate)**: Decide quais informações da célula serão usadas para a saída.\n",
    "\n",
    "Componentes Principais:\n",
    "- **Estado da Célula ($ C_t $)**: Armazena informações a longo prazo.\n",
    "- **Estado Oculto ($ h_t $)**: Armazena informações para o próximo passo de tempo.\n",
    "\n",
    "As aplicações desse modelo incluem:\n",
    "- Next character prediction\n",
    "- Chatbots\n",
    "- Music Compositon\n",
    "- Image Captioning\n",
    "- Speech Recognition\n",
    "\n",
    "### Equações da LSTM\n",
    "\n",
    "#### 1. Portão de Esquecimento\n",
    "Decide quais partes do estado da célula devem ser esquecidas:\n",
    "$$\n",
    "f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)\n",
    "$$\n",
    "Onde $ \\sigma $ é a função sigmoide, $ W_f $ são os pesos do portão de esquecimento, $ h_{t-1} $ é o estado oculto anterior, $ x_t $ é a entrada atual e $ b_f $ é o viés.\n",
    "\n",
    "#### 2. Portão de Entrada\n",
    "Decide quais novas informações serão armazenadas no estado da célula:\n",
    "$$\n",
    "i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)\n",
    "$$\n",
    "$$\n",
    "\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)\n",
    "$$\n",
    "Onde $ i_t $ é o vetor de atualização da célula e $ \\tilde{C}_t $ é o vetor de candidatos a novas informações.\n",
    "\n",
    "#### 3. Atualização do Estado da Célula\n",
    "Atualiza o estado da célula combinando o estado anterior modificado pelo portão de esquecimento e o novo candidato modificado pelo portão de entrada:\n",
    "$$\n",
    "C_t = f_t \\cdot C_{t-1} + i_t \\cdot \\tilde{C}_t\n",
    "$$\n",
    "\n",
    "#### 4. Portão de Saída\n",
    "Decide quais informações do estado da célula serão usadas para a saída:\n",
    "$$\n",
    "o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)\n",
    "$$\n",
    "$$\n",
    "h_t = o_t \\cdot \\tanh(C_t)\n",
    "$$\n",
    "\n",
    "### Resumo das Operações\n",
    "\n",
    "1. **Esquecimento**: Parte do estado da célula anterior é esquecida.\n",
    "2. **Atualização**: Novas informações relevantes são adicionadas ao estado da célula.\n",
    "3. **Saída**: O estado da célula atualizado é usado para produzir o estado oculto atual, que também pode ser usado como saída.\n",
    "\n",
    "### Diagrama da Célula LSTM\n",
    "\n",
    "Um diagrama típico de uma célula LSTM mostra a interação entre esses portões e o fluxo de informações. Cada portão é uma camada neural com uma função de ativação específica (sigmoide para $ f_t $, $ i_t $, $ o_t $ e tanh para $ \\tilde{C}_t $ e $ h_t $).\n",
    "\n",
    "### Intuição por Trás das LSTMs\n",
    "\n",
    "- **Portão de Esquecimento**: Permite que a célula decida quais informações antigas são relevantes o suficiente para serem mantidas.\n",
    "- **Portão de Entrada**: Permite que a célula decida quais novas informações devem ser adicionadas ao estado.\n",
    "- **Portão de Saída**: Permite que a célula decida quais informações do estado devem ser usadas para a saída atual.\n",
    "\n",
    "### Vantagens das LSTMs\n",
    "\n",
    "- **Aprendizagem de Dependências de Longo Prazo**: Graças à estrutura dos portões, LSTMs podem capturar dependências de longo prazo de maneira mais eficaz do que RNNs tradicionais.\n",
    "- **Controle Fino do Fluxo de Informação**: Os portões fornecem um mecanismo para controlar o fluxo de informações dentro da célula, mitigando os problemas de vanishing e exploding gradients.\n",
    "\n",
    "### Aplicações das LSTMs\n",
    "\n",
    "- **Processamento de Linguagem Natural (NLP)**: Tradução automática, geração de texto, análise de sentimentos.\n",
    "- **Reconhecimento de Fala**: Transcrição de fala para texto.\n",
    "- **Séries Temporais**: Previsão de séries temporais, como preços de ações e dados meteorológicos.\n",
    "- **Visão Computacional**: Descrição de imagens e vídeos.\n",
    "\n",
    "### Resumo\n",
    "\n",
    "- **LSTM**: Uma arquitetura de rede neural recorrente que usa portões para controlar o fluxo de informações, permitindo a captura de dependências de longo prazo.\n",
    "- **Componentes Principais**: Portões de esquecimento, entrada e saída, estado da célula e estado oculto.\n",
    "- **Vantagens**: Mitiga problemas de vanishing e exploding gradients, captura dependências de longo prazo, controle fino do fluxo de informações.\n",
    "\n",
    "As LSTMs são uma ferramenta poderosa para modelagem de dados sequenciais, proporcionando melhorias significativas em diversas aplicações práticas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e8ec29",
   "metadata": {},
   "source": [
    "Ler [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a77db5",
   "metadata": {},
   "source": [
    "## LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3582d8e4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37a99512",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b4df29f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5cdd1ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14c3ad4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece638f6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d841ea79",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "738a7a99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b4e7067",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60afee7c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6b3b419",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f88e4f63",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0df5930",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f98fa1a",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecc72d4",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe88b9",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Sequence Models, disponível em https://www.coursera.org/learn/sequence-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9216662",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
