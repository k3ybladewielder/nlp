{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eaa74c3",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Attention Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Attention Models da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e34e0c",
   "metadata": {},
   "source": [
    "# Week 1 - Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6528abb",
   "metadata": {},
   "source": [
    "## Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c351c9",
   "metadata": {},
   "source": [
    "Os modelos Seq2Seq (Sequence-to-Sequence) são amplamente utilizados em tarefas de tradução automática (machine translation) e outras tarefas de NLP que envolvem a conversão de uma sequência de entrada em uma sequência de saída. Eles foram inicialmente propostos para tarefas como tradução automática, onde uma sequência de palavras em uma língua (entrada) é convertida em uma sequência de palavras em outra língua (saída). A arquitetura básica é composta por dois componentes principais:\n",
    "\n",
    "- **Encoder**: Transforma a sequência de entrada em um vetor de contexto.\n",
    "- **Decoder**: Utiliza o vetor de contexto para gerar a sequência de saída.\n",
    "\n",
    "<img src=\"./imgs/seq2seq.png\">\n",
    "\n",
    "### Arquitetura Seq2Seq\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "O encoder é tipicamente uma rede recorrente (RNN, LSTM, GRU) que lê a sequência de entrada token por token e gera um vetor de contexto que representa a informação da sequência inteira.\n",
    "\n",
    "- **Inputs**: Sequência de entrada $X = (x_1, x_2, ..., x_n)$.\n",
    "- **Outputs**: Vetor de contexto $C$ e estado oculto $h_t$ em cada passo de tempo.\n",
    "\n",
    "<img src=\"./imgs/seq2seq_encoder.png\">\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "O decoder é outra rede recorrente que gera a sequência de saída um token por vez, **usando o vetor de contexto do encoder** e o **estado oculto anterior do decoder**. A sequencia de entrada no decoder inicia com token $\\text{<SOS>}$ (Start of Sequence)\n",
    "\n",
    "- **Inputs**: O vetor de contexto do encoder, o estado oculto anterior e o token de entrada anterior.\n",
    "- **Outputs**: Token de saída e novo estado oculto.\n",
    "\n",
    "<img src=\"./imgs/seq2seq_decoder.png\">\n",
    "\n",
    "#### Attention\n",
    "\n",
    "O mecanismo de atenção foi introduzido para melhorar a performance dos modelos Seq2Seq, permitindo que o modelo foque em diferentes partes da entrada enquanto gera cada token da saída.\n",
    "\n",
    "- **Atenção Básica**: Calcula um conjunto de pesos de atenção que indicam a importância de cada token da sequência de entrada para a geração do próximo token da saída.\n",
    "- **Context Vector**: Um vetor de contexto ponderado é calculado como uma combinação linear dos estados ocultos do encoder, ponderado pelos pesos de atenção.\n",
    "\n",
    "Matematicamente, a atenção pode ser descrita da seguinte maneira:\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})} $$\n",
    "onde $e_{ij} = a(s_{i-1}, h_j)$ é a função de alinhamento que pode ser um simples perceptron.\n",
    "\n",
    "<img src=\"./imgs/seq2seq_attention.png\">\n",
    "\n",
    "### Aplicação em Machine Translation\n",
    "\n",
    "Na tradução automática, o processo funciona da seguinte maneira:\n",
    "\n",
    "1. **Encoding**: O encoder lê a sequência de entrada (frase na língua de origem) e gera um vetor de contexto.\n",
    "2. **Attention**: Durante a decodificação, o mecanismo de atenção calcula os pesos de atenção que focam nas partes relevantes da sequência de entrada.\n",
    "3. **Decoding**: O decoder gera a sequência de saída (frase na língua de destino), token por token, utilizando o vetor de contexto ponderado pela atenção.\n",
    "\n",
    "### Exemplo Simplificado\n",
    "\n",
    "1. **Entrada**: \"I love NLP\"\n",
    "2. **Encoding**: O encoder gera vetores de estado oculto para cada token e um vetor de contexto geral.\n",
    "3. **Attention**: Para cada token gerado na saída, calcula-se a importância de cada token da entrada.\n",
    "4. **Decoding**: Gera a saída \"Yo amo PLN\" utilizando os vetores de contexto e atenção.\n",
    "\n",
    "### Benefícios do Mecanismo de Atenção\n",
    "\n",
    "- **Melhora a Tradução**: Permite que o modelo foque nas partes mais relevantes da entrada.\n",
    "- **Mitiga Problemas de Longas Sequências**: Ajuda a manter a informação relevante mesmo em sequências longas.\n",
    "\n",
    "Os modelos Seq2Seq com atenção são fundamentais para muitas tarefas de NLP, especialmente a tradução automática. A arquitetura básica de encoder-decoder, aprimorada com o mecanismo de atenção, permite que esses modelos lidem eficazmente com a complexidade das sequências de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7891707",
   "metadata": {},
   "source": [
    "## Queries, Keys, Values, and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7fb7c",
   "metadata": {},
   "source": [
    "A camada de atenção calcula um **vetor de atenção que pondera a importância de cada token da sequência de entrada para a geração de cada token da sequência de saída**. \n",
    "\n",
    "#### Inputs da Camada de Atenção\n",
    "\n",
    "A camada de atenção recebe três inputs principais:\n",
    "- **Queries (Q)**: Vetores de consulta que representam os tokens da **sequência de saída** atual.\n",
    "- **Keys (K)**: Vetores de chave que representam os tokens da **sequência de entrada**.\n",
    "- **Values (V)**: Vetores de valor que também representam os tokens da **sequência de entrada**.\n",
    "\n",
    "<img src=\"./imgs/qkv.png\">\n",
    "\n",
    "Em um modelo Seq2Seq com atenção, **os estados ocultos do decoder são usados como Queries**, enquanto **os estados ocultos do encoder são usados como Keys e Values**.\n",
    "\n",
    "#### Cálculo das Similaridades (Scores)\n",
    "\n",
    "Primeiro, calculamos uma pontuação (score) ou alinhamento que mede a **similaridade entre cada par de query e key**. Uma função de similaridade comum é o produto escalar (dot product):\n",
    "\n",
    "$$ \\text{score}(Q, K) = QK^T $$\n",
    "\n",
    "Outra opção é o produto escalar escalado (scaled dot product), que é comum em Transformers:\n",
    "\n",
    "$$ \\text{score}(Q, K) = \\frac{QK^T}{\\sqrt{d_k}} $$\n",
    "\n",
    "onde $ d_k $ é a dimensão dos vetores de chave.\n",
    "\n",
    "#### Normalização dos scores\n",
    "\n",
    "As pontuações escaladas são então normalizadas usando a função softmax para obter os pesos de atenção:\n",
    "\n",
    "$$ \\alpha = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) $$\n",
    "\n",
    "Isso transforma as pontuações escaladas em um conjunto de pesos que somam 1, facilitando a interpretação como probabilidades.\n",
    "\n",
    "#### Cálculo dos Pesos de Atenção\n",
    "\n",
    "As pontuações são então normalizadas usando uma função softmax para obter os pesos de atenção, que representam a importância relativa de cada token de entrada para cada token de saída:\n",
    "\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(\\text{score}(q_i, k_j))}{\\sum_{k=1}^{n} \\exp(\\text{score}(q_i, k_k))} $$\n",
    "\n",
    "onde $ \\alpha_{ij} $ é o peso de atenção para o $ i $-ésimo token de saída e o $ j $-ésimo token de entrada.\n",
    "\n",
    "#### Cálculo do Vetor de Contexto\n",
    "\n",
    "Finalmente, os pesos de atenção são usados para calcular uma combinação ponderada dos vetores de valor:\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "O resultado é uma matriz que representa a combinação ponderada dos valores, ajustada de acordo com a relevância calculada pelas pontuações de atenção.\n",
    "\n",
    "Os pesos de atenção são então usados para calcular uma **combinação ponderada dos vetores de valor**, resultando em um vetor de contexto para cada token de saída:\n",
    "\n",
    "$$ \\text{context}_i = \\sum_{j=1}^{n} \\alpha_{ij} v_j $$\n",
    "\n",
    "#### Atenção Multi-cabeça (Multi-head Attention)\n",
    "\n",
    "Em arquiteturas como Transformers, é comum usar várias cabeças de atenção para capturar diferentes tipos de relações entre tokens. Cada cabeça de atenção realiza o processo descrito acima de forma independente, e os resultados são concatenados e transformados:\n",
    "\n",
    "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W^O $$\n",
    "\n",
    "onde cada cabeça de atenção $ \\text{head}_i $ é calculada como:\n",
    "\n",
    "$$ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
    "\n",
    "### Tipos de Atenção\n",
    "\n",
    "Existem diferentes variações do mecanismo de atenção, incluindo:\n",
    "\n",
    "1. **Self-attention**: Quando Queries, Keys e Values vêm da mesma sequência. Utilizado principalmente em Transformers.\n",
    "2. **Cross-attention**: Quando Queries vêm de uma sequência (ex. saída) e Keys/Values vêm de outra (ex. entrada). Utilizado em modelos Seq2Seq com atenção.\n",
    "\n",
    "### Implementação Simplificada\n",
    "\n",
    "Aqui está um exemplo de implementação simplificada de uma camada de atenção em pseudo-código:\n",
    "\n",
    "```python\n",
    "def attention(Q, K, V):\n",
    "    # Cálculo das pontuações\n",
    "    scores = dot_product(Q, K.T) / sqrt(d_k)\n",
    "    \n",
    "    # Normalização das pontuações\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Cálculo do vetor de contexto\n",
    "    context = dot_product(attention_weights, V)\n",
    "    \n",
    "    return context, attention_weights\n",
    "```\n",
    "\n",
    "### Benefícios da Camada de Atenção\n",
    "\n",
    "- **Foco Dinâmico**: Permite que o modelo se concentre nas partes mais relevantes da entrada.\n",
    "- **Eficiência Computacional**: Self-attention, especialmente em Transformers, é altamente paralelizável.\n",
    "- **Flexibilidade**: Pode ser adaptada para várias tarefas de NLP, como tradução, resumo e resposta a perguntas.\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "A camada de atenção é um componente poderoso que melhora a capacidade dos modelos de NLP em capturar dependências complexas dentro das sequências de entrada. Ao permitir que o modelo foque dinamicamente nas partes mais relevantes da entrada, a atenção desempenha um papel crucial em muitos dos avanços recentes em NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635def8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T22:54:02.465276Z",
     "start_time": "2024-07-11T22:54:02.459965Z"
    }
   },
   "source": [
    "## Material complementar\n",
    "- [Visualizando a atenção, o coração de um transformador](https://www.youtube.com/watch?v=eMlx5fFNoYc)\n",
    "- [Atenção para Redes Neurais, Claramente Explicadas!!!](https://www.youtube.com/watch?v=PSs6nxngL6k)\n",
    "- [The math behind Attention: Keys, Queries, and Values matrices](https://www.youtube.com/watch?v=UPtG_38Oq8o&t=669s)\n",
    "- [The Transformer neural network architecture EXPLAINED. “Attention is all you need”](https://www.youtube.com/watch?v=FWFA4DGuzSc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0327f",
   "metadata": {},
   "source": [
    "## Teacher Forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b916de",
   "metadata": {},
   "source": [
    "Teacher forcing é uma técnica utilizada durante o treinamento de modelos de sequência para sequência (Seq2Seq), como aqueles usados em tradução automática, síntese de texto e outras tarefas de processamento de linguagem natural. \n",
    "\n",
    "Teacher forcing refere-se ao **uso da sequência de saída real (ground truth) como entrada para o próximo passo do decodificador**, em vez de usar a saída gerada pelo próprio modelo na etapa anterior. Isso ajuda o modelo a aprender mais rapidamente e de maneira mais estável.\n",
    "\n",
    "1. **Treinamento Sem Teacher Forcing**:\n",
    "   - No treinamento sem teacher forcing, o modelo gera uma palavra na sequência de saída com base nas palavras anteriores que ele próprio gerou.\n",
    "   - Esse método pode levar a problemas se o modelo fizer um erro, pois erros podem se propagar ao longo da sequência, causando previsões cada vez menos precisas.\n",
    "\n",
    "2. **Treinamento Com Teacher Forcing**:\n",
    "   - No treinamento com teacher forcing, **o modelo usa a palavra correta da sequência de treinamento como entrada para o próximo passo, independentemente de sua própria previsão**.\n",
    "   - Isso fornece ao modelo informações corretas em cada etapa, ajudando-o a aprender a sequência de forma mais eficiente e evitando a propagação de erros.\n",
    "\n",
    "<img src=\"./imgs/teacher_forcing.png\">\n",
    "\n",
    "\n",
    "### Benefícios do Teacher Forcing\n",
    "\n",
    "- **Aprendizagem Mais Rápida**: Fornecendo entradas corretas em cada passo, o modelo pode aprender a sequência-alvo de maneira mais eficiente.\n",
    "- **Estabilidade no Treinamento**: Reduz o impacto de erros na geração de sequência, o que pode ajudar na convergência mais estável do modelo.\n",
    "\n",
    "### Desvantagens do Teacher Forcing\n",
    "\n",
    "- **Diferença entre Treinamento e Inferência**: Durante a inferência (teste), o modelo não terá acesso às sequências de saída reais e terá que confiar em suas próprias previsões. Isso pode levar a uma discrepância entre o desempenho de treinamento e de inferência, conhecida como \"exposição ao viés\".\n",
    "- **Dependência do Ground Truth**: O modelo pode se tornar excessivamente dependente das sequências de saída corretas fornecidas durante o treinamento, o que pode prejudicar seu desempenho em cenários reais onde tais dados não estão disponíveis.\n",
    "\n",
    "### Mitigação de Desvantagens\n",
    "\n",
    "Para mitigar as desvantagens do teacher forcing, técnicas como **scheduled sampling** podem ser utilizadas. No scheduled sampling, o modelo é treinado usando uma mistura de suas próprias previsões e das saídas corretas (ground truth). Com o tempo, a proporção de saídas do modelo aumenta, ajudando-o a aprender a corrigir seus próprios erros e a se tornar mais robusto durante a inferência.\n",
    "\n",
    "### Exemplo de Teacher Forcing\n",
    "\n",
    "Vamos considerar um exemplo de tradução automática:\n",
    "\n",
    "- **Entrada**: \"I love NLP\"\n",
    "- **Saída Esperada**: \"Eu amo PLN\"\n",
    "\n",
    "Sem teacher forcing, o decodificador geraria a tradução palavra por palavra, usando suas próprias previsões anteriores para prever a próxima palavra.\n",
    "\n",
    "Com teacher forcing, durante o treinamento, mesmo que o modelo preveja incorretamente \"Eu amo\" como \"Eu gostar\", o próximo passo ainda usaria \"amo\" (a palavra correta) como entrada para prever a próxima palavra (\"PLN\").\n",
    "\n",
    "Por fim, o teacher forcing é uma técnica poderosa no treinamento de modelos Seq2Seq que ajuda a melhorar a eficiência e a estabilidade do treinamento ao fornecer a sequência de saída correta em cada passo. No entanto, deve ser usado com cautela devido às possíveis diferenças entre os ambientes de treinamento e de inferência."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad930c59",
   "metadata": {},
   "source": [
    "## BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb28a9f",
   "metadata": {},
   "source": [
    "O Bilingual Evaluation Understudy ou **BLEU score** é uma métrica popular para avaliar a qualidade de traduções automáticas. Ela compara uma frase traduzida por um modelo (tradução automática) com uma ou mais traduções de referência (traduções humanas) e calcula uma pontuação baseada na precisão de n-gramas. O BLEU é **focado na precisão** e não considera a estrutura da sentença ou semântica.\n",
    "\n",
    "1. **N-gram Precision**:\n",
    "   - O BLEU calcula a precisão dos n-gramas da tradução gerada pelo modelo em relação às traduções de referência.\n",
    "   - Um n-gram é uma sequência de n palavras.\n",
    "   - Por exemplo, para bigramas (n=2), as sequências são pares de palavras consecutivas.\n",
    "\n",
    "2. **Clipping**:\n",
    "   - Para evitar a inflação da precisão devido à repetição de n-gramas na tradução gerada, o BLEU aplica um \"clipping\". Isso limita o número de vezes que um n-gram pode ser contado com base na frequência máxima desse n-gram nas referências.\n",
    "\n",
    "3. **Brevity Penalty**:\n",
    "   - O BLEU penaliza traduções que são significativamente mais curtas do que as referências. Isso é feito para evitar que traduções muito curtas (que podem ter alta precisão de n-gramas) recebam pontuações altas.\n",
    "\n",
    "### Cálculo do BLEU Score\n",
    "\n",
    "1. **Calcular Precisões de N-gramas**:\n",
    "   - Para cada n-grama (por exemplo, unigramas, bigramas, trigramas, etc.), a precisão é calculada como a **razão entre o número de n-gramas corretos na tradução gerada e o número total de n-gramas na tradução gerada**.\n",
    "\n",
    "2. **Aplicar Clipping**:\n",
    "   - Para cada n-grama, limite a contagem ao máximo que ocorre nas referências.\n",
    "\n",
    "3. **Combinar Precisões**:\n",
    "   - Combine as precisões de n-gramas de diferentes tamanhos usando uma média geométrica ponderada.\n",
    "\n",
    "4. **Aplicar Brevity Penalty**:\n",
    "   - Quando a tradução gerada for mais curta que a referência, é aplicado uma penalidade para ajustar a pontuação final.\n",
    "\n",
    "A fórmula do BLEU score é:\n",
    "$$\n",
    "\\text{BLEU} = BP \\cdot \\exp \\left( \\sum_{n=1}^{N} w_n \\log p_n \\right)\n",
    "$$\n",
    "Onde:\n",
    "- $BP$ é a brevity penalty.\n",
    "- $p_n$ é a precisão dos n-gramas.\n",
    "- $w_n$ são os pesos (geralmente iguais) para as precisões de n-gramas.\n",
    "\n",
    "### Avaliação do Modelo com BLEU Score\n",
    "\n",
    "Para avaliar um modelo de tradução utilizando o BLEU score, podemos seguir esses passos:\n",
    "\n",
    "1. **Obter Traduções Geradas pelo Modelo**: Traduzir um conjunto de frases de teste utilizando o modelo treinado.\n",
    "2. **Obter Traduções de Referência**: Utilizar traduções humanas como referências para cada frase de teste.\n",
    "3. **Calcular o BLEU Score**: Utilizar uma biblioteca de NLP, como `nltk` ou `sacrebleu`, para calcular o BLEU score comparando as traduções geradas com as referências.\n",
    "\n",
    "### Exemplo em Python usando NLTK\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "# Traduções de referência (humanas)\n",
    "references = [\n",
    "    [\"this is a test\".split(), \"this is a trial\".split()],\n",
    "    [\"another test sentence\".split()]\n",
    "]\n",
    "\n",
    "# Traduções geradas pelo modelo\n",
    "candidates = [\n",
    "    \"this is a test\".split(),\n",
    "    \"another test sentence\".split()\n",
    "]\n",
    "\n",
    "# BLEU score para cada frase\n",
    "for i in range(len(candidates)):\n",
    "    print(f\"Sentence {i+1} BLEU score: {sentence_bleu(references[i], candidates[i])}\")\n",
    "\n",
    "# BLEU score para o corpus\n",
    "print(f\"Corpus BLEU score: {corpus_bleu(references, candidates)}\")\n",
    "```\n",
    "\n",
    "O BLEU score é uma métrica poderosa para avaliar traduções automáticas, fornecendo uma maneira quantitativa de comparar traduções geradas com traduções de referência. No entanto, ele tem limitações, como não capturar bem a fluência ou a adequação semântica das traduções, motivo pelo qual é frequentemente utilizado em conjunto com outras métricas e avaliações qualitativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713aed5e",
   "metadata": {},
   "source": [
    "## ROUGE-N Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07a6f7",
   "metadata": {},
   "source": [
    "O Recall-Oriented Understudy for Gisting Evaluation ou ROUGE é um conjunto de métricas utilizadas para avaliar a qualidade de resumos e traduções automáticas, comparando-os com um ou mais resumos de referência (resumos humanos). Entre as variantes do ROUGE, o **ROUGE-N** é um dos mais comuns, onde **N** representa o tamanho dos n-gramas. O ROUGE é focado no **recall**, a métrica busca identificar o quanto da referencia aparece na tradução candidata, e também não considera a estrutura da sentença ou semântica..\n",
    "\n",
    "O ROUGE-N **calcula a sobreposição de n-gramas entre um resumo gerado automaticamente e um ou mais resumos de referência**. Especificamente, ROUGE-N considera n-gramas comuns entre a saída do modelo e as referências, medindo tanto a precisão quanto o recall, mas frequentemente focando no recall.\n",
    "\n",
    "#### Fórmulas do ROUGE-N\n",
    "\n",
    "- **ROUGE-N (Recall)**:\n",
    "  $$\n",
    "  \\text{ROUGE-N} = \\frac{\\sum_{\\text{ref} \\in \\text{References}} \\sum_{\\text{n-gram} \\in \\text{ref}} \\min(\\text{Count}_{\\text{n-gram}}^{\\text{model}}, \\text{Count}_{\\text{n-gram}}^{\\text{ref}})}{\\sum_{\\text{ref} \\in \\text{References}} \\sum_{\\text{n-gram} \\in \\text{ref}} \\text{Count}_{\\text{n-gram}}^{\\text{ref}}}\n",
    "  $$\n",
    "\n",
    "Onde:\n",
    "- $\\text{Count}_{\\text{n-gram}}^{\\text{model}}$ é a contagem de um n-gram específico no resumo gerado pelo modelo.\n",
    "- $\\text{Count}_{\\text{n-gram}}^{\\text{ref}}$ é a contagem do mesmo n-gram em uma das referências.\n",
    "\n",
    "### Passos para Calcular o ROUGE-N\n",
    "\n",
    "1. **Tokenização**: Divida tanto a saída do modelo quanto as referências em tokens.\n",
    "2. **Contagem de N-gramas**: Calcule a frequência de cada n-grama em ambas as saídas.\n",
    "3. **Sobreposição de N-gramas**: Encontre a sobreposição de n-gramas entre a saída do modelo e as referências.\n",
    "4. **Cálculo do ROUGE-N**: Use a fórmula do ROUGE-N para calcular a métrica.\n",
    "\n",
    "### Exemplo de Avaliação usando ROUGE-N com Python\n",
    "\n",
    "Para calcular o ROUGE-N, você pode usar a biblioteca `rouge-score` do Google ou outras implementações, como `py-rouge`.\n",
    "\n",
    "#### Usando `rouge-score`\n",
    "\n",
    "Primeiro, instale a biblioteca:\n",
    "```bash\n",
    "pip install rouge-score\n",
    "```\n",
    "\n",
    "Em seguida, utilize o seguinte código para calcular o ROUGE-N:\n",
    "\n",
    "```python\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# Traduções de referência (humanas)\n",
    "references = [\n",
    "    \"this is a test\",\n",
    "    \"another test sentence\"\n",
    "]\n",
    "\n",
    "# Traduções geradas pelo modelo\n",
    "candidates = [\n",
    "    \"this is a test\",\n",
    "    \"a different test sentence\"\n",
    "]\n",
    "\n",
    "# Inicialize o avaliador ROUGE-N\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2'], use_stemmer=True)\n",
    "\n",
    "# Calcule o ROUGE-N para cada par de resumo gerado e de referência\n",
    "for ref, cand in zip(references, candidates):\n",
    "    scores = scorer.score(ref, cand)\n",
    "    print(f\"Reference: {ref}\")\n",
    "    print(f\"Candidate: {cand}\")\n",
    "    print(f\"ROUGE-1: {scores['rouge1']}\")\n",
    "    print(f\"ROUGE-2: {scores['rouge2']}\")\n",
    "    print(\"---\")\n",
    "```\n",
    "\n",
    "#### Saída Esperada:\n",
    "```plaintext\n",
    "Reference: this is a test\n",
    "Candidate: this is a test\n",
    "ROUGE-1: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
    "ROUGE-2: Score(precision=1.0, recall=1.0, fmeasure=1.0)\n",
    "---\n",
    "Reference: another test sentence\n",
    "Candidate: a different test sentence\n",
    "ROUGE-1: Score(precision=0.6, recall=0.6, fmeasure=0.6)\n",
    "ROUGE-2: Score(precision=0.5, recall=0.5, fmeasure=0.5)\n",
    "---\n",
    "```\n",
    "\n",
    "O ROUGE-N score é uma métrica eficiente e amplamente utilizada para avaliar a qualidade de resumos e traduções automáticas, medindo a sobreposição de n-gramas entre as saídas geradas pelo modelo e as referências humanas. Ele é particularmente útil para tarefas onde a correspondência exata de n-gramas é importante, como a sumarização e tradução de textos. A implementação do ROUGE-N é direta e pode ser facilmente realizada utilizando bibliotecas disponíveis em Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f053b8",
   "metadata": {},
   "source": [
    "## F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace11388",
   "metadata": {},
   "source": [
    "Como o BLEU é focado em precisão e o ROUGE-N é focado no recall, podemos combinar ambos e ter o F1 Score\n",
    "\n",
    "<img src=\"./imgs/f1_score.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad06140",
   "metadata": {},
   "source": [
    "## Sampling and Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d99bf4f",
   "metadata": {},
   "source": [
    "O Beam Search é uma técnica de **busca heurística** utilizada em modelos de tradução automática, sumarização, e outras tarefas de processamento de linguagem natural que envolvem a geração de sequências. Ele é especialmente útil para encontrar a sequência mais provável de palavras (ou tokens) que o modelo pode gerar, dado um estado inicial.\n",
    "\n",
    "O Beam Search expande a ideia de busca em largura, mantendo um número fixo de caminhos candidatos (ou hipóteses) em cada etapa da geração da sequência. Esse número é chamado de **beam width**. Em vez de explorar todas as possíveis expansões de cada estado, o Beam Search mantém apenas os melhores caminhos, reduzindo a complexidade computacional.\n",
    "\n",
    "### Etapas do Beam Search\n",
    "\n",
    "1. **Inicialização**:\n",
    "   - Começa com o estado inicial (geralmente o token `<sos>` ou um estado oculto inicial).\n",
    "   - Atribui uma pontuação (log-probabilidade) inicial a este estado.\n",
    "\n",
    "2. **Expansão**:\n",
    "   - Para cada estado atual no conjunto de caminhos candidatos, expande todas as possíveis próximas palavras (ou tokens).\n",
    "   - Calcula as pontuações dessas novas sequências, geralmente somando a log-probabilidade da nova palavra à pontuação do caminho até o momento.\n",
    "\n",
    "3. **Seleção**:\n",
    "   - Entre todas as novas sequências geradas, seleciona as `beam width` melhores com base nas suas pontuações.\n",
    "   - Descartam-se os caminhos com pontuações mais baixas.\n",
    "\n",
    "4. **Repetição**:\n",
    "   - Repete os passos de expansão e seleção até que se atinja um critério de parada, como gerar um token de parada (`<eos>`) ou atingir um número máximo de etapas.\n",
    "\n",
    "5. **Decodificação Final**:\n",
    "   - Após a última etapa, seleciona o caminho com a maior pontuação entre os caminhos candidatos restantes como a sequência gerada final.\n",
    "\n",
    "### Exemplo Simples\n",
    "\n",
    "Imagine um modelo de tradução onde queremos traduzir a frase \"I am\" para outra língua. Vamos utilizar o Beam Search com `beam width` de 2.\n",
    "\n",
    "**Passo 1: Inicialização**\n",
    "- Estado inicial: `<sos>`\n",
    "- Caminhos candidatos: [(`<sos>`, 0)]\n",
    "\n",
    "**Passo 2: Expansão e Seleção (Primeira Iteração)**\n",
    "- Expande `<sos>` para todas as palavras possíveis: \"Je\", \"Tu\", \"Il\", ...\n",
    "- Calcula as pontuações dessas expansões.\n",
    "- Seleciona os 2 melhores: [(\"Je\", -1.2), (\"Tu\", -1.5)]\n",
    "\n",
    "**Passo 3: Expansão e Seleção (Segunda Iteração)**\n",
    "- Expande cada um dos melhores candidatos: \"Je suis\", \"Je vais\", \"Tu es\", \"Tu vas\", ...\n",
    "- Calcula as pontuações acumuladas dessas expansões.\n",
    "- Seleciona os 2 melhores: [(\"Je suis\", -2.0), (\"Je vais\", -2.3)]\n",
    "\n",
    "**Passo 4: Repetição**\n",
    "- Continua expandindo e selecionando até atingir o critério de parada (número máximo de tokens ou token de parada).\n",
    "\n",
    "### Vantagens do Beam Search\n",
    "\n",
    "1. **Equilíbrio entre Exploration e Exploitation**: Mantém múltiplos caminhos candidatos, evitando prender-se a escolhas subótimas iniciais.\n",
    "2. **Melhoria da Qualidade da Sequência Gerada**: Aumenta a probabilidade de encontrar sequências mais naturais e coerentes em comparação com a busca gulosa (greedy search).\n",
    "3. **Controle de Complexidade**: O parâmetro `beam width` controla diretamente a quantidade de memória e tempo de computação necessários.\n",
    "\n",
    "### Desvantagens do Beam Search\n",
    "\n",
    "1. **Custo Computacional**: Mesmo com `beam width` moderado, pode ser computacionalmente intensivo.\n",
    "2. **Hipóteses Subótimas**: Ainda pode manter caminhos que eventualmente se tornam subótimos em comparação com a busca exaustiva.\n",
    "\n",
    "### Implementação em Python com PyTorch\n",
    "\n",
    "Aqui está um exemplo simplificado de implementação de Beam Search com um modelo seq2seq com atenção:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def beam_search(model, src, beam_width=3, max_len=20):\n",
    "    # Inicialização\n",
    "    src = src.unsqueeze(0)  # Adicionar dimensão de batch\n",
    "    encoder_outputs, hidden, cell = model.encoder(src)\n",
    "    \n",
    "    # Inicializar o beam com o estado inicial\n",
    "    beams = [([], hidden, cell, 0)]  # Cada beam é (seq, hidden, cell, score)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        new_beams = []\n",
    "        for seq, hidden, cell, score in beams:\n",
    "            # Se o token de parada foi gerado, mantenha o caminho\n",
    "            if seq and seq[-1] == model.eos_token:\n",
    "                new_beams.append((seq, hidden, cell, score))\n",
    "                continue\n",
    "\n",
    "            # Expansão\n",
    "            trg_input = torch.LongTensor([model.sos_token] if not seq else [seq[-1]]).unsqueeze(0)\n",
    "            output, hidden, cell = model.decoder(trg_input, hidden, cell, encoder_outputs)\n",
    "            topk = output.topk(beam_width)\n",
    "\n",
    "            # Adicionar novos caminhos ao beam\n",
    "            for i in range(beam_width):\n",
    "                next_seq = seq + [topk[1][0][i].item()]\n",
    "                next_score = score + topk[0][0][i].item()\n",
    "                new_beams.append((next_seq, hidden, cell, next_score))\n",
    "\n",
    "        # Selecionar os beam_width melhores caminhos\n",
    "        beams = sorted(new_beams, key=lambda x: x[-1], reverse=True)[:beam_width]\n",
    "\n",
    "    # Retornar o melhor caminho\n",
    "    return beams[0][0]\n",
    "```\n",
    "\n",
    "O Beam Search é uma técnica poderosa para a geração de sequências, melhorando a qualidade das saídas de modelos em tarefas de tradução e sumarização ao explorar múltiplos caminhos possíveis. Ele equilibra a eficiência computacional com a qualidade das sequências geradas, tornando-se uma escolha popular em muitos sistemas de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe4ebf",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a5f240",
   "metadata": {},
   "source": [
    "## Minimum Bayes Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc845fc",
   "metadata": {},
   "source": [
    "## Ungraded Lab: Stack Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7cd40b",
   "metadata": {},
   "source": [
    "## Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6b72e9",
   "metadata": {},
   "source": [
    "## NMT with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f1f4fb",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bc8b59",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ad034",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc26a70e",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Attention Models, disponível em https://www.coursera.org/learn/probabilistic-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ea092",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
