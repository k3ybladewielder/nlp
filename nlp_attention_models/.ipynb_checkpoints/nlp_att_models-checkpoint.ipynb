{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "838c52e5",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Attention Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Attention Models da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300c1ba",
   "metadata": {},
   "source": [
    "# Week 1 - Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d8658",
   "metadata": {},
   "source": [
    "## Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d5bd75",
   "metadata": {},
   "source": [
    "Os modelos Seq2Seq (Sequence-to-Sequence) são amplamente utilizados em tarefas de tradução automática (machine translation) e outras tarefas de NLP que envolvem a conversão de uma sequência de entrada em uma sequência de saída. Eles foram inicialmente propostos para tarefas como tradução automática, onde uma sequência de palavras em uma língua (entrada) é convertida em uma sequência de palavras em outra língua (saída). A arquitetura básica é composta por dois componentes principais:\n",
    "\n",
    "- **Encoder**: Transforma a sequência de entrada em um vetor de contexto.\n",
    "- **Decoder**: Utiliza o vetor de contexto para gerar a sequência de saída.\n",
    "\n",
    "<img src=\"./imgs/seq2seq.png\">\n",
    "\n",
    "### Arquitetura Seq2Seq\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "O encoder é tipicamente uma rede recorrente (RNN, LSTM, GRU) que lê a sequência de entrada token por token e gera um vetor de contexto que representa a informação da sequência inteira.\n",
    "\n",
    "- **Inputs**: Sequência de entrada $X = (x_1, x_2, ..., x_n)$.\n",
    "- **Outputs**: Vetor de contexto $C$ e estado oculto $h_t$ em cada passo de tempo.\n",
    "\n",
    "<img src=\"./imgs/seq2seq_encoder.png\">\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "O decoder é outra rede recorrente que gera a sequência de saída um token por vez, **usando o vetor de contexto do encoder** e o **estado oculto anterior do decoder**. A sequencia de entrada no decoder inicia com token $\\text{<SOS>}$ (Start of Sequence)\n",
    "\n",
    "- **Inputs**: O vetor de contexto do encoder, o estado oculto anterior e o token de entrada anterior.\n",
    "- **Outputs**: Token de saída e novo estado oculto.\n",
    "\n",
    "<img src=\"./imgs/seq2seq_decoder.png\">\n",
    "\n",
    "#### Attention\n",
    "\n",
    "O mecanismo de atenção foi introduzido para melhorar a performance dos modelos Seq2Seq, permitindo que o modelo foque em diferentes partes da entrada enquanto gera cada token da saída.\n",
    "\n",
    "- **Atenção Básica**: Calcula um conjunto de pesos de atenção que indicam a importância de cada token da sequência de entrada para a geração do próximo token da saída.\n",
    "- **Context Vector**: Um vetor de contexto ponderado é calculado como uma combinação linear dos estados ocultos do encoder, ponderado pelos pesos de atenção.\n",
    "\n",
    "Matematicamente, a atenção pode ser descrita da seguinte maneira:\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})} $$\n",
    "onde $e_{ij} = a(s_{i-1}, h_j)$ é a função de alinhamento que pode ser um simples perceptron.\n",
    "\n",
    "<img src=\"./imgs/seq2seq_attention.png\">\n",
    "\n",
    "### Aplicação em Machine Translation\n",
    "\n",
    "Na tradução automática, o processo funciona da seguinte maneira:\n",
    "\n",
    "1. **Encoding**: O encoder lê a sequência de entrada (frase na língua de origem) e gera um vetor de contexto.\n",
    "2. **Attention**: Durante a decodificação, o mecanismo de atenção calcula os pesos de atenção que focam nas partes relevantes da sequência de entrada.\n",
    "3. **Decoding**: O decoder gera a sequência de saída (frase na língua de destino), token por token, utilizando o vetor de contexto ponderado pela atenção.\n",
    "\n",
    "### Exemplo Simplificado\n",
    "\n",
    "1. **Entrada**: \"I love NLP\"\n",
    "2. **Encoding**: O encoder gera vetores de estado oculto para cada token e um vetor de contexto geral.\n",
    "3. **Attention**: Para cada token gerado na saída, calcula-se a importância de cada token da entrada.\n",
    "4. **Decoding**: Gera a saída \"Yo amo PLN\" utilizando os vetores de contexto e atenção.\n",
    "\n",
    "### Benefícios do Mecanismo de Atenção\n",
    "\n",
    "- **Melhora a Tradução**: Permite que o modelo foque nas partes mais relevantes da entrada.\n",
    "- **Mitiga Problemas de Longas Sequências**: Ajuda a manter a informação relevante mesmo em sequências longas.\n",
    "\n",
    "Os modelos Seq2Seq com atenção são fundamentais para muitas tarefas de NLP, especialmente a tradução automática. A arquitetura básica de encoder-decoder, aprimorada com o mecanismo de atenção, permite que esses modelos lidem eficazmente com a complexidade das sequências de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23c98b1",
   "metadata": {},
   "source": [
    "## Seq2seq Model with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1df18c",
   "metadata": {},
   "source": [
    "## Background on seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee308e60",
   "metadata": {},
   "source": [
    "## Queries, Keys, Values, and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faa5b02",
   "metadata": {},
   "source": [
    "## Setup for Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cde2a9",
   "metadata": {},
   "source": [
    "## Teacher Forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0cdc5",
   "metadata": {},
   "source": [
    "## NMT Model with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431a5d2",
   "metadata": {},
   "source": [
    "## BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21ba0fb",
   "metadata": {},
   "source": [
    "## ROUGE-N Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c911918e",
   "metadata": {},
   "source": [
    "## Sampling and Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ae342",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584183b",
   "metadata": {},
   "source": [
    "## Minimum Bayes Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44bcaa9",
   "metadata": {},
   "source": [
    "## Ungraded Lab: Stack Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ff837",
   "metadata": {},
   "source": [
    "## Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c330a61",
   "metadata": {},
   "source": [
    "## NMT with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22425c8f",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e06a4ae",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5033abb",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d4d19a",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Attention Models, disponível em https://www.coursera.org/learn/probabilistic-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ac483c",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
