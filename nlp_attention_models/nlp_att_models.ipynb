{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2ab1e57",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Attention Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Attention Models da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ea1a47",
   "metadata": {},
   "source": [
    "# Week 1 - Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a821b9",
   "metadata": {},
   "source": [
    "## Seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdbc125",
   "metadata": {},
   "source": [
    "Os modelos Seq2Seq (Sequence-to-Sequence) são amplamente utilizados em tarefas de tradução automática (machine translation) e outras tarefas de NLP que envolvem a conversão de uma sequência de entrada em uma sequência de saída. Eles foram inicialmente propostos para tarefas como tradução automática, onde uma sequência de palavras em uma língua (entrada) é convertida em uma sequência de palavras em outra língua (saída). A arquitetura básica é composta por dois componentes principais:\n",
    "\n",
    "- **Encoder**: Transforma a sequência de entrada em um vetor de contexto.\n",
    "- **Decoder**: Utiliza o vetor de contexto para gerar a sequência de saída.\n",
    "\n",
    "<img src=\"./imgs/seq2seq.png\">\n",
    "\n",
    "### Arquitetura Seq2Seq\n",
    "\n",
    "#### Encoder\n",
    "\n",
    "O encoder é tipicamente uma rede recorrente (RNN, LSTM, GRU) que lê a sequência de entrada token por token e gera um vetor de contexto que representa a informação da sequência inteira.\n",
    "\n",
    "- **Inputs**: Sequência de entrada $X = (x_1, x_2, ..., x_n)$.\n",
    "- **Outputs**: Vetor de contexto $C$ e estado oculto $h_t$ em cada passo de tempo.\n",
    "\n",
    "<img src=\"./imgs/seq2seq_encoder.png\">\n",
    "\n",
    "#### Decoder\n",
    "\n",
    "O decoder é outra rede recorrente que gera a sequência de saída um token por vez, **usando o vetor de contexto do encoder** e o **estado oculto anterior do decoder**. A sequencia de entrada no decoder inicia com token $\\text{<SOS>}$ (Start of Sequence)\n",
    "\n",
    "- **Inputs**: O vetor de contexto do encoder, o estado oculto anterior e o token de entrada anterior.\n",
    "- **Outputs**: Token de saída e novo estado oculto.\n",
    "\n",
    "<img src=\"./imgs/seq2seq_decoder.png\">\n",
    "\n",
    "#### Attention\n",
    "\n",
    "O mecanismo de atenção foi introduzido para melhorar a performance dos modelos Seq2Seq, permitindo que o modelo foque em diferentes partes da entrada enquanto gera cada token da saída.\n",
    "\n",
    "- **Atenção Básica**: Calcula um conjunto de pesos de atenção que indicam a importância de cada token da sequência de entrada para a geração do próximo token da saída.\n",
    "- **Context Vector**: Um vetor de contexto ponderado é calculado como uma combinação linear dos estados ocultos do encoder, ponderado pelos pesos de atenção.\n",
    "\n",
    "Matematicamente, a atenção pode ser descrita da seguinte maneira:\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(e_{ij})}{\\sum_{k=1}^{T_x} \\exp(e_{ik})} $$\n",
    "onde $e_{ij} = a(s_{i-1}, h_j)$ é a função de alinhamento que pode ser um simples perceptron.\n",
    "\n",
    "<img src=\"./imgs/seq2seq_attention.png\">\n",
    "\n",
    "### Aplicação em Machine Translation\n",
    "\n",
    "Na tradução automática, o processo funciona da seguinte maneira:\n",
    "\n",
    "1. **Encoding**: O encoder lê a sequência de entrada (frase na língua de origem) e gera um vetor de contexto.\n",
    "2. **Attention**: Durante a decodificação, o mecanismo de atenção calcula os pesos de atenção que focam nas partes relevantes da sequência de entrada.\n",
    "3. **Decoding**: O decoder gera a sequência de saída (frase na língua de destino), token por token, utilizando o vetor de contexto ponderado pela atenção.\n",
    "\n",
    "### Exemplo Simplificado\n",
    "\n",
    "1. **Entrada**: \"I love NLP\"\n",
    "2. **Encoding**: O encoder gera vetores de estado oculto para cada token e um vetor de contexto geral.\n",
    "3. **Attention**: Para cada token gerado na saída, calcula-se a importância de cada token da entrada.\n",
    "4. **Decoding**: Gera a saída \"Yo amo PLN\" utilizando os vetores de contexto e atenção.\n",
    "\n",
    "### Benefícios do Mecanismo de Atenção\n",
    "\n",
    "- **Melhora a Tradução**: Permite que o modelo foque nas partes mais relevantes da entrada.\n",
    "- **Mitiga Problemas de Longas Sequências**: Ajuda a manter a informação relevante mesmo em sequências longas.\n",
    "\n",
    "Os modelos Seq2Seq com atenção são fundamentais para muitas tarefas de NLP, especialmente a tradução automática. A arquitetura básica de encoder-decoder, aprimorada com o mecanismo de atenção, permite que esses modelos lidem eficazmente com a complexidade das sequências de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27ac2d",
   "metadata": {},
   "source": [
    "## Queries, Keys, Values, and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88ab550",
   "metadata": {},
   "source": [
    "A camada de atenção calcula um **vetor de atenção que pondera a importância de cada token da sequência de entrada para a geração de cada token da sequência de saída**. \n",
    "\n",
    "#### Inputs da Camada de Atenção\n",
    "\n",
    "A camada de atenção recebe três inputs principais:\n",
    "- **Queries (Q)**: Vetores de consulta que representam os tokens da **sequência de saída** atual.\n",
    "- **Keys (K)**: Vetores de chave que representam os tokens da **sequência de entrada**.\n",
    "- **Values (V)**: Vetores de valor que também representam os tokens da **sequência de entrada**.\n",
    "\n",
    "<img src=\"./imgs/qkv.png\">\n",
    "\n",
    "Em um modelo Seq2Seq com atenção, **os estados ocultos do decoder são usados como Queries**, enquanto **os estados ocultos do encoder são usados como Keys e Values**.\n",
    "\n",
    "#### Cálculo das Similaridades (Scores)\n",
    "\n",
    "Primeiro, calculamos uma pontuação (score) ou alinhamento que mede a **similaridade entre cada par de query e key**. Uma função de similaridade comum é o produto escalar (dot product):\n",
    "\n",
    "$$ \\text{score}(Q, K) = QK^T $$\n",
    "\n",
    "Outra opção é o produto escalar escalado (scaled dot product), que é comum em Transformers:\n",
    "\n",
    "$$ \\text{score}(Q, K) = \\frac{QK^T}{\\sqrt{d_k}} $$\n",
    "\n",
    "onde $ d_k $ é a dimensão dos vetores de chave.\n",
    "\n",
    "#### Normalização dos scores\n",
    "\n",
    "As pontuações escaladas são então normalizadas usando a função softmax para obter os pesos de atenção:\n",
    "\n",
    "$$ \\alpha = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) $$\n",
    "\n",
    "Isso transforma as pontuações escaladas em um conjunto de pesos que somam 1, facilitando a interpretação como probabilidades.\n",
    "\n",
    "#### Cálculo dos Pesos de Atenção\n",
    "\n",
    "As pontuações são então normalizadas usando uma função softmax para obter os pesos de atenção, que representam a importância relativa de cada token de entrada para cada token de saída:\n",
    "\n",
    "$$ \\alpha_{ij} = \\frac{\\exp(\\text{score}(q_i, k_j))}{\\sum_{k=1}^{n} \\exp(\\text{score}(q_i, k_k))} $$\n",
    "\n",
    "onde $ \\alpha_{ij} $ é o peso de atenção para o $ i $-ésimo token de saída e o $ j $-ésimo token de entrada.\n",
    "\n",
    "#### Cálculo do Vetor de Contexto\n",
    "\n",
    "Finalmente, os pesos de atenção são usados para calcular uma combinação ponderada dos vetores de valor:\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "O resultado é uma matriz que representa a combinação ponderada dos valores, ajustada de acordo com a relevância calculada pelas pontuações de atenção.\n",
    "\n",
    "Os pesos de atenção são então usados para calcular uma **combinação ponderada dos vetores de valor**, resultando em um vetor de contexto para cada token de saída:\n",
    "\n",
    "$$ \\text{context}_i = \\sum_{j=1}^{n} \\alpha_{ij} v_j $$\n",
    "\n",
    "#### Atenção Multi-cabeça (Multi-head Attention)\n",
    "\n",
    "Em arquiteturas como Transformers, é comum usar várias cabeças de atenção para capturar diferentes tipos de relações entre tokens. Cada cabeça de atenção realiza o processo descrito acima de forma independente, e os resultados são concatenados e transformados:\n",
    "\n",
    "$$ \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)W^O $$\n",
    "\n",
    "onde cada cabeça de atenção $ \\text{head}_i $ é calculada como:\n",
    "\n",
    "$$ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $$\n",
    "\n",
    "### Tipos de Atenção\n",
    "\n",
    "Existem diferentes variações do mecanismo de atenção, incluindo:\n",
    "\n",
    "1. **Self-attention**: Quando Queries, Keys e Values vêm da mesma sequência. Utilizado principalmente em Transformers.\n",
    "2. **Cross-attention**: Quando Queries vêm de uma sequência (ex. saída) e Keys/Values vêm de outra (ex. entrada). Utilizado em modelos Seq2Seq com atenção.\n",
    "\n",
    "### Implementação Simplificada\n",
    "\n",
    "Aqui está um exemplo de implementação simplificada de uma camada de atenção em pseudo-código:\n",
    "\n",
    "```python\n",
    "def attention(Q, K, V):\n",
    "    # Cálculo das pontuações\n",
    "    scores = dot_product(Q, K.T) / sqrt(d_k)\n",
    "    \n",
    "    # Normalização das pontuações\n",
    "    attention_weights = softmax(scores, axis=-1)\n",
    "    \n",
    "    # Cálculo do vetor de contexto\n",
    "    context = dot_product(attention_weights, V)\n",
    "    \n",
    "    return context, attention_weights\n",
    "```\n",
    "\n",
    "### Benefícios da Camada de Atenção\n",
    "\n",
    "- **Foco Dinâmico**: Permite que o modelo se concentre nas partes mais relevantes da entrada.\n",
    "- **Eficiência Computacional**: Self-attention, especialmente em Transformers, é altamente paralelizável.\n",
    "- **Flexibilidade**: Pode ser adaptada para várias tarefas de NLP, como tradução, resumo e resposta a perguntas.\n",
    "\n",
    "### Conclusão\n",
    "\n",
    "A camada de atenção é um componente poderoso que melhora a capacidade dos modelos de NLP em capturar dependências complexas dentro das sequências de entrada. Ao permitir que o modelo foque dinamicamente nas partes mais relevantes da entrada, a atenção desempenha um papel crucial em muitos dos avanços recentes em NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07514de5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-11T22:54:02.465276Z",
     "start_time": "2024-07-11T22:54:02.459965Z"
    }
   },
   "source": [
    "## Material complementar\n",
    "- [Visualizando a atenção, o coração de um transformador](https://www.youtube.com/watch?v=eMlx5fFNoYc)\n",
    "- [Atenção para Redes Neurais, Claramente Explicadas!!!](https://www.youtube.com/watch?v=PSs6nxngL6k)\n",
    "- [The math behind Attention: Keys, Queries, and Values matrices](https://www.youtube.com/watch?v=UPtG_38Oq8o&t=669s)\n",
    "- [The Transformer neural network architecture EXPLAINED. “Attention is all you need”](https://www.youtube.com/watch?v=FWFA4DGuzSc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669901e",
   "metadata": {},
   "source": [
    "## Setup for Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d55934e",
   "metadata": {},
   "source": [
    "## Teacher Forcing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bf483e",
   "metadata": {},
   "source": [
    "## NMT Model with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc267a9b",
   "metadata": {},
   "source": [
    "## BLEU Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a25ef",
   "metadata": {},
   "source": [
    "## ROUGE-N Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e85b8c",
   "metadata": {},
   "source": [
    "## Sampling and Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edebfa3",
   "metadata": {},
   "source": [
    "## Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fe49a4",
   "metadata": {},
   "source": [
    "## Minimum Bayes Risk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f075e5",
   "metadata": {},
   "source": [
    "## Ungraded Lab: Stack Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d26e2b",
   "metadata": {},
   "source": [
    "## Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0612ae",
   "metadata": {},
   "source": [
    "## NMT with Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f65fad",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aff72a",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88271d8",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9478fd",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Attention Models, disponível em https://www.coursera.org/learn/probabilistic-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19e855c",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
