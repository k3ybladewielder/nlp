{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea619bbb",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Classification and Vector Spaces\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Classification and Vector Spaces da DeeplearninigAI.\n",
    "\n",
    "Repositório com a trilha Natural Language Processing:\n",
    "https://github.com/k3ybladewielder/nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587bcc1e",
   "metadata": {},
   "source": [
    "# Week 1 - Sentiment Analysis With Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7dc70b",
   "metadata": {},
   "source": [
    "## Objetivos de aprendizagem\n",
    "- Sentiment analysis\n",
    "- Logistic regression\n",
    "- Data pre-processing\n",
    "- Calculating word frequencies\n",
    "- Feature extraction\n",
    "- Vocabulary creation\n",
    "- Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc6444a",
   "metadata": {},
   "source": [
    "## Supervised ML and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d126bfa",
   "metadata": {},
   "source": [
    "Os algoritmos de **Machine Learning (ML)** supervisionados são um tipo de algoritmo que aprende a partir de dados rotulados, ou seja, dados que já possuem um rótulo ou uma classificação pré-definida. Esses algoritmos usam esses dados rotulados para aprender a fazer previsões ou classificações em novos dados.\n",
    "\n",
    "Na área de Processamento de Linguagem Natural (NLP), os algoritmos de ML supervisionados são usados para uma variedade de tarefas, como classificação de sentimentos, identificação de entidades nomeadas, análise de tópicos, tradução automática, entre outras.\n",
    "\n",
    "Um exemplo de como esses algoritmos são usados em NLP é na classificação de sentimentos em textos. Nesse caso, um modelo de ML supervisionado seria **treinado em um conjunto de dados rotulados** que contém textos e suas respectivas classificações de sentimento (por exemplo, positivo, negativo ou neutro). O algoritmo usaria esses dados para **aprender a reconhecer padrões nos textos** e, em seguida, aplicaria esses padrões para **classificar o sentimento em novos textos**.\n",
    "\n",
    "Outro exemplo é na identificação de **entidades nomeadas (NER)**, que é uma tarefa que envolve a identificação de nomes de pessoas, locais, organizações e outras entidades em um texto. Nesse caso, um modelo de ML supervisionado seria treinado em um conjunto de dados rotulados que contém textos e suas respectivas entidades nomeadas. O algoritmo usaria esses dados para aprender a reconhecer os padrões de palavras e contextos que indicam a presença de uma entidade nomeada em um texto, e, em seguida, aplicaria esses padrões para identificar entidades em novos textos.\n",
    "\n",
    "Em resumo, os algoritmos de ML supervisionados são uma técnica poderosa para resolver problemas em NLP, permitindo que modelos aprendam com dados rotulados e possam fazer previsões ou classificações em novos dados com base no que foi aprendido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602de673",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/supervised_ml.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fff80d",
   "metadata": {},
   "source": [
    "Um pipeline básico para a _task_ de análise de sentimento (classificação) geralmente envolve as etapas de:\n",
    "- **Pré-processamento de texto**: Esta etapa envolve a limpeza e preparação dos dados. O texto é normalmente convertido em minúsculas, removidos caracteres especiais, removidos números e pontuações, e realizada a tokenização do texto para obter as palavras individuais.\n",
    "- **Criação de features**: Nesta etapa, são criadas as features que serão usadas pelo modelo de classificação. As features podem incluir a contagem de palavras, a frequência de palavras, o tipo de palavras, o uso de negação, entre outras.\n",
    "- **Treinamento do modelo**: O modelo de classificação é treinado em um conjunto de dados rotulados que contêm exemplos de texto e suas respectivas classificações de sentimento. Existem vários algoritmos de aprendizado de máquina que podem ser usados para treinar um modelo, como Árvores de Decisão, Naive Bayes, Regressão Logística, SVM, Redes Neurais, etc.\n",
    "- **Avaliação do modelo**: Após o treinamento, o modelo é avaliado em um conjunto de dados de teste para verificar sua precisão. É comum dividir o conjunto de dados em conjunto de treinamento, validação e teste.\n",
    "- **Implantação**: Finalmente, o modelo treinado é usado para classificar o sentimento de novos textos. Novos textos passam pelas mesmas etapas de pré-processamento e criação de features, e o modelo treinado é usado para prever a classificação de sentimento do texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e75a3a",
   "metadata": {},
   "source": [
    "## Vocabulary and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f87bba",
   "metadata": {},
   "source": [
    "Para representar textos de forma numérica, primeiro precisamos construir um vocabulário, com eles poderemos _encodar_ qualquer texto como um array de números. Um Vocabulário é uma lista com palavras únicas, não repetidas.\n",
    "\n",
    "Uma forma simples de extrair features do texto é usando o vocabulário, verificando cada palavra do vocabulário que aparece no texto. Caso as palavras no texto que estamos extraindo a feature tenham apareca no vocabulário, atribuímos o valor 1 para ela, e zero para as palavras que do vocabulário que não aparecem no texto. Assim, estamos representando o texto usando [**one-hot encoding**](https://k3ybladewielder.medium.com/introdu%C3%A7%C3%A3o-%C3%A0-nlp-4d7d98b9a36a) ou representação esparsa. Mas esse método pode ser problemático porquê o número de features é igual ao número de palavras no vocabulário e a grande maioria das features serão zero, aumentando excessivamente o tempo de treino e predição dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722752d1",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/vocabulary_and_feature_extraction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae2039",
   "metadata": {},
   "source": [
    "Existem diversas técnicas para representar textos como vetores, sendo as mais comuns a Bag of Words (BoW) e a Representação Distribuída de Palavras (Word Embeddings). Vou explicar brevemente cada uma delas:\n",
    "- **Bag of Words (BoW)**: Nessa técnica, o texto é representado como um vetor contendo a contagem de ocorrências de cada palavra presente no texto. Cada palavra é considerada como uma dimensão do vetor. Dessa forma, quanto mais vezes uma palavra aparecer no texto, maior será o valor correspondente na dimensão correspondente no vetor. Por exemplo, se um texto contém as palavras \"gato\", \"cão\" e \"casa\", a representação BoW seria um vetor com três dimensões, com valores correspondentes à contagem de ocorrências de cada palavra no texto.\n",
    "- **Word Embeddings**: Essa técnica é baseada em modelos de linguagem neural, que mapeiam cada palavra em um espaço vetorial de alta dimensão, onde palavras semelhantes têm representações próximas. A ideia é que cada palavra seja representada por um vetor de números reais que captura seu significado semântico. Esses vetores podem ser aprendidos a partir de grandes quantidades de textos usando técnicas de aprendizado de máquina, como Word2Vec, GloVe ou FastText. Os vetores resultantes podem ser usados para representar cada palavra em um texto como um vetor numérico. A representação distribuída de palavras pode capturar relações semânticas entre palavras, como sinonímia e antonímia, e pode ser usada para tarefas mais complexas, como análise de sentimento ou classificação de texto.\n",
    "\n",
    "Ambas as técnicas são amplamente utilizadas em NLP, dependendo do objetivo e do contexto da tarefa em questão. A escolha da técnica de representação de texto pode influenciar significativamente o desempenho do modelo de aprendizado de máquina, e é importante escolher a técnica mais adequada para a tarefa específica.\n",
    "\n",
    "Tanto a técnica de Bag of Words (BoW) quanto a Word Embeddings têm seus **pontos fortes e fracos**, e a escolha de qual usar depende do contexto e do objetivo da tarefa em questão.\n",
    "\n",
    "A representação **BoW** pode ser uma escolha adequada para **tarefas simples de classificação de texto**, como classificação de spam ou análise de sentimento, onde a presença ou ausência de palavras específicas pode ser um indicador importante para a classificação. Além disso, a representação **BoW é computacionalmente eficiente e fácil de interpretar**, o que pode ser uma vantagem para problemas onde a transparência do modelo é importante. No entanto, a representação **BoW não leva em consideração a ordem das palavras no texto, o que pode limitar sua capacidade de capturar nuances semânticas.**\n",
    "\n",
    "Já Word Embedding é mais adequada para **tarefas que envolvem análise semântica**, como tradução automática, classificação de tópicos ou análise de sentimento baseada em frases complexas. A representação Word Embedding **leva em consideração a ordem e o contexto das palavras**, e **pode capturar a similaridade semântica entre palavras que não aparecem juntas com frequência**. Além disso, a representação distribuída de palavras **pode ser usada para inicializar redes neurais em tarefas de aprendizado profundo**, melhorando o desempenho do modelo. No entanto, Word Embedding **pode ser computacionalmente intensiva e requer grandes quantidades de dados de treinamento para obter bons resultados**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9dcfc1",
   "metadata": {},
   "source": [
    "## Feature Extraction with Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab556a",
   "metadata": {},
   "source": [
    "Numa task de classificação de sentimentos, podemos identificar as palavras positivas e negativas a partir da frequencia de ocorrencia em que elas ocorrem nos textos positivos e negativos. Usando essa contagem, podemos extrair features e usá-las no modelo de classificação, como a **regressão logística**. \n",
    "\n",
    "<img src=\"./imgs/tweet_corpus.png\">\n",
    "\n",
    "A partir da contagem da frequencia das palavras em cada classe, chegamos a essa tabela. Na prática, essa tabela será um dicionário que mapeia a classe da palavra e sua frequência de ocorrência.\n",
    "\n",
    "<img src=\"./imgs/word_freq_tweets.png\">\n",
    "\n",
    "Podemos representar essa tabela de frequencia com um array com 3 features, aumentando a velocidade na implementação, porquê em vez de termos v features, teremos apenas 3 para que o modelo aprenda. \n",
    "\n",
    "Aqui, a primeira feature é um bias, depois o somatório das palavras da label positiva e o somatório das palavras da label negativa. Assim, teremos o novo vetor com 3 features.\n",
    "\n",
    "<img src=\"./imgs/vector_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd10ce",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/feature_extraction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6fc801",
   "metadata": {},
   "source": [
    "Essa abordagem apesar de ser bem simples é genial, e tem as seguintes vantagens:\n",
    "\n",
    "1. **Simplicidade e Interpretabilidade:**\n",
    "   * A técnica é relativamente simples de implementar e entender, mesmo para aqueles sem profundo conhecimento em machine learning.\n",
    "   * A utilização de frequências de palavras como características facilita a interpretação dos resultados, pois permite identificar quais palavras positivas e negativas contribuem mais para a classificação de um tweet.\n",
    "\n",
    "2. **Eficiência:**\n",
    "   * A extração de frequências de palavras é computacionalmente eficiente, especialmente quando comparada a métodos mais complexos de representação textual, como modelos neurais.\n",
    "   * Isso torna a abordagem adequada para datasets grandes e para implementações em tempo real.\n",
    "\n",
    "3. **Robustez:**\n",
    "   * A abordagem é robusta a ruídos e faltas de dados, pois se baseia na contagem de palavras, que é menos sensível a pequenas alterações no texto do que outros métodos.\n",
    "   * Isso a torna útil para lidar com tweets curtos ou com erros ortográficos.\n",
    "\n",
    "4. **Flexibilidade:**\n",
    "   * A técnica pode ser facilmente adaptada para outras tarefas de classificação binária de texto, como detecção de spam ou análise de opiniões.\n",
    "   * Basta ajustar o conjunto de palavras positivas e negativas de acordo com o contexto da tarefa.\n",
    "\n",
    "5. **Desempenho:**\n",
    "   * Apesar da simplicidade, a modelagem com frequências de palavras pode alcançar resultados competitivos em comparação com métodos mais complexos, especialmente para datasets com boa distribuição de classes.\n",
    "\n",
    "Mas ela também possui suas limitações, como:\n",
    "1. **Falta de Consideração da Ordem das Palavras:** A abordagem não leva em conta a ordem das palavras no tweet, o que pode ser importante para capturar o sentimento.\n",
    "2. **Sensibilidade a Sinônimos e Sarcasmo:** A técnica pode não capturar bem o sentimento de tweets que utilizam sinônimos para expressar sentimentos positivos ou negativos, ou que empregam sarcasmo.\n",
    "3. **Desempenho Inferior em Textos Complexos:** Para textos longos e complexos, a simples contagem de palavras pode não ser suficiente para capturar nuances do sentimento.\n",
    "\n",
    "Ainda assim, é um ótimo baseline, e caso seja necessário, podemos escalar para abordagens mais complexas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b2abd95",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:20.916304Z",
     "start_time": "2024-04-12T22:26:20.884437Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exemplo de tweets e seus respectivos sentimentos\n",
    "tweets = [\n",
    "    (\"Eu amo o meu cachorro\", \"positivo\"),\n",
    "    (\"Eu odeio acordar cedo\", \"negativo\"),\n",
    "    (\"A comida deste restaurante é incrível\", \"positivo\"),\n",
    "    (\"Estou cansado de estudar\", \"negativo\"),\n",
    "    (\"Que dia lindo para passear no parque\", \"positivo\"),\n",
    "    (\"O filme que vi ontem foi excelente\", \"positivo\"),\n",
    "    (\"Não vejo a hora de encontrar meus amigos\", \"positivo\"),\n",
    "    (\"Estou muito feliz com os resultados\", \"positivo\"),\n",
    "    (\"Que tristeza ver essa notícia\", \"negativo\"),\n",
    "    (\"Estou decepcionado com o serviço dessa empresa\", \"negativo\"),\n",
    "    (\"A chuva estragou meu dia\", \"negativo\"),\n",
    "    (\"Fiquei surpreso com o presente que recebi\", \"positivo\"),\n",
    "    (\"Esse livro é incrível, não consigo parar de ler\", \"positivo\"),\n",
    "    (\"Estou preocupado com o futuro do país\", \"negativo\"),\n",
    "    (\"Adorei o novo restaurante que experimentei\", \"positivo\"),\n",
    "    (\"O trânsito está terrível hoje\", \"negativo\"),\n",
    "    (\"Essa música me faz sentir feliz\", \"positivo\"),\n",
    "    (\"Perdi o ônibus e vou me atrasar para o trabalho\", \"negativo\"),\n",
    "    (\"Estou ansioso para o feriado chegar\", \"positivo\"),\n",
    "    (\"Que vergonha, esqueci meu aniversário de casamento\", \"negativo\"),\n",
    "    (\"Fui promovido no trabalho, estou radiante\", \"positivo\"),\n",
    "    (\"Não suporto essa pessoa, ela é muito arrogante\", \"negativo\"),\n",
    "    (\"O jantar que preparei ficou delicioso\", \"positivo\"),\n",
    "    (\"Meu time perdeu o jogo, estou arrasado\", \"negativo\"),\n",
    "    (\"Que saudade de casa\", \"negativo\"),\n",
    "    (\"Estou animado para o final de semana\", \"positivo\"),\n",
    "    (\"Esse filme é terrível, não recomendo\", \"negativo\"),\n",
    "    (\"A festa de aniversário foi um sucesso\", \"positivo\"),\n",
    "    (\"Estou cansado de tanto trabalhar\", \"negativo\"),\n",
    "    (\"Que alívio, finalmente terminei meu projeto\", \"positivo\"),\n",
    "    (\"Esse lugar é incrível, preciso voltar mais vezes\", \"positivo\"),\n",
    "    (\"Perdi meu voo e agora estou preso no aeroporto\", \"negativo\"),\n",
    "    (\"Estou feliz por ter encontrado um novo emprego\", \"positivo\"),\n",
    "    (\"Não aguento mais essa situação\", \"negativo\"),\n",
    "    (\"O passeio de barco foi maravilhoso\", \"positivo\"),\n",
    "    (\"Estou preocupado com a saúde dos meus pais\", \"negativo\"),\n",
    "    (\"Adorei a nova série que comecei a assistir\", \"positivo\"),\n",
    "    (\"Não vejo a hora de começar minhas férias\", \"positivo\"),\n",
    "    (\"Estou chateado com o cancelamento do evento\", \"negativo\"),\n",
    "    (\"Ganhei um prêmio, estou emocionado\", \"positivo\"),\n",
    "    (\"Esse restaurante não vale o preço que cobram\", \"negativo\"),\n",
    "    (\"Estou apaixonado por essa música\", \"positivo\"),\n",
    "    (\"Não gostei do atendimento dessa loja\", \"negativo\"),\n",
    "    (\"Meu cachorro está doente, estou preocupado\", \"negativo\"),\n",
    "    (\"Adorei o presente que ganhei de aniversário\", \"positivo\"),\n",
    "    (\"Estou cansado de tanto estudar para as provas\", \"negativo\"),\n",
    "    (\"Que alegria, hoje é meu aniversário\", \"positivo\"),\n",
    "    (\"Não vejo a hora de conhecer meu sobrinho que está a caminho\", \"positivo\"),\n",
    "    (\"Estou frustrado com o atraso no projeto\", \"negativo\"),\n",
    "    (\"Que bom ver meus amigos depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Não aguento mais essa dor de cabeça\", \"negativo\"),\n",
    "    (\"O pôr do sol na praia é magnífico\", \"positivo\"),\n",
    "    (\"Estou triste com a perda do meu animal de estimação\", \"negativo\"),\n",
    "    (\"Que maravilha, vou me formar na faculdade\", \"positivo\"),\n",
    "    (\"Estou desanimado com o futuro da economia\", \"negativo\"),\n",
    "    (\"O concerto de música clássica foi incrível\", \"positivo\"),\n",
    "    (\"Não suporto mais essa discussão\", \"negativo\"),\n",
    "    (\"Estou feliz por ter começado a fazer exercícios\", \"positivo\"),\n",
    "    (\"O tráfego está caótico hoje\", \"negativo\"),\n",
    "    (\"Estou radiante com a notícia do meu amigo\", \"positivo\"),\n",
    "    (\"Esse filme me deixou emocionado\", \"positivo\"),\n",
    "    (\"Estou decepcionado com o resultado da eleição\", \"negativo\"),\n",
    "    (\"Que bom encontrar você aqui\", \"positivo\"),\n",
    "    (\"Não gostei do sabor desse prato\", \"negativo\"),\n",
    "    (\"Estou contente por ter conhecido novas pessoas\", \"positivo\"),\n",
    "    (\"Estou estressado com o excesso de trabalho\", \"negativo\"),\n",
    "    (\"Essa viagem foi incrível, quero repetir\", \"positivo\"),\n",
    "    (\"Não aguento mais a rotina cansativa\", \"negativo\"),\n",
    "    (\"Estou preocupado com a segurança da minha cidade\", \"negativo\"),\n",
    "    (\"Que felicidade, consegui uma promoção no trabalho\", \"positivo\"),\n",
    "    (\"Não gosto desse clima frio\", \"negativo\"),\n",
    "    (\"Estou maravilhado com a beleza desse lugar\", \"positivo\"),\n",
    "    (\"Estou triste com a partida do meu amigo\", \"negativo\"),\n",
    "    (\"Adorei o show que fui ontem à noite\", \"positivo\"),\n",
    "    (\"Estou irritado com o comportamento dessa pessoa\", \"negativo\"),\n",
    "    (\"Que alegria, vou passar as férias na praia\", \"positivo\"),\n",
    "    (\"Não consigo parar de pensar naquela situação\", \"negativo\"),\n",
    "    (\"Estou ansioso para o lançamento do novo filme\", \"positivo\"),\n",
    "    (\"Não gostei do resultado do jogo\", \"negativo\"),\n",
    "    (\"Estou encantado com a cultura desse país\", \"positivo\"),\n",
    "    (\"Estou cansado de tantas brigas\", \"negativo\"),\n",
    "    (\"Que bom estar de volta em casa\", \"positivo\"),\n",
    "    (\"Não suporto mais esse barulho\", \"negativo\"),\n",
    "    (\"Estou empolgado para a festa de aniversário\", \"positivo\"),\n",
    "    (\"Estou preocupado com o meio ambiente\", \"negativo\"),\n",
    "    (\"Adorei o presente que ganhei de Natal\", \"positivo\"),\n",
    "    (\"Estou frustrado com a demora no atendimento\", \"negativo\"),\n",
    "    (\"Que alívio, consegui resolver o problema\", \"positivo\"),\n",
    "    (\"Não vejo a hora de voltar para casa\", \"negativo\"),\n",
    "    (\"Estou emocionado com o nascimento do meu sobrinho\", \"positivo\"),\n",
    "    (\"Esse lugar é incrível, não vejo a hora de voltar\", \"positivo\"),\n",
    "    (\"Estou triste por não ter conseguido o emprego\", \"negativo\"),\n",
    "    (\"Estou radiante com a notícia do casamento do meu amigo\", \"positivo\"),\n",
    "    (\"Não aguento mais essa situação difícil\", \"negativo\"),\n",
    "    (\"Que bom te ver depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Estou preocupado com a saúde da minha família\", \"negativo\"),\n",
    "    (\"Adorei a comida desse restaurante\", \"positivo\"),\n",
    "    (\"Estou irritado com o barulho constante\", \"negativo\"),\n",
    "    (\"Estou feliz por ter terminado meus estudos\", \"positivo\"),\n",
    "    (\"Não gostei do final desse livro\", \"negativo\"),\n",
    "    (\"Estou contente por ter conseguido resolver o problema\", \"positivo\"),\n",
    "    (\"Estou estressado com os prazos do trabalho\", \"negativo\"),\n",
    "    (\"Que alegria, vou passar o final de semana na praia\", \"positivo\"),\n",
    "    (\"Não vejo a hora de viajar para o exterior\", \"positivo\"),\n",
    "    (\"Estou triste com a notícia da doença de um amigo\", \"negativo\"),\n",
    "    (\"Adorei o filme que assisti ontem à noite\", \"positivo\"),\n",
    "    (\"Estou frustrado com a falta de oportunidades\", \"negativo\"),\n",
    "    (\"Que alívio, consegui encontrar minhas chaves perdidas\", \"positivo\"),\n",
    "    (\"Não suporto mais essa situação complicada\", \"negativo\"),\n",
    "    (\"Estou emocionado com o nascimento do meu sobrinho\", \"positivo\"),\n",
    "    (\"Estou irritado com o comportamento dessa pessoa\", \"negativo\"),\n",
    "    (\"Estou feliz por ter encontrado um novo emprego\", \"positivo\"),\n",
    "    (\"Não gostei do atendimento dessa loja\", \"negativo\"),\n",
    "    (\"Que bom te ver depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Estou chateado com o cancelamento do evento\", \"negativo\"),\n",
    "    (\"Estou empolgado para a festa de aniversário\", \"positivo\"),\n",
    "    (\"Não aguento mais essa dor de cabeça\", \"negativo\"),\n",
    "    (\"Estou ansioso para o lançamento do novo filme\", \"positivo\"),\n",
    "    (\"Estou preocupado com o meio ambiente\", \"negativo\"),\n",
    "    (\"Estou maravilhado com a beleza desse lugar\", \"positivo\"),\n",
    "    (\"Não consigo parar de pensar naquela situação\", \"negativo\"),\n",
    "    (\"Estou radiante com a notícia do casamento do meu amigo\", \"positivo\"),\n",
    "    (\"Não gostei do final desse livro\", \"negativo\"),\n",
    "    (\"Estou feliz por ter terminado meus estudos\", \"positivo\"),\n",
    "    (\"Estou irritado com o barulho constante\", \"negativo\"),\n",
    "    (\"Adorei a comida desse restaurante\", \"positivo\"),\n",
    "    (\"Estou preocupado com a saúde da minha família\", \"negativo\"),\n",
    "    (\"Que bom te ver depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Estou cansado de tantas brigas\", \"negativo\"),\n",
    "    (\"Estou encantado com a cultura desse país\", \"positivo\"),\n",
    "    (\"Não gostei do resultado do jogo\", \"negativo\"),\n",
    "    (\"Estou ansioso para o final de semana\", \"positivo\"),\n",
    "    (\"Estou empolgado para a festa de aniversário\", \"positivo\"),\n",
    "    (\"Que bom encontrar você aqui\", \"positivo\"),\n",
    "    (\"Estou decepcionado com o resultado da eleição\", \"negativo\"),\n",
    "    (\"Esse filme me deixou emocionado\", \"positivo\"),\n",
    "    (\"Estou frustrado com o atraso no projeto\", \"negativo\"),\n",
    "    (\"Não vejo a hora de conhecer meu sobrinho que está a caminho\", \"positivo\"),\n",
    "    (\"Estou feliz por ter encontrado um novo emprego\", \"positivo\"),\n",
    "    (\"Estou radiante com a notícia do meu amigo\", \"positivo\"),\n",
    "    (\"O tráfego está caótico hoje\", \"negativo\"),\n",
    "    (\"Estou feliz por ter começado a fazer exercícios\", \"positivo\"),\n",
    "    (\"Não suporto mais essa discussão\", \"negativo\"),\n",
    "    (\"O concerto de música clássica foi incrível\", \"positivo\"),\n",
    "    (\"Estou desanimado com o futuro da economia\", \"negativo\"),\n",
    "    (\"Que maravilha, vou me formar na faculdade\", \"positivo\"),\n",
    "    (\"Estou triste com a perda do meu animal de estimação\", \"negativo\"),\n",
    "    (\"O pôr do sol na praia é magnífico\", \"positivo\"),\n",
    "    (\"Estou irritado com o comportamento dessa pessoa\", \"negativo\"),\n",
    "    (\"Adorei o show que fui ontem à noite\", \"positivo\"),\n",
    "    (\"Estou feliz por ter encontrado um novo emprego\", \"positivo\"),\n",
    "    (\"Que bom te ver depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Estou cansado de tantas brigas\", \"negativo\"),\n",
    "    (\"Estou radiante com a notícia do meu amigo\", \"positivo\"),\n",
    "    (\"Não gostei do resultado do jogo\", \"negativo\"),\n",
    "    (\"Que alegria, vou passar o final de semana na praia\", \"positivo\"),\n",
    "    (\"Estou irritado com o barulho constante\", \"negativo\"),\n",
    "    (\"Estou feliz por ter terminado meus estudos\", \"positivo\"),\n",
    "    (\"Estou preocupado com a saúde da minha família\", \"negativo\"),\n",
    "    (\"Estou encantado com a cultura desse país\", \"positivo\"),\n",
    "    (\"Não consigo parar de pensar naquela situação\", \"negativo\"),\n",
    "    (\"Estou maravilhado com a beleza desse lugar\", \"positivo\"),\n",
    "    (\"Não suporto mais essa situação complicada\", \"negativo\"),\n",
    "    (\"Estou emocionado com o nascimento do meu sobrinho\", \"positivo\"),\n",
    "    (\"Estou irritado com o comportamento dessa pessoa\", \"negativo\"),\n",
    "    (\"Estou feliz por ter encontrado um novo emprego\", \"positivo\"),\n",
    "    (\"Não gostei do atendimento dessa loja\", \"negativo\"),\n",
    "    (\"Que bom te ver depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Estou chateado com o cancelamento do evento\", \"negativo\"),\n",
    "    (\"Estou empolgado para a festa de aniversário\", \"positivo\"),\n",
    "    (\"Não aguento mais essa dor de cabeça\", \"negativo\"),\n",
    "    (\"Estou ansioso para o lançamento do novo filme\", \"positivo\"),\n",
    "]\n",
    "\n",
    "# Cria o DataFrame\n",
    "data = pd.DataFrame(tweets, columns=['tweet', 'label'])\n",
    "data[\"label\"] = data[\"label\"].map({\"positivo\": 0, \"negativo\": 1})\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6c8ad40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:20.938378Z",
     "start_time": "2024-04-12T22:26:20.918744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu amo o meu cachorro</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu odeio acordar cedo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A comida deste restaurante é incrível</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estou cansado de estudar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Que dia lindo para passear no parque</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tweet  label\n",
       "0                  Eu amo o meu cachorro      0\n",
       "1                  Eu odeio acordar cedo      1\n",
       "2  A comida deste restaurante é incrível      0\n",
       "3               Estou cansado de estudar      1\n",
       "4   Que dia lindo para passear no parque      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b252f990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:21.937316Z",
     "start_time": "2024-04-12T22:26:20.941384Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>bias</th>\n",
       "      <th>pos_freq</th>\n",
       "      <th>neg_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu amo o meu cachorro</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu odeio acordar cedo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A comida deste restaurante é incrível</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estou cansado de estudar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Que dia lindo para passear no parque</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O filme que vi ontem foi excelente</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Não vejo a hora de encontrar meus amigos</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Estou muito feliz com os resultados</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Que tristeza ver essa notícia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Estou decepcionado com o serviço dessa empresa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A chuva estragou meu dia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fiquei surpreso com o presente que recebi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Esse livro é incrível, não consigo parar de ler</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Estou preocupado com o futuro do país</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Adorei o novo restaurante que experimentei</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>O trânsito está terrível hoje</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Essa música me faz sentir feliz</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Perdi o ônibus e vou me atrasar para o trabalho</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Estou ansioso para o feriado chegar</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Que vergonha, esqueci meu aniversário de casam...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  label  bias  pos_freq  \\\n",
       "0                               Eu amo o meu cachorro      0     1        37   \n",
       "1                               Eu odeio acordar cedo      1     1         0   \n",
       "2               A comida deste restaurante é incrível      0     1        51   \n",
       "3                            Estou cansado de estudar      1     1        30   \n",
       "4                Que dia lindo para passear no parque      0     1        31   \n",
       "5                  O filme que vi ontem foi excelente      0     1        53   \n",
       "6            Não vejo a hora de encontrar meus amigos      0     1        82   \n",
       "7                 Estou muito feliz com os resultados      0     1        33   \n",
       "8                       Que tristeza ver essa notícia      1     1        24   \n",
       "9      Estou decepcionado com o serviço dessa empresa      1     1        40   \n",
       "10                           A chuva estragou meu dia      1     1        43   \n",
       "11          Fiquei surpreso com o presente que recebi      0     1        55   \n",
       "12    Esse livro é incrível, não consigo parar de ler      0     1        64   \n",
       "13              Estou preocupado com o futuro do país      1     1        58   \n",
       "14         Adorei o novo restaurante que experimentei      0     1        48   \n",
       "15                      O trânsito está terrível hoje      1     1        25   \n",
       "16                    Essa música me faz sentir feliz      0     1        24   \n",
       "17    Perdi o ônibus e vou me atrasar para o trabalho      1     1        68   \n",
       "18                Estou ansioso para o feriado chegar      0     1        43   \n",
       "19  Que vergonha, esqueci meu aniversário de casam...      1     1        78   \n",
       "\n",
       "    neg_freq  \n",
       "0         33  \n",
       "1          3  \n",
       "2         16  \n",
       "3         31  \n",
       "4          9  \n",
       "5         28  \n",
       "6         39  \n",
       "7         38  \n",
       "8         16  \n",
       "9         75  \n",
       "10        22  \n",
       "11        60  \n",
       "12        39  \n",
       "13        95  \n",
       "14        28  \n",
       "15        36  \n",
       "16        12  \n",
       "17        64  \n",
       "18        32  \n",
       "19        36  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Exemplo de dataframe de tweets\n",
    "# data = {'tweet': ['Este é um tweet não tóxico', \n",
    "#                   'Este é um tweet tóxico com palavras negativas',\n",
    "#                   'Outro tweet não tóxico com palavras positivas'],\n",
    "#         'label': [0, 1, 0]}\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "positive_words = df[df[\"label\"] == 0][\"tweet\"].to_list() # Lista de palavras positivas\n",
    "positive_words = word_tokenize(' '.join(positive_words))\n",
    "\n",
    "negative_words = df[df[\"label\"] == 1][\"tweet\"].to_list()# Lista de palavras negativas\n",
    "negative_words = word_tokenize(' '.join(negative_words))\n",
    "\n",
    "# Função para mapear a frequência das palavras do subconjunto de palavras positivas (label = 0)\n",
    "def positive_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())  # Tokenização e conversão para minúsculas\n",
    "    freq = FreqDist(tokens)\n",
    "    positive_freq = sum([freq[word] for word in positive_words])\n",
    "    return positive_freq\n",
    "\n",
    "# Função para mapear a frequência das palavras do subconjunto de palavras negativas (label = 1)\n",
    "def negative_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())  # Tokenização e conversão para minúsculas\n",
    "    freq = FreqDist(tokens)\n",
    "    negative_freq = sum([freq[word] for word in negative_words])\n",
    "    return negative_freq\n",
    "\n",
    "# Aplicar as funções aos tweets e adicionar os resultados ao dataframe\n",
    "df['bias'] = 1  # Coluna de bias com valor 1\n",
    "df['pos_freq'] = df['tweet'].apply(positive_words_frequency)\n",
    "df['neg_freq'] = df['tweet'].apply(negative_words_frequency)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a7569",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32423fa1",
   "metadata": {},
   "source": [
    "O pré-processamento geralmente envolve as etapas de limpeza e preparação dos dados. O texto é normalmente convertido em minúsculas, removidos caracteres especiais, removidos números e pontuações, e realizada a tokenização do texto para obter as palavras individuais.\n",
    "\n",
    "Exemplo em python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f80d6a69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:22.402939Z",
     "start_time": "2024-04-12T22:26:21.939652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exemplo', 'texto', 'limpo', 'tokenizado']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/joaoag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def text_cleaning(text):\n",
    "    # Extrai as stopwords em português\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"Este é um exemplo de texto que será limpo e tokenizado!\"\n",
    "tokens = text_cleaning(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05998cc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:22.560718Z",
     "start_time": "2024-04-12T22:26:22.406748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>bias</th>\n",
       "      <th>pos_freq</th>\n",
       "      <th>neg_freq</th>\n",
       "      <th>ttweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu amo o meu cachorro</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>amo cachorro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu odeio acordar cedo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>odeio acordar cedo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A comida deste restaurante é incrível</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>comida deste restaurante incrível</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estou cansado de estudar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>cansado estudar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Que dia lindo para passear no parque</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>dia lindo passear parque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O filme que vi ontem foi excelente</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>filme vi ontem excelente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Não vejo a hora de encontrar meus amigos</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>vejo hora encontrar amigos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Estou muito feliz com os resultados</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>feliz resultados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Que tristeza ver essa notícia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>tristeza ver notícia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Estou decepcionado com o serviço dessa empresa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>decepcionado serviço dessa empresa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A chuva estragou meu dia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>chuva estragou dia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fiquei surpreso com o presente que recebi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>fiquei surpreso presente recebi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Esse livro é incrível, não consigo parar de ler</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>livro incrível consigo parar ler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Estou preocupado com o futuro do país</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>preocupado futuro país</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Adorei o novo restaurante que experimentei</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>adorei novo restaurante experimentei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>O trânsito está terrível hoje</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>trânsito terrível hoje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Essa música me faz sentir feliz</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>música faz sentir feliz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Perdi o ônibus e vou me atrasar para o trabalho</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>perdi ônibus vou atrasar trabalho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Estou ansioso para o feriado chegar</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ansioso feriado chegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Que vergonha, esqueci meu aniversário de casam...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>vergonha esqueci aniversário casamento</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  label  bias  pos_freq  \\\n",
       "0                               Eu amo o meu cachorro      0     1         2   \n",
       "1                               Eu odeio acordar cedo      1     1         0   \n",
       "2               A comida deste restaurante é incrível      0     1        15   \n",
       "3                            Estou cansado de estudar      1     1         0   \n",
       "4                Que dia lindo para passear no parque      0     1         4   \n",
       "5                  O filme que vi ontem foi excelente      0     1        13   \n",
       "6            Não vejo a hora de encontrar meus amigos      0     1        18   \n",
       "7                 Estou muito feliz com os resultados      0     1        13   \n",
       "8                       Que tristeza ver essa notícia      1     1        11   \n",
       "9      Estou decepcionado com o serviço dessa empresa      1     1         0   \n",
       "10                           A chuva estragou meu dia      1     1         1   \n",
       "11          Fiquei surpreso com o presente que recebi      0     1         6   \n",
       "12    Esse livro é incrível, não consigo parar de ler      0     1        11   \n",
       "13              Estou preocupado com o futuro do país      1     1         3   \n",
       "14         Adorei o novo restaurante que experimentei      0     1        23   \n",
       "15                      O trânsito está terrível hoje      1     1         1   \n",
       "16                    Essa música me faz sentir feliz      0     1        18   \n",
       "17    Perdi o ônibus e vou me atrasar para o trabalho      1     1         7   \n",
       "18                Estou ansioso para o feriado chegar      0     1         7   \n",
       "19  Que vergonha, esqueci meu aniversário de casam...      1     1         9   \n",
       "\n",
       "    neg_freq                                  ttweet  \n",
       "0          1                            amo cachorro  \n",
       "1          3                      odeio acordar cedo  \n",
       "2          1       comida deste restaurante incrível  \n",
       "3          8                         cansado estudar  \n",
       "4          1                dia lindo passear parque  \n",
       "5          1                filme vi ontem excelente  \n",
       "6          2              vejo hora encontrar amigos  \n",
       "7          0                        feliz resultados  \n",
       "8          4                    tristeza ver notícia  \n",
       "9         13      decepcionado serviço dessa empresa  \n",
       "10         3                      chuva estragou dia  \n",
       "11         0         fiquei surpreso presente recebi  \n",
       "12         8        livro incrível consigo parar ler  \n",
       "13        13                  preocupado futuro país  \n",
       "14         1    adorei novo restaurante experimentei  \n",
       "15         6                  trânsito terrível hoje  \n",
       "16         0                 música faz sentir feliz  \n",
       "17         8       perdi ônibus vou atrasar trabalho  \n",
       "18         0                  ansioso feriado chegar  \n",
       "19         4  vergonha esqueci aniversário casamento  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessando o texto\n",
    "df[\"ttweet\"] = df[\"tweet\"].apply(text_cleaning)\n",
    "df[\"ttweet\"] = df[\"ttweet\"].apply(lambda x: \" \".join(x)) # retornando a lista como string\n",
    "\n",
    "# novo conjunto de palavras\n",
    "positive_words = df[df[\"label\"] == 0][\"ttweet\"].to_list() # Lista de palavras positivas\n",
    "positive_words = word_tokenize(' '.join(positive_words))\n",
    "\n",
    "negative_words = df[df[\"label\"] == 1][\"ttweet\"].to_list()# Lista de palavras negativas\n",
    "negative_words = word_tokenize(' '.join(negative_words))\n",
    "\n",
    "# Função para mapear a frequência das palavras do subconjunto de palavras positivas (label = 0)\n",
    "def positive_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())  # Tokenização e conversão para minúsculas\n",
    "    freq = FreqDist(tokens)\n",
    "    positive_freq = sum([freq[word] for word in positive_words])\n",
    "    return positive_freq\n",
    "\n",
    "# Função para mapear a frequência das palavras do subconjunto de palavras negativas (label = 1)\n",
    "def negative_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())  # Tokenização e conversão para minúsculas\n",
    "    freq = FreqDist(tokens)\n",
    "    negative_freq = sum([freq[word] for word in negative_words])\n",
    "    return negative_freq\n",
    "\n",
    "# Aplicar as funções aos tweets e adicionar os resultados ao dataframe\n",
    "df['bias'] = 1  # Coluna de bias com valor 1\n",
    "df['pos_freq'] = df['ttweet'].apply(positive_words_frequency)\n",
    "df['neg_freq'] = df['ttweet'].apply(negative_words_frequency)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f1c4e5",
   "metadata": {},
   "source": [
    "Mas além disso também podemos fazer **Stemming** e **lematização**. Elas são técnicas de pré-processamento de texto com o objetivo de reduzir as palavras em sua forma base ou raiz, simplificando o processo de análise de texto.\n",
    "\n",
    "**Stemming** é um processo mais simples e rápido de normalização de palavras, que envolve a remoção de sufixos com o objetivo de transformar uma palavra em sua raiz, ou seja, em sua forma básica. Um exemplo de algoritmo de stemming é o Porter Stemmer, que é amplamente utilizado em NLP. O ponto positivo do stemming é sua simplicidade e velocidade de processamento, o que pode ser útil em projetos com grandes volumes de dados. No entanto, o stemming pode produzir algumas palavras raiz que não são facilmente reconhecidas, o que pode ser um problema em alguns casos.\n",
    "\n",
    "Já a **lematização** é um processo mais complexo de normalização de palavras, que envolve a análise do contexto da palavra para determinar sua forma básica. A lematização utiliza um dicionário de palavras ou um algoritmo para mapear a palavra para sua forma base. O ponto positivo da lematização é que ela produz palavras que são facilmente reconhecíveis, o que pode ser importante em projetos que exigem maior precisão na análise de texto. No entanto, a lematização é um processo mais lento e computacionalmente mais caro do que o stemming, o que pode ser um problema em projetos com grandes volumes de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4127c2cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:22.570470Z",
     "start_time": "2024-04-12T22:26:22.562524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exempl', 'text', 'limp', 'tokeniz', 'stemmed']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def text_cleaning_s(text):\n",
    "    # Extrai as stopwords em português\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza o stemming dos tokens\n",
    "    stemmer = SnowballStemmer('portuguese')\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"Este é um exemplo de texto que será limpo, tokenizado e stemmed!\"\n",
    "tokens = text_cleaning_s(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dc9a543",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:24.472607Z",
     "start_time": "2024-04-12T22:26:22.572091Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/joaoag/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/joaoag/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['este', 'é', 'um', 'exemplo', 'de', 'texto', 'que', 'será', 'limpo', 'tokenizado', 'e', 'stemmed']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def text_cleaning_l(text):\n",
    "    # Extrai as stopwords em inglês\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza a lematização dos tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "# text = \"This is an example of text that will be cleaned, tokenized, and lemmatized!\"\n",
    "text = \"Este é um exemplo de texto que será limpo, tokenizado e stemmed!\"\n",
    "tokens = text_cleaning_l(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376fbd2",
   "metadata": {},
   "source": [
    "## Logistic Regression Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f8457b",
   "metadata": {},
   "source": [
    "A **regressão logística** é frequentemente utilizada em análise de sentimento porque é um algoritmo de classificação simples, rápido e eficiente para problemas de classificação binária, ou seja, quando há apenas duas classes, como positivo e negativo. Além disso, a regressão logística é facilmente interpretável, o que significa que é possível entender como o modelo chegou a uma determinada previsão.\n",
    "\n",
    "Outra vantagem da regressão logística é que ela lida bem com problemas de dados desbalanceados, que é comum em análise de sentimento, onde muitas vezes há mais exemplos de uma classe do que outra. A regressão logística usa uma função sigmoide para calcular as probabilidades de cada classe, e essa função é bem adequada para casos de classes desbalanceadas.\n",
    "\n",
    "Outro motivo para a popularidade da regressão logística em análise de sentimento é que ela é relativamente fácil de implementar e ajustar. É possível utilizar diferentes técnicas de regularização, como a regularização L1 ou L2, para evitar overfitting e melhorar o desempenho do modelo.\n",
    "\n",
    "No entanto, é importante ressaltar que a regressão logística pode não ser adequada para problemas de classificação multiclasse, ou seja, quando há mais de duas classes, como em análise de tópicos. Nesses casos, outros algoritmos, como Árvores de Decisão, SVMs ou Redes Neurais, podem ser mais apropriados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a435ec",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/logistic_regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df78e7ba",
   "metadata": {},
   "source": [
    "O modelo é treinado usando um conjunto de dados rotulados, onde cada texto é rotulado como tendo um sentimento positivo ou negativo. O objetivo do treinamento é ajustar os parâmetros do modelo para que ele possa fazer previsões precisas em novos dados.\n",
    "\n",
    "Durante o treinamento, o modelo utiliza a função logística para calcular a probabilidade de um texto ter um sentimento positivo ou negativo com base em seus vetores de características. A função logística produz um valor entre 0 e 1, que representa a probabilidade de um texto ter um sentimento positivo. Se a probabilidade for maior que 0,5, o modelo classifica o texto como tendo um sentimento positivo. Caso contrário, o texto é classificado como tendo um sentimento negativo.\n",
    "\n",
    "Após o treinamento, o modelo pode ser usado para fazer previsões em novos dados. Para cada novo texto, o modelo converte-o em um vetor de características e utiliza a função logística para calcular a probabilidade de ter um sentimento positivo ou negativo. Em seguida, classifica o texto como tendo um sentimento positivo ou negativo com base no valor calculado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd0c98",
   "metadata": {},
   "source": [
    "Note que conforme $\\theta^T x^{(i)}$ se aproxima cada vez mais de $-\\infty$, o denominador da função sigmoidal fica cada vez maior e, como resultado, a sigmoidal se aproxima de $0$. Por outro lado, conforme $\\theta^T x^{(i)}$ se aproxima cada vez mais de $\\infty$, o denominador da função sigmoidal se aproxima de 1 e, como resultado, a sigmoidal também se aproxima de $1$.\n",
    "\n",
    "Dado um tweet, podemos transformá-lo em um vetor e passá-lo pela sua função sigmoidal para obter uma previsão da seguinte forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3dcf92",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/logistic_regression2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77604529",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:24.479486Z",
     "start_time": "2024-04-12T22:26:24.474421Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_cleaning_l(text):\n",
    "    # Extrai as stopwords\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza a lematização dos tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85f45ee9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:24.485776Z",
     "start_time": "2024-04-12T22:26:24.481178Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Criação das frases aleatórias\n",
    "# np.random.seed(42)\n",
    "# frases = np.random.choice(['Esta é uma frase positiva.',\n",
    "#                            'Esta é uma frase negativa.'], size=100000)\n",
    "\n",
    "# # Criação das labels aleatórias\n",
    "# labels = np.random.choice(['positivo', 'negativo'], size=100000)\n",
    "\n",
    "# # Criação do DataFrame\n",
    "# df = pd.DataFrame({'frase': frases, 'label': labels})\n",
    "\n",
    "# # Visualização das primeiras linhas\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d04912",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:24.495058Z",
     "start_time": "2024-04-12T22:26:24.490846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "430270ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:26:25.524354Z",
     "start_time": "2024-04-12T22:26:24.498000Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby('label').agg({'tweet': 'count'}).reset_index(drop=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e675b0ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:27:58.517067Z",
     "start_time": "2024-04-12T22:27:58.484721Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classificação: 0.9807692307692307\n",
      "Precisão da classificação (Classe 0): 1.0\n",
      "Precisão da classificação (Classe 1): 0.9655172413793104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaoag/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/joaoag/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divisão em treino e teste\n",
    "X = df[['pos_freq', 'neg_freq']]\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# criação do modelo de regressão logística e treinamento\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# avaliação do modelo nos dados de teste\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# avaliando o modelo\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "print(\"Acurácia da classificação:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precisão da classificação (Classe 0):\", precision_score(y_test, y_pred, pos_label=0))\n",
    "print(\"Precisão da classificação (Classe 1):\", precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f210319d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:28:18.549463Z",
     "start_time": "2024-04-12T22:28:18.410490Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classificação: 0.7692307692307693\n",
      "Precisão da classificação (Classe 0): 0.7142857142857143\n",
      "Precisão da classificação (Classe 1): 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pré-processamento das frases\n",
    "# df['frase_processada'] = df['frase'].apply(text_cleaning_l)\n",
    "df['ttweet'] = df['tweet'].apply(text_cleaning_l)\n",
    "\n",
    "# vetorização das frases\n",
    "vectorizer = CountVectorizer(tokenizer=lambda text: text, lowercase=False)\n",
    "X = vectorizer.fit_transform(df['ttweet'].apply(lambda tokens: ' '.join(tokens)))\n",
    "\n",
    "# divisão em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# criação do modelo de regressão logística e treinamento\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# avaliação do modelo nos dados de teste\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# avaliando o modelo\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "print(\"Acurácia da classificação:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precisão da classificação (Classe 0):\", precision_score(y_test, y_pred, pos_label=0))\n",
    "print(\"Precisão da classificação (Classe 1):\", precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "927bedd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:29:03.246404Z",
     "start_time": "2024-04-12T22:29:03.205577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classificação: 0.7115384615384616\n",
      "Precisão da classificação (Classe 0): 0.6216216216216216\n",
      "Precisão da classificação (Classe 1): 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Mesmo exemplo mas com o TF-IDF como vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pré-processamento das frases\n",
    "# df['frase_processada'] = df['frase'].apply(text_cleaning_l)\n",
    "\n",
    "# vetorização das frases\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda text: text, lowercase=False)\n",
    "X = vectorizer.fit_transform(df['ttweet'].apply(lambda tokens: ' '.join(tokens)))\n",
    "\n",
    "# divisão em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# criação do modelo de regressão logística e treinamento\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# avaliação do modelo nos dados de teste\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# avaliando o modelo\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "print(\"Acurácia da classificação:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precisão da classificação (Classe 0):\", precision_score(y_test, y_pred, pos_label=0))\n",
    "print(\"Precisão da classificação (Classe 1):\", precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f7826",
   "metadata": {},
   "source": [
    "## Logistic Regression Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187dc9d0",
   "metadata": {},
   "source": [
    "A função de custo da regressão logística é usada para avaliar quão bem um modelo de regressão logística está performando em relação aos dados observados. Ela é frequentemente utilizada em problemas de classificação binária, onde o objetivo é prever se uma observação pertence a uma das duas classes possíveis. Aqui está uma explicação de cada etapa da função de custo da regressão logística:\n",
    "\n",
    "1. **Hipótese da Regressão Logística:**\n",
    "   A primeira etapa é a formulação da hipótese do modelo de regressão logística. A hipótese é uma função que mapeia as entradas para uma probabilidade estimada de pertencer a uma das classes. A função logística (também conhecida como função sigmoide) é comumente utilizada como a função de hipótese na regressão logística e é dada pela seguinte equação:\n",
    "\n",
    "   $$ h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^Tx}}$$\n",
    "\n",
    "   Onde:\n",
    "   - $ h_{\\theta}(x) $ é a estimativa da probabilidade de \\( x \\) pertencer à classe positiva,\n",
    "   - $ \\theta $ são os parâmetros do modelo,\n",
    "   - $ x $ é o vetor de entrada,\n",
    "   - $ e $ é o número de Euler (aproximadamente 2.71828).\n",
    "\n",
    "\n",
    "2. **Função de Custo Logístico:**\n",
    "   A função de custo é uma medida de quão bem a hipótese do modelo se ajusta aos dados observados. Para a regressão logística, a função de custo (também conhecida como função de perda ou função de erro) é definida usando a técnica de máxima verossimilhança. A função de custo logístico para um único exemplo de treinamento é dada pela seguinte equação:\n",
    "\n",
    "   $$ J(\\theta) = -y \\log(h_{\\theta}(x)) - (1 - y) \\log(1 - h_{\\theta}(x)) $$\n",
    "\n",
    "   Onde:\n",
    "   - $ J(\\theta) $ é a função de custo,\n",
    "   - $ y $ é a classe verdadeira do exemplo (0 ou 1),\n",
    "   - $ h_{\\theta}(x) $ é a estimativa da probabilidade da classe positiva dada pela hipótese.\n",
    "\n",
    "\n",
    "3. **Função de Custo Médio (ou Função de Custo Regularizada):**\n",
    "   Para avaliar o desempenho do modelo em todo o conjunto de dados, a função de custo médio é calculada. Isso é feito tirando a média dos custos individuais de todos os exemplos de treinamento. Além disso, uma penalidade de regularização pode ser adicionada para evitar overfitting. A função de custo médio (ou função de custo regularizada) é dada pela seguinte equação:\n",
    "\n",
    "   $$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}[-y^{(i)} \\log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2 $$\n",
    "\n",
    "   Onde:\n",
    "   - $ m $ é o número total de exemplos de treinamento,\n",
    "   - $ n $ é o número de características,\n",
    "   - $ \\lambda $ é o parâmetro de regularização,\n",
    "   - $ \\theta_j $ é o j-ésimo parâmetro do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8559a5cf",
   "metadata": {},
   "source": [
    "# Week 2 - Sentiment Analysis with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364cd8f9",
   "metadata": {},
   "source": [
    "## Objetivos de aprendizagem\n",
    "- Error analysis\n",
    "- Naive Bayes inference\n",
    "- Log likelihood\n",
    "- Laplacian smoothing\n",
    "- conditional probabilities\n",
    "- Bayes rule\n",
    "- Sentiment analysis\n",
    "- Vocabulary creation\n",
    "- Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2821ee5",
   "metadata": {},
   "source": [
    "## Recaptulando...\n",
    "\n",
    "1. **Probabilidade:**\n",
    "   A probabilidade é uma medida numérica que quantifica a incerteza associada a um evento. Em termos simples, é a chance de que algo aconteça. A probabilidade de um evento é sempre um número entre 0 e 1, onde 0 indica impossibilidade absoluta do evento ocorrer e 1 indica certeza absoluta de que o evento ocorrerá.\n",
    "   \n",
    "<img src=\"./imgs/prob_positive_tweet.png\">\n",
    "<img src=\"./imgs/prob_positive_tweet2.png\">\n",
    "\n",
    "2. **Probabilidade Condicional:**\n",
    "   A probabilidade condicional é a probabilidade de um evento ocorrer dado que outro evento já ocorreu. É denotada por $ P(A|B) $, que lê-se como \"a probabilidade de A dado B\". A fórmula para calcular a probabilidade condicional é:\n",
    "\n",
    "   $$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$\n",
    "\n",
    "   Onde:\n",
    "   - $ P(A|B) $ é a probabilidade de A dado B,\n",
    "   - $ P(A \\cap B) $ é a probabilidade da interseção de A e B,\n",
    "   - $ P(B) $ é a probabilidade de B.\n",
    "\n",
    "   Em palavras simples, a probabilidade condicional é a proporção de vezes que o evento A ocorre quando o evento B ocorre.\n",
    "\n",
    "<img src=\"./imgs/conditional_prob_tweet.png\">\n",
    "\n",
    "\n",
    "3. **Regra de Bayes:**\n",
    "   A regra de Bayes é uma ferramenta fundamental na teoria das probabilidades que permite atualizar as probabilidades de uma hipótese à luz de novas evidências. Formalmente, a regra de Bayes é expressa como:\n",
    "\n",
    "   $ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $\n",
    "\n",
    "   Onde:\n",
    "   - $ P(A|B) $ é a probabilidade de A dado B (posterior),\n",
    "   - $ P(B|A) $ é a probabilidade de B dado A (likelihood),\n",
    "   - $ P(A) $ é a probabilidade de A (prior),\n",
    "   - $ P(B) $ é a probabilidade de B.\n",
    "\n",
    "   A regra de Bayes nos permite calcular a probabilidade de uma hipótese (A) ser verdadeira dada uma evidência observada (B), usando a probabilidade da evidência dada a hipótese (likelihood), a probabilidade a priori da hipótese e a probabilidade marginal da evidência.A diferença principal entre a probabilidade condicional e a regra de Bayes é que a probabilidade condicional é uma medida da probabilidade de um evento ocorrer dado que outro evento já ocorreu, enquanto a regra de Bayes é uma ferramenta para atualizar a probabilidade de uma hipótese à luz de novas evidências. A regra de Bayes utiliza a probabilidade condicional como um de seus componentes para calcular a probabilidade posterior.\n",
    "\n",
    "<img src=\"./imgs/bayes_rule_tweet.png\">\n",
    "\n",
    "A probabilidade é a chance de um evento ocorrer, a probabilidade condicional é a probabilidade de um evento ocorrer dado que outro evento já ocorreu, e a regra de Bayes é uma ferramenta para atualizar a probabilidade de uma hipótese à luz de novas evidências."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bb3ffc",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "O algoritmo Naive Bayes é um método de classificação probabilístico baseado no teorema de Bayes, com uma suposição \"ingênua\" de independência condicional entre os recursos.\n",
    "\n",
    "1. **Suposição de Independência Condicional:**\n",
    "   A primeira etapa do algoritmo Naive Bayes é a suposição de independência condicional entre os recursos. Isso significa que assumimos que os recursos são independentes entre si, dado o valor da variável de classe. Apesar de ser uma suposição forte e muitas vezes não ser verdadeira na prática, ela simplifica os cálculos e torna o algoritmo computacionalmente eficiente.\n",
    "\n",
    "2. **Construção do Modelo de Probabilidade:**\n",
    "   O próximo passo é construir o modelo de probabilidade. Isso envolve calcular a probabilidade de cada classe e a probabilidade de cada valor do recurso dado cada classe. Em outras palavras, para cada classe, calculamos a probabilidade a priori da classe ( $ P(C_k) $ ) e a probabilidade de cada recurso ( $ P(X_i | C_k) $ ).\n",
    "\n",
    "3. **Classificação:**\n",
    "   Depois que o modelo de probabilidade é construído, podemos usá-lo para fazer previsões sobre novos exemplos. Dada uma nova instância com valores de recursos $ x_1, x_2, ..., x_n $, queremos calcular a probabilidade de pertencer a cada classe e, em seguida, atribuir a classe com a maior probabilidade como a classe prevista para a instância. Isso é feito usando o teorema de Bayes:\n",
    "\n",
    "   $$ P(C_k | x_1, x_2, ..., x_n) = \\frac{P(C_k) \\cdot \\prod_{i=1}^{n} P(x_i | C_k)}{P(x_1, x_2, ..., x_n)} $$\n",
    "\n",
    "   Onde:\n",
    "   - $ P(C_k | x_1, x_2, ..., x_n) $ é a probabilidade da classe $ C_k $ dado os valores dos recursos,\n",
    "   - $ P(C_k) $ é a probabilidade a priori da classe $ C_k $,\n",
    "   - $ P(x_i | C_k) $ é a probabilidade de cada valor do recurso dado a classe $ C_k $,\n",
    "   - $ P(x_1, x_2, ..., x_n) $ é a probabilidade dos valores dos recursos.\n",
    "\n",
    "4. **Estimação de Parâmetros:**\n",
    "   Durante a etapa de construção do modelo, precisamos estimar os parâmetros do modelo, ou seja, as probabilidades a priori das classes e as probabilidades condicionais dos recursos para cada classe. Isso geralmente é feito usando técnicas como a frequência relativa de ocorrência dos dados de treinamento.\n",
    "\n",
    "5. **Suavização de Laplace (Opcional):**\n",
    "   Em alguns casos, para evitar probabilidades condicionais iguais a zero para recursos não observados em uma classe particular, pode ser aplicada a suavização de Laplace, adicionando uma pequena quantidade aos contadores de frequência de cada valor de recurso para cada classe durante a estimativa dos parâmetros.\n",
    "\n",
    "Essas são as etapas principais da função do algoritmo Naive Bayes, desde a suposição de independência condicional até a classificação de novas instâncias usando o teorema de Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c085640",
   "metadata": {},
   "source": [
    "## Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b05e80",
   "metadata": {},
   "source": [
    "Para construir um classificador, começaremos primeiro criando probabilidades condicionais, dada a tabela a seguir:\n",
    "\n",
    "<img src=\"./imgs/naive_bayes_intro1.png\">\n",
    "          \n",
    "Isso nos permite calcular a seguinte tabela de probabilidades:\n",
    "<img src=\"./imgs/naive_bayes_intro2.png\">\n",
    "\n",
    "Depois de ter as probabilidades, podemos calcular a pontuação de probabilidade da seguinte forma\n",
    "<img src=\"./imgs/naive_bayes_intro3.png\">\n",
    "\n",
    "Uma pontuação maior que 1 indica que a classe é positiva, caso contrário é negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a1796",
   "metadata": {},
   "source": [
    "Costumamos calcular a probabilidade de uma palavra dada uma classe da seguinte forma:\n",
    "\n",
    "$$\n",
    "P(w_i \\mid \\text{classe}) = \\frac{\\text{freq}(w_i, \\text{classe})}{N_{\\text{classe}}}\n",
    "\\quad \\text{classe} \\in \\{\\text{Positivo}, \\text{Negativo}\\}\n",
    "$$\n",
    "\n",
    "No entanto, se uma palavra não aparecer no treinamento, ela automaticamente recebe uma probabilidade de 0. Para corrigir isso, adicionamos **suavização de laplace** da seguinte forma:\n",
    "\n",
    "$$\n",
    "P(w_i \\mid \\text{classe}) = \\frac{\\text{freq}(w_i, \\text{classe}) + 1}{N_{\\text{classe}} + V}\n",
    "$$\n",
    "\n",
    "Observe que adicionamos um 1 no numerador e, como há V palavras para normalizar, adicionamos V no denominador.\n",
    "\n",
    "Onde:\n",
    "- $N_{\\text{classe}}$: frequência de todas as palavras na classe.\n",
    "- $V$: número de palavras únicas no vocabulário.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3b478",
   "metadata": {},
   "source": [
    "Para calcular a log-verossimilhança (log likelihood), precisamos obter as razões e usá-las para calcular uma pontuação que nos permitirá decidir se um tweet é positivo ou negativo. Quanto maior a razão, mais positiva é a palavra:\n",
    "\n",
    "<img src=\"./imgs/log_likelihood1.png\">\n",
    "\n",
    "Para fazer inferência, você pode calcular o seguinte:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\prod_{i=1}^{m} \\frac{P(w_i \\mid \\text{neg})}{P(w_i \\mid \\text{pos})} \\right) > \\frac{1}{P(\\text{neg}) P(\\text{pos})}\n",
    "$$\n",
    "\n",
    "A expressão acima começa com um logaritmo, $\\log$, que é aplicado a um produto de probabilidades condicionais. O produto é denotado pelo símbolo $\\prod$ e representa o produto de todas as probabilidades condicionais das palavras $w_i$ dadas as classes \"negativo\" e \"positivo\". Isso significa que estamos multiplicando a probabilidade de cada palavra $w_i$ ocorrer, dado que o tweet é negativo, e dividindo pelo mesmo para tweets positivos.\n",
    "\n",
    "A expressão é comparada com o inverso do produto das probabilidades das classes \"negativo\" e \"positivo\". Isso é feito usando o sinal $>$, indicando que queremos que a expressão à esquerda seja maior do que o inverso do produto das probabilidades das classes.\n",
    "\n",
    "Em termos de interpretação, isso significa que estamos comparando a probabilidade conjunta de todas as palavras em um tweet serem negativas (ou positivas) com a probabilidade de um tweet ser classificado como negativo (ou positivo), independente do conteúdo do tweet. Se a probabilidade conjunta de todas as palavras serem negativas (ou positivas) for maior do que a probabilidade do tweet ser classificado como negativo (ou positivo) independentemente do conteúdo, então a inferência seria que o tweet é mais provavelmente negativo (ou positivo).\n",
    "\n",
    "Essa expressão é uma maneira de inferir a polaridade (positiva ou negativa) de um tweet com base na probabilidade condicional de cada palavra em relação às classes \"negativo\" e \"positivo\", em comparação com a probabilidade marginal das classes. Se a probabilidade conjunta das palavras sendo negativas (ou positivas) for maior do que a probabilidade do tweet ser classificado como negativo (ou positivo) independentemente do conteúdo, então a inferência seria que o tweet é mais provavelmente negativo (ou positivo).\n",
    "\n",
    "Conforme o número de palavras do tweet (m) aumenta, podemos ter problemas numéricos, então introduzimos o logaritmo, que nos dá a seguinte equação:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\prod_{i=1}^{m} \\frac{P(w_i \\mid \\text{neg})}{P(w_i \\mid \\text{pos})} \\right) \\Rightarrow \\log \\prod_{i=1}^{m} \\frac{P(w_i \\mid \\text{neg})}{P(w_i \\mid \\text{pos})} + \\sum_{i=1}^{m} \\log \\frac{P(w_i \\mid \\text{neg})}{P(w_i \\mid \\text{pos})}\n",
    "$$\n",
    "\n",
    "Utilizando a propriedade do logaritmo de um produto, podemos reescrever a expressão como a soma dos logaritmos dos fatores dentro do produto. Esta transformação nos permite calcular a log-verossimilhança de forma mais eficiente e robusta, evitando o custo computacional de calcular o logaritmo de um produtório. A expressão agora é uma soma de logaritmos individuais, o que é mais estável numericamente. A nova expressão nos permite calcular a log-verossimilhança somando os logaritmos das razões das probabilidades condicionais de cada palavra em relação às classes \"negativo\" e \"positivo\". Isso nos dá uma medida da probabilidade de observar as palavras em um tweet dado que ele é classificado como negativo, em comparação com a mesma probabilidade para tweets positivos. Essa transformação simplifica o cálculo da log-verossimilhança e reduz a chance de problemas numéricos, tornando a inferência mais eficiente e precisa.\n",
    "\n",
    "O primeiro componente é chamado de log prior e o segundo componente é a log-verossimilhança. Introduzimos ainda $\\lambda$ como segue:\n",
    "\n",
    "<img src=\"./imgs/log_likelihood2.png\">\n",
    "\n",
    "Ter o dicionário $\\lambda$ ajudará muito ao fazer inferência, mas uma vez que computemos o dicionário $\\lambda$, se torna simples fazer a inferência, simplesmente somando os lambdas e comparando com os thresholds de negativo, neutro e positivo.\n",
    "\n",
    "<img src=\"./imgs/log_likelihood3.png\">\n",
    "\n",
    "O resultado foi **3.3**, sendo > 0, classificaremos o documento como positivo. Se tivessemos um número negativo, classificaríamos como a classe negativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368205e5",
   "metadata": {},
   "source": [
    "## Training Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "629580bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:29:36.721433Z",
     "start_time": "2024-04-12T22:29:36.558733Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.9709302325581395\n",
      "Precisão da classificação (Classe 0): 0.6216216216216216\n",
      "Precisão da classificação (Classe 1): 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaoag/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/joaoag/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Pré-processamento\n",
    "df[\"ttweet\"] = df[\"tweet\"].apply(text_cleaning)\n",
    "df[\"ttweet\"] = df[\"ttweet\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Listas de palavras positivas e negativas\n",
    "positive_words = df[df[\"label\"] == 0][\"ttweet\"].to_list()\n",
    "positive_words = word_tokenize(' '.join(positive_words))\n",
    "\n",
    "negative_words = df[df[\"label\"] == 1][\"ttweet\"].to_list()\n",
    "negative_words = word_tokenize(' '.join(negative_words))\n",
    "\n",
    "# Funções para mapear frequências de palavras\n",
    "def positive_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())\n",
    "    freq = FreqDist(tokens)\n",
    "    positive_freq = sum([freq[word] for word in positive_words])\n",
    "    return positive_freq\n",
    "\n",
    "def negative_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())\n",
    "    freq = FreqDist(tokens)\n",
    "    negative_freq = sum([freq[word] for word in negative_words])\n",
    "    return negative_freq\n",
    "\n",
    "# Aplicando as funções e adicionando colunas ao dataframe\n",
    "df['bias'] = 1\n",
    "df['pos_freq'] = df['ttweet'].apply(positive_words_frequency)\n",
    "df['neg_freq'] = df['ttweet'].apply(negative_words_frequency)\n",
    "\n",
    "# Modelagem com Naive Bayes ou Regressão Logística\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Criando e treinando o modelo\n",
    "model = MultinomialNB() # Exemplo: usando Naive Bayes\n",
    "model.fit(df[['bias', 'pos_freq', 'neg_freq']], df['label'])\n",
    "\n",
    "# Fazendo previsões\n",
    "predictions = model.predict(df[['bias', 'pos_freq', 'neg_freq']])\n",
    "\n",
    "# Avaliando o desempenho do modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(df['label'], predictions)\n",
    "print(f\"Acurácia do modelo: {accuracy}\")\n",
    "print(\"Precisão da classificação (Classe 0):\", precision_score(y_test, y_pred, pos_label=0))\n",
    "print(\"Precisão da classificação (Classe 1):\", precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc582c80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:29:59.589937Z",
     "start_time": "2024-04-12T22:29:59.572932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.8857142857142857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Separando os tweets e seus sentimentos\n",
    "X = [tweet[0] for tweet in tweets]\n",
    "y = [sentimento for _, sentimento in tweets]\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criando uma matriz de frequência de palavras\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Treinando o classificador Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "predictions = clf.predict(X_test_counts)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Acurácia do modelo:\", accuracy)\n",
    "# print(\"Precisão da classificação (Classe 0):\", precision_score(y_test, y_pred, pos_label=0))\n",
    "# print(\"Precisão da classificação (Classe 1):\", precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f7fc7a",
   "metadata": {},
   "source": [
    "Visão geral de como o algoritmo funciona:\n",
    "\n",
    "- Treinamento: Durante a fase de treinamento, o algoritmo calcula a probabilidade de cada classe (por exemplo, positivo, negativo) ocorrer, bem como as probabilidades condicionais de cada feature (palavra) dado cada classe. Essas probabilidades são estimadas a partir dos dados de treinamento.\n",
    "\n",
    "- Previsão: Durante a fase de previsão, o algoritmo utiliza o Teorema de Bayes para calcular a probabilidade de cada classe dado um conjunto de features (no caso, as palavras em um tweet). A classe com a maior probabilidade posterior é então atribuída como a classe prevista para a instância.\n",
    "\n",
    "- Suavização de Laplace: Para evitar problemas quando uma palavra não aparece no conjunto de treinamento para uma determinada classe, é comum aplicar uma técnica de suavização chamada Suavização de Laplace, que adiciona uma contagem pseudocount para todas as features durante o cálculo das probabilidades condicionais.\n",
    "\n",
    "- Vantagens e Desvantagens: O Naive Bayes é fácil de implementar, eficiente em termos de tempo de treinamento e pode funcionar bem mesmo com conjuntos de dados pequenos. No entanto, sua suposição de independência entre as features pode não ser realista em muitos casos, o que pode levar a resultados subótimos em certas situações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030f571b",
   "metadata": {},
   "source": [
    "## Testing Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831e187",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/naive_bayes_test.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e576763",
   "metadata": {},
   "source": [
    "## Applications of Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3951541",
   "metadata": {},
   "source": [
    "O algoritmo Naive Bayes é amplamente utilizado em uma variedade de aplicações em Data Science e Machine Learning devido à sua simplicidade, eficiência computacional e bom desempenho em muitos cenários. Como em:\n",
    "\n",
    "1. **Classificação de Texto:** Naive Bayes é frequentemente usado para classificar documentos de texto em categorias, como spam vs. não spam em e-mails, classificação de sentimentos em redes sociais, detecção de tópicos em artigos de notícias, entre outros.\n",
    "\n",
    "2. **Filtragem de Spam:** É um uso clássico de Naive Bayes, onde o algoritmo é treinado com uma base de dados de e-mails rotulados como spam ou não spam, e então usado para prever se novos e-mails são spam ou não.\n",
    "\n",
    "3. **Análise de Sentimento:** Naive Bayes é eficaz na análise de sentimentos em dados textuais, como comentários de clientes, análise de feedbacks de produtos e análise de redes sociais, ajudando a determinar se um texto é positivo, negativo ou neutro.\n",
    "\n",
    "4. **Classificação de Documentos:** Além de filtragem de spam, Naive Bayes é usado para categorizar documentos em diferentes classes, como classificar notícias em categorias como política, esportes, entretenimento, etc.\n",
    "\n",
    "5. **Sistemas de Recomendação:** Pode ser utilizado em sistemas de recomendação para classificar e sugerir itens com base no histórico de interações do usuário, como classificar produtos em sites de compras ou recomendar filmes ou músicas em plataformas de streaming.\n",
    "\n",
    "6. **Diagnóstico Médico:** Na área médica, o Naive Bayes pode ser aplicado para auxiliar no diagnóstico de doenças, utilizando características dos pacientes para prever a presença ou ausência de certas condições médicas.\n",
    "\n",
    "7. **Análise de Risco Financeiro:** Naive Bayes é utilizado em análises de risco financeiro para prever riscos de crédito, detectar fraudes em transações financeiras e realizar análises de mercado.\n",
    "\n",
    "8. **Previsão de Churn:** Na área de negócios, o Naive Bayes pode ser usado para prever a probabilidade de um cliente cancelar um serviço (churn), com base em dados históricos de comportamento do cliente.\n",
    "\n",
    "Sua simplicidade e eficiência o tornam uma escolha popular em uma variedade de cenários."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c09f089",
   "metadata": {},
   "source": [
    "## Naïve Bayes Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cfe69d",
   "metadata": {},
   "source": [
    "O algoritmo Naive Bayes é baseado em algumas premissas importantes, que são fundamentais para o seu funcionamento. Como:\n",
    "\n",
    "1. **Independência Condicional:** Esta é a premissa mais crucial e é de onde o \"Naive\" em \"Naive Bayes\" vem. O algoritmo assume que as features (ou atributos) usadas para a classificação são independentes entre si, dadas as classes. Em outras palavras, ele assume que a presença ou ausência de uma característica não está relacionada à presença ou ausência de outras features, dado o resultado da classe. Embora esta premissa raramente seja verdadeira na prática, o Naive Bayes muitas vezes funciona bem mesmo quando ela é violada, tornando-o muito eficaz em muitos casos.\n",
    "\n",
    "2. **Presença de Dados de Treinamento:** O Naive Bayes requer um conjunto de dados de treinamento que inclua exemplos rotulados. Ou seja, para cada exemplo, deve-se saber a que classe ele pertence. Esses exemplos de treinamento são essenciais para estimar as probabilidades necessárias para a classificação.\n",
    "\n",
    "3. **Distribuição de Features:** O Naive Bayes assume uma distribuição específica para as features. Embora seja comum assumir uma distribuição de Bernoulli para features binárias, o Naive Bayes também pode ser aplicado com **distribuições multinomiais** (para features categóricas) e distribuições gaussianas (para features contínuas).\n",
    "\n",
    "4. **Probabilidades Condicionais:** O algoritmo Naive Bayes usa a teoria das probabilidades para calcular a probabilidade de uma instância pertencer a cada classe com base nas features observadas. Ele assume que as probabilidades condicionais de cada classe dado o conjunto de features podem ser calculadas facilmente.\n",
    "\n",
    "Essas premissas são simplificações significativas da realidade, e é por isso que o algoritmo é chamado de \"Naive\" (ingênuo). No entanto, apesar de suas simplificações, o Naive Bayes muitas vezes funciona surpreendentemente bem em uma variedade de problemas de classificação, desde que as premissas sejam razoavelmente satisfeitas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c69ba14",
   "metadata": {},
   "source": [
    "# Week 3 - Vector Space Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f9b19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:22:55.561680Z",
     "start_time": "2024-04-12T22:22:55.556434Z"
    }
   },
   "source": [
    "## Objetivos de aprendizagem\n",
    "- Covariance matrices\n",
    "- Dimensionality reduction\n",
    "- Principal component analysis\n",
    "- Cosine similarity\n",
    "- Euclidean distance\n",
    "- Co-occurrence matrices\n",
    "- Vector representations\n",
    "- Vector space models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4c4194",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bff9421d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5924616e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "462260a7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ad10cb9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2216d78a",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.465px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
