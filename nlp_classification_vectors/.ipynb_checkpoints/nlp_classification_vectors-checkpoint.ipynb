{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f53e68d",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Classification and Vector Spaces\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Classification and Vector Spaces da DeeplearninigAI.\n",
    "\n",
    "Repositório com a trilha Natural Language Processing:\n",
    "https://github.com/k3ybladewielder/nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbe782",
   "metadata": {},
   "source": [
    "# Objetivos de aprendizagem\n",
    "- Sentiment analysis\n",
    "- Logistic regression\n",
    "- Data pre-processing\n",
    "- Calculating word frequencies\n",
    "- Feature extraction\n",
    "- Vocabulary creation\n",
    "- Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1d75c",
   "metadata": {},
   "source": [
    "# Week 1 - Sentiment Analysis With Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0694bb",
   "metadata": {},
   "source": [
    "## Supervised ML and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250c7e2",
   "metadata": {},
   "source": [
    "Os algoritmos de **Machine Learning (ML)** supervisionados são um tipo de algoritmo que aprende a partir de dados rotulados, ou seja, dados que já possuem um rótulo ou uma classificação pré-definida. Esses algoritmos usam esses dados rotulados para aprender a fazer previsões ou classificações em novos dados.\n",
    "\n",
    "Na área de Processamento de Linguagem Natural (NLP), os algoritmos de ML supervisionados são usados para uma variedade de tarefas, como classificação de sentimentos, identificação de entidades nomeadas, análise de tópicos, tradução automática, entre outras.\n",
    "\n",
    "Um exemplo de como esses algoritmos são usados em NLP é na classificação de sentimentos em textos. Nesse caso, um modelo de ML supervisionado seria **treinado em um conjunto de dados rotulados** que contém textos e suas respectivas classificações de sentimento (por exemplo, positivo, negativo ou neutro). O algoritmo usaria esses dados para **aprender a reconhecer padrões nos textos** e, em seguida, aplicaria esses padrões para **classificar o sentimento em novos textos**.\n",
    "\n",
    "Outro exemplo é na identificação de **entidades nomeadas (NER)**, que é uma tarefa que envolve a identificação de nomes de pessoas, locais, organizações e outras entidades em um texto. Nesse caso, um modelo de ML supervisionado seria treinado em um conjunto de dados rotulados que contém textos e suas respectivas entidades nomeadas. O algoritmo usaria esses dados para aprender a reconhecer os padrões de palavras e contextos que indicam a presença de uma entidade nomeada em um texto, e, em seguida, aplicaria esses padrões para identificar entidades em novos textos.\n",
    "\n",
    "Em resumo, os algoritmos de ML supervisionados são uma técnica poderosa para resolver problemas em NLP, permitindo que modelos aprendam com dados rotulados e possam fazer previsões ou classificações em novos dados com base no que foi aprendido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf56de",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/supervised_ml.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ab5ee5",
   "metadata": {},
   "source": [
    "Um pipeline básico para a _task_ de análise de sentimento (classificação) geralmente envolve as etapas de:\n",
    "- **Pré-processamento de texto**: Esta etapa envolve a limpeza e preparação dos dados. O texto é normalmente convertido em minúsculas, removidos caracteres especiais, removidos números e pontuações, e realizada a tokenização do texto para obter as palavras individuais.\n",
    "- **Criação de features**: Nesta etapa, são criadas as features que serão usadas pelo modelo de classificação. As features podem incluir a contagem de palavras, a frequência de palavras, o tipo de palavras, o uso de negação, entre outras.\n",
    "- **Treinamento do modelo**: O modelo de classificação é treinado em um conjunto de dados rotulados que contêm exemplos de texto e suas respectivas classificações de sentimento. Existem vários algoritmos de aprendizado de máquina que podem ser usados para treinar um modelo, como Árvores de Decisão, Naive Bayes, Regressão Logística, SVM, Redes Neurais, etc.\n",
    "- **Avaliação do modelo**: Após o treinamento, o modelo é avaliado em um conjunto de dados de teste para verificar sua precisão. É comum dividir o conjunto de dados em conjunto de treinamento, validação e teste.\n",
    "- **Implantação**: Finalmente, o modelo treinado é usado para classificar o sentimento de novos textos. Novos textos passam pelas mesmas etapas de pré-processamento e criação de features, e o modelo treinado é usado para prever a classificação de sentimento do texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe988769",
   "metadata": {},
   "source": [
    "## Vocabulary and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389dbe23",
   "metadata": {},
   "source": [
    "Para representar textos de forma numérica, primeiro precisamos construir um vocabulário, com eles poderemos _encodar_ qualquer texto como um array de números. Um Vocabulário é uma lista com palavras únicas, não repetidas.\n",
    "\n",
    "Uma forma simples de extrair features do texto é usando o vocabulário, verificando cada palavra do vocabulário que aparece no texto. Caso as palavras no texto que estamos extraindo a feature tenham apareca no vocabulário, atribuímos o valor 1 para ela, e zero para as palavras que do vocabulário que não aparecem no texto. Assim, estamos representando o texto usando [**one-hot encoding**](https://k3ybladewielder.medium.com/introdu%C3%A7%C3%A3o-%C3%A0-nlp-4d7d98b9a36a) ou representação esparsa. Mas esse método pode ser problemático porquê o número de features é igual ao número de palavras no vocabulário e a grande maioria das features serão zero, aumentando excessivamente o tempo de treino e predição dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e076b02",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/vocab.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d7a4c5",
   "metadata": {},
   "source": [
    "Existem diversas técnicas para representar textos como vetores, sendo as mais comuns a Bag of Words (BoW) e a Representação Distribuída de Palavras (Word Embeddings). Vou explicar brevemente cada uma delas:\n",
    "- **Bag of Words (BoW)**: Nessa técnica, o texto é representado como um vetor contendo a contagem de ocorrências de cada palavra presente no texto. Cada palavra é considerada como uma dimensão do vetor. Dessa forma, quanto mais vezes uma palavra aparecer no texto, maior será o valor correspondente na dimensão correspondente no vetor. Por exemplo, se um texto contém as palavras \"gato\", \"cão\" e \"casa\", a representação BoW seria um vetor com três dimensões, com valores correspondentes à contagem de ocorrências de cada palavra no texto.\n",
    "- **Word Embeddings**: Essa técnica é baseada em modelos de linguagem neural, que mapeiam cada palavra em um espaço vetorial de alta dimensão, onde palavras semelhantes têm representações próximas. A ideia é que cada palavra seja representada por um vetor de números reais que captura seu significado semântico. Esses vetores podem ser aprendidos a partir de grandes quantidades de textos usando técnicas de aprendizado de máquina, como Word2Vec, GloVe ou FastText. Os vetores resultantes podem ser usados para representar cada palavra em um texto como um vetor numérico. A representação distribuída de palavras pode capturar relações semânticas entre palavras, como sinonímia e antonímia, e pode ser usada para tarefas mais complexas, como análise de sentimento ou classificação de texto.\n",
    "\n",
    "Ambas as técnicas são amplamente utilizadas em NLP, dependendo do objetivo e do contexto da tarefa em questão. A escolha da técnica de representação de texto pode influenciar significativamente o desempenho do modelo de aprendizado de máquina, e é importante escolher a técnica mais adequada para a tarefa específica.\n",
    "\n",
    "Tanto a técnica de Bag of Words (BoW) quanto a Word Embeddings têm seus **pontos fortes e fracos**, e a escolha de qual usar depende do contexto e do objetivo da tarefa em questão.\n",
    "\n",
    "A representação **BoW** pode ser uma escolha adequada para **tarefas simples de classificação de texto**, como classificação de spam ou análise de sentimento, onde a presença ou ausência de palavras específicas pode ser um indicador importante para a classificação. Além disso, a representação **BoW é computacionalmente eficiente e fácil de interpretar**, o que pode ser uma vantagem para problemas onde a transparência do modelo é importante. No entanto, a representação **BoW não leva em consideração a ordem das palavras no texto, o que pode limitar sua capacidade de capturar nuances semânticas.**\n",
    "\n",
    "Já Word Embedding é mais adequada para **tarefas que envolvem análise semântica**, como tradução automática, classificação de tópicos ou análise de sentimento baseada em frases complexas. A representação Word Embedding **leva em consideração a ordem e o contexto das palavras**, e **pode capturar a similaridade semântica entre palavras que não aparecem juntas com frequência**. Além disso, a representação distribuída de palavras **pode ser usada para inicializar redes neurais em tarefas de aprendizado profundo**, melhorando o desempenho do modelo. No entanto, Word Embedding **pode ser computacionalmente intensiva e requer grandes quantidades de dados de treinamento para obter bons resultados**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745bf6a0",
   "metadata": {},
   "source": [
    "## Negative and Positive Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8160eaee",
   "metadata": {},
   "source": [
    "Numa task de classificação de sentimentos, podemos identificar as palavras positivas e negativas a partir da frequencia de ocorrencia em que elas ocorrem nos textos positivos e negativos. Usando essa contagem, podemos extrair features e usá-las no modelo de classificação, como a **regressão logística**. \n",
    "\n",
    "<img src=\"./imgs/tweet_corpus.png\">\n",
    "\n",
    "A partir da contagem da frequencia das palavras em cada classe, chegamos a essa tabela. Na prática, essa tabela será um dicionário que mapeia a classe da palavra e sua frequência de ocorrência.\n",
    "\n",
    "<img src=\"./imgs/word_freq_tweets.png\">\n",
    "\n",
    "Podemos representar essa tabela de frequencia com um array com 3 features, aumentando a velocidade na implementação, porquê em vez de termos v features, teremos apenas 3 para que o modelo aprenda. \n",
    "\n",
    "Aqui, a primeira feature é um bias, depois o somatório das palavras da label positiva e o somatório das palavras da label negativa. Assim, teremos o novo vetor com 3 features.\n",
    "\n",
    "<img src=\"./imgs/vector_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873453e1",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/feature_extraction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a367dc41",
   "metadata": {},
   "source": [
    "## Feature Extraction with Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25adb3",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214aa9de",
   "metadata": {},
   "source": [
    "O pré-processamento geralmente envolve as etapas de limpeza e preparação dos dados. O texto é normalmente convertido em minúsculas, removidos caracteres especiais, removidos números e pontuações, e realizada a tokenização do texto para obter as palavras individuais.\n",
    "\n",
    "Exemplo em python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6ea58a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T22:03:05.318997Z",
     "start_time": "2024-04-08T22:02:59.150249Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exemplo', 'texto', 'limpo', 'tokenizado']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/joaoag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def text_cleaning(text):\n",
    "    # Extrai as stopwords em português\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"Este é um exemplo de texto que será limpo e tokenizado!\"\n",
    "tokens = text_cleaning(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce2d59f",
   "metadata": {},
   "source": [
    "Mas além disso também podemos fazer **Stemming** e **lematização**. Elas são técnicas de pré-processamento de texto com o objetivo de reduzir as palavras em sua forma base ou raiz, simplificando o processo de análise de texto.\n",
    "\n",
    "**Stemming** é um processo mais simples e rápido de normalização de palavras, que envolve a remoção de sufixos com o objetivo de transformar uma palavra em sua raiz, ou seja, em sua forma básica. Um exemplo de algoritmo de stemming é o Porter Stemmer, que é amplamente utilizado em NLP. O ponto positivo do stemming é sua simplicidade e velocidade de processamento, o que pode ser útil em projetos com grandes volumes de dados. No entanto, o stemming pode produzir algumas palavras raiz que não são facilmente reconhecidas, o que pode ser um problema em alguns casos.\n",
    "\n",
    "Já a **lematização** é um processo mais complexo de normalização de palavras, que envolve a análise do contexto da palavra para determinar sua forma básica. A lematização utiliza um dicionário de palavras ou um algoritmo para mapear a palavra para sua forma base. O ponto positivo da lematização é que ela produz palavras que são facilmente reconhecíveis, o que pode ser importante em projetos que exigem maior precisão na análise de texto. No entanto, a lematização é um processo mais lento e computacionalmente mais caro do que o stemming, o que pode ser um problema em projetos com grandes volumes de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbaa0f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T22:03:05.331021Z",
     "start_time": "2024-04-08T22:03:05.321793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exempl', 'text', 'limp', 'tokeniz', 'stemmed']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def text_cleaning_s(text):\n",
    "    # Extrai as stopwords em português\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza o stemming dos tokens\n",
    "    stemmer = SnowballStemmer('portuguese')\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"Este é um exemplo de texto que será limpo, tokenizado e stemmed!\"\n",
    "tokens = text_cleaning_s(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a82a6b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T22:03:07.299795Z",
     "start_time": "2024-04-08T22:03:05.332772Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/joaoag/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/joaoag/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'text', 'cleaned', 'tokenized', 'lemmatized']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def text_cleaning_l(text):\n",
    "    # Extrai as stopwords em inglês\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza a lematização dos tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"This is an example of text that will be cleaned, tokenized, and lemmatized!\"\n",
    "tokens = text_cleaning_l(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da471b4",
   "metadata": {},
   "source": [
    "## Logistic Regression Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9657fd",
   "metadata": {},
   "source": [
    "A **regressão logística** é frequentemente utilizada em análise de sentimento porque é um algoritmo de classificação simples, rápido e eficiente para problemas de classificação binária, ou seja, quando há apenas duas classes, como positivo e negativo. Além disso, a regressão logística é facilmente interpretável, o que significa que é possível entender como o modelo chegou a uma determinada previsão.\n",
    "\n",
    "Outra vantagem da regressão logística é que ela lida bem com problemas de dados desbalanceados, que é comum em análise de sentimento, onde muitas vezes há mais exemplos de uma classe do que outra. A regressão logística usa uma função sigmoide para calcular as probabilidades de cada classe, e essa função é bem adequada para casos de classes desbalanceadas.\n",
    "\n",
    "Outro motivo para a popularidade da regressão logística em análise de sentimento é que ela é relativamente fácil de implementar e ajustar. É possível utilizar diferentes técnicas de regularização, como a regularização L1 ou L2, para evitar overfitting e melhorar o desempenho do modelo.\n",
    "\n",
    "No entanto, é importante ressaltar que a regressão logística pode não ser adequada para problemas de classificação multiclasse, ou seja, quando há mais de duas classes, como em análise de tópicos. Nesses casos, outros algoritmos, como Árvores de Decisão, SVMs ou Redes Neurais, podem ser mais apropriados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed79c3d",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/logistic_regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cda409d",
   "metadata": {},
   "source": [
    "O modelo é treinado usando um conjunto de dados rotulados, onde cada texto é rotulado como tendo um sentimento positivo ou negativo. O objetivo do treinamento é ajustar os parâmetros do modelo para que ele possa fazer previsões precisas em novos dados.\n",
    "\n",
    "Durante o treinamento, o modelo utiliza a função logística para calcular a probabilidade de um texto ter um sentimento positivo ou negativo com base em seus vetores de características. A função logística produz um valor entre 0 e 1, que representa a probabilidade de um texto ter um sentimento positivo. Se a probabilidade for maior que 0,5, o modelo classifica o texto como tendo um sentimento positivo. Caso contrário, o texto é classificado como tendo um sentimento negativo.\n",
    "\n",
    "Após o treinamento, o modelo pode ser usado para fazer previsões em novos dados. Para cada novo texto, o modelo converte-o em um vetor de características e utiliza a função logística para calcular a probabilidade de ter um sentimento positivo ou negativo. Em seguida, classifica o texto como tendo um sentimento positivo ou negativo com base no valor calculado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43158a15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T22:03:07.310698Z",
     "start_time": "2024-04-08T22:03:07.303668Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_cleaning_l(text):\n",
    "    # Extrai as stopwords\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza a lematização dos tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01bcd386",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T22:03:07.819369Z",
     "start_time": "2024-04-08T22:03:07.313204Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frase</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Esta é uma frase positiva.</td>\n",
       "      <td>positivo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Esta é uma frase negativa.</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Esta é uma frase positiva.</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Esta é uma frase positiva.</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Esta é uma frase positiva.</td>\n",
       "      <td>negativo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        frase     label\n",
       "0  Esta é uma frase positiva.  positivo\n",
       "1  Esta é uma frase negativa.  negativo\n",
       "2  Esta é uma frase positiva.  negativo\n",
       "3  Esta é uma frase positiva.  negativo\n",
       "4  Esta é uma frase positiva.  negativo"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Criação das frases aleatórias\n",
    "np.random.seed(42)\n",
    "frases = np.random.choice(['Esta é uma frase positiva.',\n",
    "                           'Esta é uma frase negativa.'], size=100000)\n",
    "\n",
    "# Criação das labels aleatórias\n",
    "labels = np.random.choice(['positivo', 'negativo'], size=100000)\n",
    "\n",
    "# Criação do DataFrame\n",
    "df = pd.DataFrame({'frase': frases, 'label': labels})\n",
    "\n",
    "# Visualização das primeiras linhas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37770154",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T22:03:09.078661Z",
     "start_time": "2024-04-08T22:03:07.821522Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby('label').agg({'frase': 'count'}).reset_index(drop=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed88b71f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T22:03:42.591409Z",
     "start_time": "2024-04-08T22:03:09.081434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classificação: 0.5006333333333334\n",
      "Precisão da classificação: 0.5049816115011702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pré-processamento das frases\n",
    "df['frase_processada'] = df['frase'].apply(text_cleaning_l)\n",
    "\n",
    "# vetorização das frases\n",
    "vectorizer = CountVectorizer(tokenizer=lambda text: text, lowercase=False)\n",
    "X = vectorizer.fit_transform(df['frase_processada'].apply(lambda tokens: ' '.join(tokens)))\n",
    "\n",
    "# divisão em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# criação do modelo de regressão logística e treinamento\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# avaliação do modelo nos dados de teste\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# avaliando o modelo\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "print(\"Acurácia da classificação:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precisão da classificação:\", precision_score(y_test, y_pred, pos_label='positivo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e49a8a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T22:04:13.904116Z",
     "start_time": "2024-04-08T22:03:42.593154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classificação: 0.5006333333333334\n",
      "Precisão da classificação: 0.5049816115011702\n"
     ]
    }
   ],
   "source": [
    "# Mesmo exemplo mas com o TF-IDF como vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pré-processamento das frases\n",
    "df['frase_processada'] = df['frase'].apply(text_cleaning_l)\n",
    "\n",
    "# vetorização das frases\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda text: text, lowercase=False)\n",
    "X = vectorizer.fit_transform(df['frase_processada'].apply(lambda tokens: ' '.join(tokens)))\n",
    "\n",
    "# divisão em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# criação do modelo de regressão logística e treinamento\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# avaliação do modelo nos dados de teste\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# avaliando o modelo\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "print(\"Acurácia da classificação:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precisão da classificação:\", precision_score(y_test, y_pred, pos_label='positivo'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52192b18",
   "metadata": {},
   "source": [
    "## Logistic Regression Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d5feff",
   "metadata": {},
   "source": [
    "A função de custo da regressão logística é usada para avaliar quão bem um modelo de regressão logística está performando em relação aos dados observados. Ela é frequentemente utilizada em problemas de classificação binária, onde o objetivo é prever se uma observação pertence a uma das duas classes possíveis. Aqui está uma explicação de cada etapa da função de custo da regressão logística:\n",
    "\n",
    "1. **Hipótese da Regressão Logística:**\n",
    "   A primeira etapa é a formulação da hipótese do modelo de regressão logística. A hipótese é uma função que mapeia as entradas para uma probabilidade estimada de pertencer a uma das classes. A função logística (também conhecida como função sigmoide) é comumente utilizada como a função de hipótese na regressão logística e é dada pela seguinte equação:\n",
    "\n",
    "   $$ h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^Tx}}$$\n",
    "\n",
    "   Onde:\n",
    "   - $ h_{\\theta}(x) $ é a estimativa da probabilidade de \\( x \\) pertencer à classe positiva,\n",
    "   - $ \\theta $ são os parâmetros do modelo,\n",
    "   - $ x $ é o vetor de entrada,\n",
    "   - $ e $ é o número de Euler (aproximadamente 2.71828).\n",
    "\n",
    "\n",
    "2. **Função de Custo Logístico:**\n",
    "   A função de custo é uma medida de quão bem a hipótese do modelo se ajusta aos dados observados. Para a regressão logística, a função de custo (também conhecida como função de perda ou função de erro) é definida usando a técnica de máxima verossimilhança. A função de custo logístico para um único exemplo de treinamento é dada pela seguinte equação:\n",
    "\n",
    "   $$ J(\\theta) = -y \\log(h_{\\theta}(x)) - (1 - y) \\log(1 - h_{\\theta}(x)) $$\n",
    "\n",
    "   Onde:\n",
    "   - $ J(\\theta) $ é a função de custo,\n",
    "   - $ y $ é a classe verdadeira do exemplo (0 ou 1),\n",
    "   - $ h_{\\theta}(x) $ é a estimativa da probabilidade da classe positiva dada pela hipótese.\n",
    "\n",
    "\n",
    "3. **Função de Custo Médio (ou Função de Custo Regularizada):**\n",
    "   Para avaliar o desempenho do modelo em todo o conjunto de dados, a função de custo médio é calculada. Isso é feito tirando a média dos custos individuais de todos os exemplos de treinamento. Além disso, uma penalidade de regularização pode ser adicionada para evitar overfitting. A função de custo médio (ou função de custo regularizada) é dada pela seguinte equação:\n",
    "\n",
    "   $$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}[-y^{(i)} \\log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2 $$\n",
    "\n",
    "   Onde:\n",
    "   - $ m $ é o número total de exemplos de treinamento,\n",
    "   - $ n $ é o número de características,\n",
    "   - $ \\lambda $ é o parâmetro de regularização,\n",
    "   - $ \\theta_j $ é o j-ésimo parâmetro do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf29282",
   "metadata": {},
   "source": [
    "# Week 2 - Sentiment Analysis with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2f2f6a",
   "metadata": {},
   "source": [
    "## Recaptulando...\n",
    "\n",
    "1. **Probabilidade:**\n",
    "   A probabilidade é uma medida numérica que quantifica a incerteza associada a um evento. Em termos simples, é a chance de que algo aconteça. A probabilidade de um evento é sempre um número entre 0 e 1, onde 0 indica impossibilidade absoluta do evento ocorrer e 1 indica certeza absoluta de que o evento ocorrerá.\n",
    "   \n",
    "<img src=\"./imgs/prob_positive_tweet.png\">\n",
    "<img src=\"./imgs/prob_positive_tweet2.png\">\n",
    "\n",
    "2. **Probabilidade Condicional:**\n",
    "   A probabilidade condicional é a probabilidade de um evento ocorrer dado que outro evento já ocorreu. É denotada por $ P(A|B) $, que lê-se como \"a probabilidade de A dado B\". A fórmula para calcular a probabilidade condicional é:\n",
    "\n",
    "   $$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$\n",
    "\n",
    "   Onde:\n",
    "   - $ P(A|B) $ é a probabilidade de A dado B,\n",
    "   - $ P(A \\cap B) $ é a probabilidade da interseção de A e B,\n",
    "   - $ P(B) $ é a probabilidade de B.\n",
    "\n",
    "   Em palavras simples, a probabilidade condicional é a proporção de vezes que o evento A ocorre quando o evento B ocorre.\n",
    "\n",
    "<img src=\"./imgs/conditional_prob_tweet.png\">\n",
    "\n",
    "\n",
    "3. **Regra de Bayes:**\n",
    "   A regra de Bayes é uma ferramenta fundamental na teoria das probabilidades que permite atualizar as probabilidades de uma hipótese à luz de novas evidências. Formalmente, a regra de Bayes é expressa como:\n",
    "\n",
    "   $ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $\n",
    "\n",
    "   Onde:\n",
    "   - $ P(A|B) $ é a probabilidade de A dado B (posterior),\n",
    "   - $ P(B|A) $ é a probabilidade de B dado A (likelihood),\n",
    "   - $ P(A) $ é a probabilidade de A (prior),\n",
    "   - $ P(B) $ é a probabilidade de B.\n",
    "\n",
    "   A regra de Bayes nos permite calcular a probabilidade de uma hipótese (A) ser verdadeira dada uma evidência observada (B), usando a probabilidade da evidência dada a hipótese (likelihood), a probabilidade a priori da hipótese e a probabilidade marginal da evidência.A diferença principal entre a probabilidade condicional e a regra de Bayes é que a probabilidade condicional é uma medida da probabilidade de um evento ocorrer dado que outro evento já ocorreu, enquanto a regra de Bayes é uma ferramenta para atualizar a probabilidade de uma hipótese à luz de novas evidências. A regra de Bayes utiliza a probabilidade condicional como um de seus componentes para calcular a probabilidade posterior.\n",
    "\n",
    "<img src=\"./imgs/bayes_rule_tweet.png\">\n",
    "\n",
    "A probabilidade é a chance de um evento ocorrer, a probabilidade condicional é a probabilidade de um evento ocorrer dado que outro evento já ocorreu, e a regra de Bayes é uma ferramenta para atualizar a probabilidade de uma hipótese à luz de novas evidências."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7f7f89",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "O algoritmo Naive Bayes é um método de classificação probabilístico baseado no teorema de Bayes, com uma suposição \"ingênua\" de independência condicional entre os recursos.\n",
    "\n",
    "1. **Suposição de Independência Condicional:**\n",
    "   A primeira etapa do algoritmo Naive Bayes é a suposição de independência condicional entre os recursos. Isso significa que assumimos que os recursos são independentes entre si, dado o valor da variável de classe. Apesar de ser uma suposição forte e muitas vezes não ser verdadeira na prática, ela simplifica os cálculos e torna o algoritmo computacionalmente eficiente.\n",
    "\n",
    "2. **Construção do Modelo de Probabilidade:**\n",
    "   O próximo passo é construir o modelo de probabilidade. Isso envolve calcular a probabilidade de cada classe e a probabilidade de cada valor do recurso dado cada classe. Em outras palavras, para cada classe, calculamos a probabilidade a priori da classe ( $ P(C_k) $ ) e a probabilidade de cada recurso ( $ P(X_i | C_k) $ ).\n",
    "\n",
    "3. **Classificação:**\n",
    "   Depois que o modelo de probabilidade é construído, podemos usá-lo para fazer previsões sobre novos exemplos. Dada uma nova instância com valores de recursos $ x_1, x_2, ..., x_n $, queremos calcular a probabilidade de pertencer a cada classe e, em seguida, atribuir a classe com a maior probabilidade como a classe prevista para a instância. Isso é feito usando o teorema de Bayes:\n",
    "\n",
    "   $$ P(C_k | x_1, x_2, ..., x_n) = \\frac{P(C_k) \\cdot \\prod_{i=1}^{n} P(x_i | C_k)}{P(x_1, x_2, ..., x_n)} $$\n",
    "\n",
    "   Onde:\n",
    "   - $ P(C_k | x_1, x_2, ..., x_n) $ é a probabilidade da classe $ C_k $ dado os valores dos recursos,\n",
    "   - $ P(C_k) $ é a probabilidade a priori da classe $ C_k $,\n",
    "   - $ P(x_i | C_k) $ é a probabilidade de cada valor do recurso dado a classe $ C_k $,\n",
    "   - $ P(x_1, x_2, ..., x_n) $ é a probabilidade dos valores dos recursos.\n",
    "\n",
    "4. **Estimação de Parâmetros:**\n",
    "   Durante a etapa de construção do modelo, precisamos estimar os parâmetros do modelo, ou seja, as probabilidades a priori das classes e as probabilidades condicionais dos recursos para cada classe. Isso geralmente é feito usando técnicas como a frequência relativa de ocorrência dos dados de treinamento.\n",
    "\n",
    "5. **Suavização de Laplace (Opcional):**\n",
    "   Em alguns casos, para evitar probabilidades condicionais iguais a zero para recursos não observados em uma classe particular, pode ser aplicada a suavização de Laplace, adicionando uma pequena quantidade aos contadores de frequência de cada valor de recurso para cada classe durante a estimativa dos parâmetros.\n",
    "\n",
    "Essas são as etapas principais da função do algoritmo Naive Bayes, desde a suposição de independência condicional até a classificação de novas instâncias usando o teorema de Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f515b321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972023bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575474d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cb3d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6051ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9333dd34",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9f4c5c",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.465px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
