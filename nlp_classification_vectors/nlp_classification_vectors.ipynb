{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb15905d",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Classification and Vector Spaces\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Classification and Vector Spaces da DeeplearninigAI.\n",
    "\n",
    "Repositório com a trilha Natural Language Processing:\n",
    "https://github.com/k3ybladewielder/nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9573774",
   "metadata": {},
   "source": [
    "# Objetivos de aprendizagem\n",
    "- Sentiment analysis\n",
    "- Logistic regression\n",
    "- Data pre-processing\n",
    "- Calculating word frequencies\n",
    "- Feature extraction\n",
    "- Vocabulary creation\n",
    "- Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd42fb",
   "metadata": {},
   "source": [
    "# Week 1 - Sentiment Analysis With Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7272d5fa",
   "metadata": {},
   "source": [
    "## Supervised ML and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f69d0",
   "metadata": {},
   "source": [
    "Os algoritmos de **Machine Learning (ML)** supervisionados são um tipo de algoritmo que aprende a partir de dados rotulados, ou seja, dados que já possuem um rótulo ou uma classificação pré-definida. Esses algoritmos usam esses dados rotulados para aprender a fazer previsões ou classificações em novos dados.\n",
    "\n",
    "Na área de Processamento de Linguagem Natural (NLP), os algoritmos de ML supervisionados são usados para uma variedade de tarefas, como classificação de sentimentos, identificação de entidades nomeadas, análise de tópicos, tradução automática, entre outras.\n",
    "\n",
    "Um exemplo de como esses algoritmos são usados em NLP é na classificação de sentimentos em textos. Nesse caso, um modelo de ML supervisionado seria **treinado em um conjunto de dados rotulados** que contém textos e suas respectivas classificações de sentimento (por exemplo, positivo, negativo ou neutro). O algoritmo usaria esses dados para **aprender a reconhecer padrões nos textos** e, em seguida, aplicaria esses padrões para **classificar o sentimento em novos textos**.\n",
    "\n",
    "Outro exemplo é na identificação de **entidades nomeadas (NER)**, que é uma tarefa que envolve a identificação de nomes de pessoas, locais, organizações e outras entidades em um texto. Nesse caso, um modelo de ML supervisionado seria treinado em um conjunto de dados rotulados que contém textos e suas respectivas entidades nomeadas. O algoritmo usaria esses dados para aprender a reconhecer os padrões de palavras e contextos que indicam a presença de uma entidade nomeada em um texto, e, em seguida, aplicaria esses padrões para identificar entidades em novos textos.\n",
    "\n",
    "Em resumo, os algoritmos de ML supervisionados são uma técnica poderosa para resolver problemas em NLP, permitindo que modelos aprendam com dados rotulados e possam fazer previsões ou classificações em novos dados com base no que foi aprendido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd5e0ff",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/nlp_process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97079bff",
   "metadata": {},
   "source": [
    "Um pipeline básico para a _task_ de análise de sentimento (classificação) geralmente envolve as etapas de:\n",
    "- **Pré-processamento de texto**: Esta etapa envolve a limpeza e preparação dos dados. O texto é normalmente convertido em minúsculas, removidos caracteres especiais, removidos números e pontuações, e realizada a tokenização do texto para obter as palavras individuais.\n",
    "- **Criação de features**: Nesta etapa, são criadas as features que serão usadas pelo modelo de classificação. As features podem incluir a contagem de palavras, a frequência de palavras, o tipo de palavras, o uso de negação, entre outras.\n",
    "- **Treinamento do modelo**: O modelo de classificação é treinado em um conjunto de dados rotulados que contêm exemplos de texto e suas respectivas classificações de sentimento. Existem vários algoritmos de aprendizado de máquina que podem ser usados para treinar um modelo, como Árvores de Decisão, Naive Bayes, Regressão Logística, SVM, Redes Neurais, etc.\n",
    "- **Avaliação do modelo**: Após o treinamento, o modelo é avaliado em um conjunto de dados de teste para verificar sua precisão. É comum dividir o conjunto de dados em conjunto de treinamento, validação e teste.\n",
    "- **Implantação**: Finalmente, o modelo treinado é usado para classificar o sentimento de novos textos. Novos textos passam pelas mesmas etapas de pré-processamento e criação de features, e o modelo treinado é usado para prever a classificação de sentimento do texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5918a8",
   "metadata": {},
   "source": [
    "A **regressão logística** é frequentemente utilizada em análise de sentimento porque é um algoritmo de classificação simples, rápido e eficiente para problemas de classificação binária, ou seja, quando há apenas duas classes, como positivo e negativo. Além disso, a regressão logística é facilmente interpretável, o que significa que é possível entender como o modelo chegou a uma determinada previsão.\n",
    "\n",
    "Outra vantagem da regressão logística é que ela lida bem com problemas de dados desbalanceados, que é comum em análise de sentimento, onde muitas vezes há mais exemplos de uma classe do que outra. A regressão logística usa uma função sigmoide para calcular as probabilidades de cada classe, e essa função é bem adequada para casos de classes desbalanceadas.\n",
    "\n",
    "Outro motivo para a popularidade da regressão logística em análise de sentimento é que ela é relativamente fácil de implementar e ajustar. É possível utilizar diferentes técnicas de regularização, como a regularização L1 ou L2, para evitar overfitting e melhorar o desempenho do modelo.\n",
    "\n",
    "No entanto, é importante ressaltar que a regressão logística pode não ser adequada para problemas de classificação multiclasse, ou seja, quando há mais de duas classes, como em análise de tópicos. Nesses casos, outros algoritmos, como Árvores de Decisão, SVMs ou Redes Neurais, podem ser mais apropriados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27eaaf",
   "metadata": {},
   "source": [
    "## Vocabulary and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42187bbd",
   "metadata": {},
   "source": [
    "Para representar textos de forma numérica, primeiro precisamos construir um vocabulário, com eles poderemos _encodar_ qualquer texto como um array de números. Um Vocabulário é uma lista com palavras únicas, não repetidas.\n",
    "\n",
    "Uma forma simples de extrair features do texto é usando o vocabulário, verificando cada palavra do vocabulário que aparece no texto. Caso as palavras no texto que estamos extraindo a feature tenham apareca no vocabulário, atribuímos o valor 1 para ela, e zero para as palavras que do vocabulário que não aparecem no texto. Assim, estamos representando o texto usando [**one-hot encoding**](https://k3ybladewielder.medium.com/introdu%C3%A7%C3%A3o-%C3%A0-nlp-4d7d98b9a36a) ou representação esparsa. Mas esse método pode ser problemático porquê o número de features é igual ao número de palavras no vocabulário e a grande maioria das features serão zero, aumentando excessivamente o tempo de treino e predição dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa32ed9a",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/vocabulary_and_feature_extraction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38cb780",
   "metadata": {},
   "source": [
    "Existem diversas técnicas para representar textos como vetores, sendo as mais comuns a Bag of Words (BoW) e a Representação Distribuída de Palavras (Word Embeddings). Vou explicar brevemente cada uma delas:\n",
    "- **Bag of Words (BoW)**: Nessa técnica, o texto é representado como um vetor contendo a contagem de ocorrências de cada palavra presente no texto. Cada palavra é considerada como uma dimensão do vetor. Dessa forma, quanto mais vezes uma palavra aparecer no texto, maior será o valor correspondente na dimensão correspondente no vetor. Por exemplo, se um texto contém as palavras \"gato\", \"cão\" e \"casa\", a representação BoW seria um vetor com três dimensões, com valores correspondentes à contagem de ocorrências de cada palavra no texto.\n",
    "- **Word Embeddings**: Essa técnica é baseada em modelos de linguagem neural, que mapeiam cada palavra em um espaço vetorial de alta dimensão, onde palavras semelhantes têm representações próximas. A ideia é que cada palavra seja representada por um vetor de números reais que captura seu significado semântico. Esses vetores podem ser aprendidos a partir de grandes quantidades de textos usando técnicas de aprendizado de máquina, como Word2Vec, GloVe ou FastText. Os vetores resultantes podem ser usados para representar cada palavra em um texto como um vetor numérico. A representação distribuída de palavras pode capturar relações semânticas entre palavras, como sinonímia e antonímia, e pode ser usada para tarefas mais complexas, como análise de sentimento ou classificação de texto.\n",
    "\n",
    "Ambas as técnicas são amplamente utilizadas em NLP, dependendo do objetivo e do contexto da tarefa em questão. A escolha da técnica de representação de texto pode influenciar significativamente o desempenho do modelo de aprendizado de máquina, e é importante escolher a técnica mais adequada para a tarefa específica.\n",
    "\n",
    "Tanto a técnica de Bag of Words (BoW) quanto a Word Embeddings têm seus **pontos fortes e fracos**, e a escolha de qual usar depende do contexto e do objetivo da tarefa em questão.\n",
    "\n",
    "A representação **BoW** pode ser uma escolha adequada para **tarefas simples de classificação de texto**, como classificação de spam ou análise de sentimento, onde a presença ou ausência de palavras específicas pode ser um indicador importante para a classificação. Além disso, a representação **BoW é computacionalmente eficiente e fácil de interpretar**, o que pode ser uma vantagem para problemas onde a transparência do modelo é importante. No entanto, a representação **BoW não leva em consideração a ordem das palavras no texto, o que pode limitar sua capacidade de capturar nuances semânticas.**\n",
    "\n",
    "Já Word Embedding é mais adequada para **tarefas que envolvem análise semântica**, como tradução automática, classificação de tópicos ou análise de sentimento baseada em frases complexas. A representação Word Embedding **leva em consideração a ordem e o contexto das palavras**, e **pode capturar a similaridade semântica entre palavras que não aparecem juntas com frequência**. Além disso, a representação distribuída de palavras **pode ser usada para inicializar redes neurais em tarefas de aprendizado profundo**, melhorando o desempenho do modelo. No entanto, Word Embedding **pode ser computacionalmente intensiva e requer grandes quantidades de dados de treinamento para obter bons resultados**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e22bc7",
   "metadata": {},
   "source": [
    "## Negativa and Positive Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d1c78",
   "metadata": {},
   "source": [
    "Numa task de classificação de sentimentos, podemos identificar as palavras positivas e negativas a partir da frequencia de ocorrencia em que elas ocorrem nos textos positivos e negativos. Usando essa contagem, podemos extrair features e usá-las no modelo de classificação, como a **regressão logística**. \n",
    "\n",
    "<img src=\"./imgs/tweet_corpus.png\">\n",
    "\n",
    "A partir da contagem da frequencia das palavras em cada classe, chegamos a essa tabela. Na prática, essa tabela será um dicionário que mapeia a classe da palavra e sua frequência de ocorrência.\n",
    "\n",
    "<img src=\"./imgs/word_freq_tweets.png\">\n",
    "\n",
    "Podemos representar essa tabela de frequencia com um array com 3 features, aumentando a velocidade na implementação, porquê em vez de termos v features, teremos apenas 3 para que o modelo aprenda. \n",
    "\n",
    "Aqui, a primeira feature é um bias, depois o somatório das palavras da label positiva e o somatório das palavras da label negativa. Assim, teremos o novo vetor com 3 features.\n",
    "\n",
    "<img src=\"./imgs/vector_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032618e3",
   "metadata": {},
   "source": [
    "## Feature Extraction with Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b18d6",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7b9f9",
   "metadata": {},
   "source": [
    "O pré-processamento geralmente envolve as etapas de limpeza e preparação dos dados. O texto é normalmente convertido em minúsculas, removidos caracteres especiais, removidos números e pontuações, e realizada a tokenização do texto para obter as palavras individuais.\n",
    "\n",
    "Exemplo em python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfb3837e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T22:29:56.107429Z",
     "start_time": "2023-04-13T22:29:55.744889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exemplo', 'texto', 'limpo', 'tokenizado']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "def text_cleaning(text):\n",
    "    # Extrai as stopwords em português\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"Este é um exemplo de texto que será limpo e tokenizado!\"\n",
    "tokens = text_cleaning(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3056d361",
   "metadata": {},
   "source": [
    "Mas além disso também podemos fazer **Stemming** e **lematização**. Elas são técnicas de pré-processamento de texto com o objetivo de reduzir as palavras em sua forma base ou raiz, simplificando o processo de análise de texto.\n",
    "\n",
    "**Stemming** é um processo mais simples e rápido de normalização de palavras, que envolve a remoção de sufixos com o objetivo de transformar uma palavra em sua raiz, ou seja, em sua forma básica. Um exemplo de algoritmo de stemming é o Porter Stemmer, que é amplamente utilizado em NLP. O ponto positivo do stemming é sua simplicidade e velocidade de processamento, o que pode ser útil em projetos com grandes volumes de dados. No entanto, o stemming pode produzir algumas palavras raiz que não são facilmente reconhecidas, o que pode ser um problema em alguns casos.\n",
    "\n",
    "Já a **lematização** é um processo mais complexo de normalização de palavras, que envolve a análise do contexto da palavra para determinar sua forma básica. A lematização utiliza um dicionário de palavras ou um algoritmo para mapear a palavra para sua forma base. O ponto positivo da lematização é que ela produz palavras que são facilmente reconhecíveis, o que pode ser importante em projetos que exigem maior precisão na análise de texto. No entanto, a lematização é um processo mais lento e computacionalmente mais caro do que o stemming, o que pode ser um problema em projetos com grandes volumes de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097b0299",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T22:29:56.114687Z",
     "start_time": "2023-04-13T22:29:56.108799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exempl', 'text', 'limp', 'tokeniz', 'stemmed']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def text_cleaning_s(text):\n",
    "    # Extrai as stopwords em português\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza o stemming dos tokens\n",
    "    stemmer = SnowballStemmer('portuguese')\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"Este é um exemplo de texto que será limpo, tokenizado e stemmed!\"\n",
    "tokens = text_cleaning_s(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a381d7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-13T22:29:56.863540Z",
     "start_time": "2023-04-13T22:29:56.116728Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['example', 'text', 'cleaned', 'tokenized', 'lemmatized']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "def text_cleaning_l(text):\n",
    "    # Extrai as stopwords em inglês\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza a lematização dos tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"This is an example of text that will be cleaned, tokenized, and lemmatized!\"\n",
    "tokens = text_cleaning_l(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4768b3",
   "metadata": {},
   "source": [
    "## Putting all together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaf2161",
   "metadata": {},
   "source": [
    "## Logistic Regression Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e0e8fd",
   "metadata": {},
   "source": [
    "## Logistic Regression Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3433bbe",
   "metadata": {},
   "source": [
    "## Logistic Regression Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec3ce9",
   "metadata": {},
   "source": [
    "## Logistic Regression Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed553de",
   "metadata": {},
   "source": [
    "# Week 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2daf51",
   "metadata": {},
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b1435",
   "metadata": {},
   "source": [
    "# Week 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
