{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c72e402",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Classification and Vector Spaces\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Classification and Vector Spaces da DeeplearninigAI. O notebook é composto majoritariamente de material original, mas também possui material da **Deep Learning AI**, como figuras e algumas explicações \"Readings\" das Lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2930820e",
   "metadata": {},
   "source": [
    "# Week 1 - Sentiment Analysis With Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770aa7d1",
   "metadata": {},
   "source": [
    "## Objetivos de aprendizagem\n",
    "- Sentiment analysis\n",
    "- Logistic regression\n",
    "- Data pre-processing\n",
    "- Calculating word frequencies\n",
    "- Feature extraction\n",
    "- Vocabulary creation\n",
    "- Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6efeeb",
   "metadata": {},
   "source": [
    "## Supervised ML and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017f1f7",
   "metadata": {},
   "source": [
    "Os algoritmos de **Machine Learning (ML)** supervisionados são um tipo de algoritmo que aprende a partir de dados rotulados, ou seja, dados que já possuem um rótulo ou uma classificação pré-definida. Esses algoritmos usam esses dados rotulados para aprender a fazer previsões ou classificações em novos dados.\n",
    "\n",
    "Na área de Processamento de Linguagem Natural (NLP), os algoritmos de ML supervisionados são usados para uma variedade de tarefas, como classificação de sentimentos, identificação de entidades nomeadas, análise de tópicos, tradução automática, entre outras.\n",
    "\n",
    "Um exemplo de como esses algoritmos são usados em NLP é na classificação de sentimentos em textos. Nesse caso, um modelo de ML supervisionado seria **treinado em um conjunto de dados rotulados** que contém textos e suas respectivas classificações de sentimento (por exemplo, positivo, negativo ou neutro). O algoritmo usaria esses dados para **aprender a reconhecer padrões nos textos** e, em seguida, aplicaria esses padrões para **classificar o sentimento em novos textos**.\n",
    "\n",
    "Outro exemplo é na identificação de **entidades nomeadas (NER)**, que é uma tarefa que envolve a identificação de nomes de pessoas, locais, organizações e outras entidades em um texto. Nesse caso, um modelo de ML supervisionado seria treinado em um conjunto de dados rotulados que contém textos e suas respectivas entidades nomeadas. O algoritmo usaria esses dados para aprender a reconhecer os padrões de palavras e contextos que indicam a presença de uma entidade nomeada em um texto, e, em seguida, aplicaria esses padrões para identificar entidades em novos textos.\n",
    "\n",
    "Em resumo, os algoritmos de ML supervisionados são uma técnica poderosa para resolver problemas em NLP, permitindo que modelos aprendam com dados rotulados e possam fazer previsões ou classificações em novos dados com base no que foi aprendido."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672944a6",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/supervised_ml.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221f7e0",
   "metadata": {},
   "source": [
    "Um pipeline básico para a _task_ de análise de sentimento (classificação) geralmente envolve as etapas de:\n",
    "- **Pré-processamento de texto**: Esta etapa envolve a limpeza e preparação dos dados. O texto é normalmente convertido em minúsculas, removidos caracteres especiais, removidos números e pontuações, e realizada a tokenização do texto para obter as palavras individuais.\n",
    "- **Criação de features**: Nesta etapa, são criadas as features que serão usadas pelo modelo de classificação. As features podem incluir a contagem de palavras, a frequência de palavras, o tipo de palavras, o uso de negação, entre outras.\n",
    "- **Treinamento do modelo**: O modelo de classificação é treinado em um conjunto de dados rotulados que contêm exemplos de texto e suas respectivas classificações de sentimento. Existem vários algoritmos de aprendizado de máquina que podem ser usados para treinar um modelo, como Árvores de Decisão, Naive Bayes, Regressão Logística, SVM, Redes Neurais, etc.\n",
    "- **Avaliação do modelo**: Após o treinamento, o modelo é avaliado em um conjunto de dados de teste para verificar sua precisão. É comum dividir o conjunto de dados em conjunto de treinamento, validação e teste.\n",
    "- **Implantação**: Finalmente, o modelo treinado é usado para classificar o sentimento de novos textos. Novos textos passam pelas mesmas etapas de pré-processamento e criação de features, e o modelo treinado é usado para prever a classificação de sentimento do texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5699891c",
   "metadata": {},
   "source": [
    "## Vocabulary and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5ee47",
   "metadata": {},
   "source": [
    "Para representar textos de forma numérica, primeiro precisamos construir um vocabulário, com eles poderemos _encodar_ qualquer texto como um array de números. Um Vocabulário é uma lista com palavras únicas, não repetidas.\n",
    "\n",
    "Uma forma simples de extrair features do texto é usando o vocabulário, verificando cada palavra do vocabulário que aparece no texto. Caso as palavras no texto que estamos extraindo a feature tenham apareca no vocabulário, atribuímos o valor 1 para ela, e zero para as palavras que do vocabulário que não aparecem no texto. Assim, estamos representando o texto usando [**one-hot encoding**](https://k3ybladewielder.medium.com/introdu%C3%A7%C3%A3o-%C3%A0-nlp-4d7d98b9a36a) ou representação esparsa. Mas esse método pode ser problemático porquê o número de features é igual ao número de palavras no vocabulário e a grande maioria das features serão zero, aumentando excessivamente o tempo de treino e predição dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb08d346",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/vocabulary_and_feature_extraction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2edbfc",
   "metadata": {},
   "source": [
    "Existem diversas técnicas para representar textos como vetores, sendo as mais comuns a Bag of Words (BoW) e a Representação Distribuída de Palavras (Word Embeddings). Vou explicar brevemente cada uma delas:\n",
    "- **Bag of Words (BoW)**: Nessa técnica, o texto é representado como um vetor contendo a contagem de ocorrências de cada palavra presente no texto. Cada palavra é considerada como uma dimensão do vetor. Dessa forma, quanto mais vezes uma palavra aparecer no texto, maior será o valor correspondente na dimensão correspondente no vetor. Por exemplo, se um texto contém as palavras \"gato\", \"cão\" e \"casa\", a representação BoW seria um vetor com três dimensões, com valores correspondentes à contagem de ocorrências de cada palavra no texto.\n",
    "- **Word Embeddings**: Essa técnica é baseada em modelos de linguagem neural, que mapeiam cada palavra em um espaço vetorial de alta dimensão, onde palavras semelhantes têm representações próximas. A ideia é que cada palavra seja representada por um vetor de números reais que captura seu significado semântico. Esses vetores podem ser aprendidos a partir de grandes quantidades de textos usando técnicas de aprendizado de máquina, como Word2Vec, GloVe ou FastText. Os vetores resultantes podem ser usados para representar cada palavra em um texto como um vetor numérico. A representação distribuída de palavras pode capturar relações semânticas entre palavras, como sinonímia e antonímia, e pode ser usada para tarefas mais complexas, como análise de sentimento ou classificação de texto.\n",
    "\n",
    "Ambas as técnicas são amplamente utilizadas em NLP, dependendo do objetivo e do contexto da tarefa em questão. A escolha da técnica de representação de texto pode influenciar significativamente o desempenho do modelo de aprendizado de máquina, e é importante escolher a técnica mais adequada para a tarefa específica.\n",
    "\n",
    "Tanto a técnica de Bag of Words (BoW) quanto a Word Embeddings têm seus **pontos fortes e fracos**, e a escolha de qual usar depende do contexto e do objetivo da tarefa em questão.\n",
    "\n",
    "A representação **BoW** pode ser uma escolha adequada para **tarefas simples de classificação de texto**, como classificação de spam ou análise de sentimento, onde a presença ou ausência de palavras específicas pode ser um indicador importante para a classificação. Além disso, a representação **BoW é computacionalmente eficiente e fácil de interpretar**, o que pode ser uma vantagem para problemas onde a transparência do modelo é importante. No entanto, a representação **BoW não leva em consideração a ordem das palavras no texto, o que pode limitar sua capacidade de capturar nuances semânticas.**\n",
    "\n",
    "Já Word Embedding é mais adequada para **tarefas que envolvem análise semântica**, como tradução automática, classificação de tópicos ou análise de sentimento baseada em frases complexas. A representação Word Embedding **leva em consideração a ordem e o contexto das palavras**, e **pode capturar a similaridade semântica entre palavras que não aparecem juntas com frequência**. Além disso, a representação distribuída de palavras **pode ser usada para inicializar redes neurais em tarefas de aprendizado profundo**, melhorando o desempenho do modelo. No entanto, Word Embedding **pode ser computacionalmente intensiva e requer grandes quantidades de dados de treinamento para obter bons resultados**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062d552",
   "metadata": {},
   "source": [
    "## Feature Extraction with Frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b3700",
   "metadata": {},
   "source": [
    "Numa task de classificação de sentimentos, podemos identificar as palavras positivas e negativas a partir da frequencia de ocorrencia em que elas ocorrem nos textos positivos e negativos. Usando essa contagem, podemos extrair features e usá-las no modelo de classificação, como a **regressão logística**. \n",
    "\n",
    "<img src=\"./imgs/tweet_corpus.png\">\n",
    "\n",
    "A partir da contagem da frequencia das palavras em cada classe, chegamos a essa tabela. Na prática, essa tabela será um dicionário que mapeia a classe da palavra e sua frequência de ocorrência.\n",
    "\n",
    "<img src=\"./imgs/word_freq_tweets.png\">\n",
    "\n",
    "Podemos representar essa tabela de frequencia com um array com 3 features, aumentando a velocidade na implementação, porquê em vez de termos v features, teremos apenas 3 para que o modelo aprenda. \n",
    "\n",
    "Aqui, a primeira feature é um bias, depois o somatório das palavras da label positiva e o somatório das palavras da label negativa. Assim, teremos o novo vetor com 3 features.\n",
    "\n",
    "<img src=\"./imgs/vector_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e660575",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/feature_extraction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfe62eb",
   "metadata": {},
   "source": [
    "Essa abordagem apesar de ser bem simples é genial, e tem as seguintes vantagens:\n",
    "\n",
    "1. **Simplicidade e Interpretabilidade:**\n",
    "   * A técnica é relativamente simples de implementar e entender, mesmo para aqueles sem profundo conhecimento em machine learning.\n",
    "   * A utilização de frequências de palavras como características facilita a interpretação dos resultados, pois permite identificar quais palavras positivas e negativas contribuem mais para a classificação de um tweet.\n",
    "\n",
    "2. **Eficiência:**\n",
    "   * A extração de frequências de palavras é computacionalmente eficiente, especialmente quando comparada a métodos mais complexos de representação textual, como modelos neurais.\n",
    "   * Isso torna a abordagem adequada para datasets grandes e para implementações em tempo real.\n",
    "\n",
    "3. **Robustez:**\n",
    "   * A abordagem é robusta a ruídos e faltas de dados, pois se baseia na contagem de palavras, que é menos sensível a pequenas alterações no texto do que outros métodos.\n",
    "   * Isso a torna útil para lidar com tweets curtos ou com erros ortográficos.\n",
    "\n",
    "4. **Flexibilidade:**\n",
    "   * A técnica pode ser facilmente adaptada para outras tarefas de classificação binária de texto, como detecção de spam ou análise de opiniões.\n",
    "   * Basta ajustar o conjunto de palavras positivas e negativas de acordo com o contexto da tarefa.\n",
    "\n",
    "5. **Desempenho:**\n",
    "   * Apesar da simplicidade, a modelagem com frequências de palavras pode alcançar resultados competitivos em comparação com métodos mais complexos, especialmente para datasets com boa distribuição de classes.\n",
    "\n",
    "Mas ela também possui suas limitações, como:\n",
    "1. **Falta de Consideração da Ordem das Palavras:** A abordagem não leva em conta a ordem das palavras no tweet, o que pode ser importante para capturar o sentimento.\n",
    "2. **Sensibilidade a Sinônimos e Sarcasmo:** A técnica pode não capturar bem o sentimento de tweets que utilizam sinônimos para expressar sentimentos positivos ou negativos, ou que empregam sarcasmo.\n",
    "3. **Desempenho Inferior em Textos Complexos:** Para textos longos e complexos, a simples contagem de palavras pode não ser suficiente para capturar nuances do sentimento.\n",
    "\n",
    "Ainda assim, é um ótimo baseline, e caso seja necessário, podemos escalar para abordagens mais complexas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7200ec47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:24.217577Z",
     "start_time": "2024-04-13T12:37:23.546326Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Exemplo de tweets e seus respectivos sentimentos\n",
    "tweets = [\n",
    "    (\"Eu amo o meu cachorro\", \"positivo\"),\n",
    "    (\"Eu odeio acordar cedo\", \"negativo\"),\n",
    "    (\"A comida deste restaurante é incrível\", \"positivo\"),\n",
    "    (\"Estou cansado de estudar\", \"negativo\"),\n",
    "    (\"Que dia lindo para passear no parque\", \"positivo\"),\n",
    "    (\"O filme que vi ontem foi excelente\", \"positivo\"),\n",
    "    (\"Não vejo a hora de encontrar meus amigos\", \"positivo\"),\n",
    "    (\"Estou muito feliz com os resultados\", \"positivo\"),\n",
    "    (\"Que tristeza ver essa notícia\", \"negativo\"),\n",
    "    (\"Estou decepcionado com o serviço dessa empresa\", \"negativo\"),\n",
    "    (\"A chuva estragou meu dia\", \"negativo\"),\n",
    "    (\"Fiquei surpreso com o presente que recebi\", \"positivo\"),\n",
    "    (\"Esse livro é incrível, não consigo parar de ler\", \"positivo\"),\n",
    "    (\"Estou preocupado com o futuro do país\", \"negativo\"),\n",
    "    (\"Adorei o novo restaurante que experimentei\", \"positivo\"),\n",
    "    (\"O trânsito está terrível hoje\", \"negativo\"),\n",
    "    (\"Essa música me faz sentir feliz\", \"positivo\"),\n",
    "    (\"Perdi o ônibus e vou me atrasar para o trabalho\", \"negativo\"),\n",
    "    (\"Estou ansioso para o feriado chegar\", \"positivo\"),\n",
    "    (\"Que vergonha, esqueci meu aniversário de casamento\", \"negativo\"),\n",
    "    (\"Fui promovido no trabalho, estou radiante\", \"positivo\"),\n",
    "    (\"Não suporto essa pessoa, ela é muito arrogante\", \"negativo\"),\n",
    "    (\"O jantar que preparei ficou delicioso\", \"positivo\"),\n",
    "    (\"Meu time perdeu o jogo, estou arrasado\", \"negativo\"),\n",
    "    (\"Que saudade de casa\", \"negativo\"),\n",
    "    (\"Estou animado para o final de semana\", \"positivo\"),\n",
    "    (\"Esse filme é terrível, não recomendo\", \"negativo\"),\n",
    "    (\"A festa de aniversário foi um sucesso\", \"positivo\"),\n",
    "    (\"Estou cansado de tanto trabalhar\", \"negativo\"),\n",
    "    (\"Que alívio, finalmente terminei meu projeto\", \"positivo\"),\n",
    "    (\"Esse lugar é incrível, preciso voltar mais vezes\", \"positivo\"),\n",
    "    (\"Perdi meu voo e agora estou preso no aeroporto\", \"negativo\"),\n",
    "    (\"Estou feliz por ter encontrado um novo emprego\", \"positivo\"),\n",
    "    (\"Não aguento mais essa situação\", \"negativo\"),\n",
    "    (\"O passeio de barco foi maravilhoso\", \"positivo\"),\n",
    "    (\"Estou preocupado com a saúde dos meus pais\", \"negativo\"),\n",
    "    (\"Adorei a nova série que comecei a assistir\", \"positivo\"),\n",
    "    (\"Não vejo a hora de começar minhas férias\", \"positivo\"),\n",
    "    (\"Estou chateado com o cancelamento do evento\", \"negativo\"),\n",
    "    (\"Ganhei um prêmio, estou emocionado\", \"positivo\"),\n",
    "    (\"Esse restaurante não vale o preço que cobram\", \"negativo\"),\n",
    "    (\"Estou apaixonado por essa música\", \"positivo\"),\n",
    "    (\"Não gostei do atendimento dessa loja\", \"negativo\"),\n",
    "    (\"Meu cachorro está doente, estou preocupado\", \"negativo\"),\n",
    "    (\"Adorei o presente que ganhei de aniversário\", \"positivo\"),\n",
    "    (\"Estou cansado de tanto estudar para as provas\", \"negativo\"),\n",
    "    (\"Que alegria, hoje é meu aniversário\", \"positivo\"),\n",
    "    (\"Não vejo a hora de conhecer meu sobrinho que está a caminho\", \"positivo\"),\n",
    "    (\"Estou frustrado com o atraso no projeto\", \"negativo\"),\n",
    "    (\"Que bom ver meus amigos depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Não aguento mais essa dor de cabeça\", \"negativo\"),\n",
    "    (\"O pôr do sol na praia é magnífico\", \"positivo\"),\n",
    "    (\"Estou triste com a perda do meu animal de estimação\", \"negativo\"),\n",
    "    (\"Que maravilha, vou me formar na faculdade\", \"positivo\"),\n",
    "    (\"Estou desanimado com o futuro da economia\", \"negativo\"),\n",
    "    (\"O concerto de música clássica foi incrível\", \"positivo\"),\n",
    "    (\"Não suporto mais essa discussão\", \"negativo\"),\n",
    "    (\"Estou feliz por ter começado a fazer exercícios\", \"positivo\"),\n",
    "    (\"O tráfego está caótico hoje\", \"negativo\"),\n",
    "    (\"Estou radiante com a notícia do meu amigo\", \"positivo\"),\n",
    "    (\"Esse filme me deixou emocionado\", \"positivo\"),\n",
    "    (\"Estou decepcionado com o resultado da eleição\", \"negativo\"),\n",
    "    (\"Que bom encontrar você aqui\", \"positivo\"),\n",
    "    (\"Não gostei do sabor desse prato\", \"negativo\"),\n",
    "    (\"Estou contente por ter conhecido novas pessoas\", \"positivo\"),\n",
    "    (\"Estou estressado com o excesso de trabalho\", \"negativo\"),\n",
    "    (\"Essa viagem foi incrível, quero repetir\", \"positivo\"),\n",
    "    (\"Não aguento mais a rotina cansativa\", \"negativo\"),\n",
    "    (\"Estou preocupado com a segurança da minha cidade\", \"negativo\"),\n",
    "    (\"Que felicidade, consegui uma promoção no trabalho\", \"positivo\"),\n",
    "    (\"Não gosto desse clima frio\", \"negativo\"),\n",
    "    (\"Estou maravilhado com a beleza desse lugar\", \"positivo\"),\n",
    "    (\"Estou triste com a partida do meu amigo\", \"negativo\"),\n",
    "    (\"Adorei o show que fui ontem à noite\", \"positivo\"),\n",
    "    (\"Estou irritado com o comportamento dessa pessoa\", \"negativo\"),\n",
    "    (\"Que alegria, vou passar as férias na praia\", \"positivo\"),\n",
    "    (\"Não consigo parar de pensar naquela situação\", \"negativo\"),\n",
    "    (\"Estou ansioso para o lançamento do novo filme\", \"positivo\"),\n",
    "    (\"Não gostei do resultado do jogo\", \"negativo\"),\n",
    "    (\"Estou encantado com a cultura desse país\", \"positivo\"),\n",
    "    (\"Estou cansado de tantas brigas\", \"negativo\"),\n",
    "    (\"Que bom estar de volta em casa\", \"positivo\"),\n",
    "    (\"Não suporto mais esse barulho\", \"negativo\"),\n",
    "    (\"Estou empolgado para a festa de aniversário\", \"positivo\"),\n",
    "    (\"Estou preocupado com o meio ambiente\", \"negativo\"),\n",
    "    (\"Adorei o presente que ganhei de Natal\", \"positivo\"),\n",
    "    (\"Estou frustrado com a demora no atendimento\", \"negativo\"),\n",
    "    (\"Que alívio, consegui resolver o problema\", \"positivo\"),\n",
    "    (\"Não vejo a hora de voltar para casa\", \"negativo\"),\n",
    "    (\"Estou emocionado com o nascimento do meu sobrinho\", \"positivo\"),\n",
    "    (\"Esse lugar é incrível, não vejo a hora de voltar\", \"positivo\"),\n",
    "    (\"Estou triste por não ter conseguido o emprego\", \"negativo\"),\n",
    "    (\"Estou radiante com a notícia do casamento do meu amigo\", \"positivo\"),\n",
    "    (\"Não aguento mais essa situação difícil\", \"negativo\"),\n",
    "    (\"Que bom te ver depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Estou preocupado com a saúde da minha família\", \"negativo\"),\n",
    "    (\"Adorei a comida desse restaurante\", \"positivo\"),\n",
    "    (\"Estou irritado com o barulho constante\", \"negativo\"),\n",
    "    (\"Estou feliz por ter terminado meus estudos\", \"positivo\"),\n",
    "    (\"Não gostei do final desse livro\", \"negativo\"),\n",
    "    (\"Estou contente por ter conseguido resolver o problema\", \"positivo\"),\n",
    "    (\"Estou estressado com os prazos do trabalho\", \"negativo\"),\n",
    "    (\"Que alegria, vou passar o final de semana na praia\", \"positivo\"),\n",
    "    (\"Não vejo a hora de viajar para o exterior\", \"positivo\"),\n",
    "    (\"Estou triste com a notícia da doença de um amigo\", \"negativo\"),\n",
    "    (\"Adorei o filme que assisti ontem à noite\", \"positivo\"),\n",
    "    (\"Estou frustrado com a falta de oportunidades\", \"negativo\"),\n",
    "    (\"Que alívio, consegui encontrar minhas chaves perdidas\", \"positivo\"),\n",
    "    (\"Não suporto mais essa situação complicada\", \"negativo\"),\n",
    "    (\"Estou emocionado com o nascimento do meu sobrinho\", \"positivo\"),\n",
    "    (\"Estou irritado com o comportamento dessa pessoa\", \"negativo\"),\n",
    "    (\"Estou feliz por ter encontrado um novo emprego\", \"positivo\"),\n",
    "    (\"Não gostei do atendimento dessa loja\", \"negativo\"),\n",
    "    (\"Que bom te ver depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Estou chateado com o cancelamento do evento\", \"negativo\"),\n",
    "    (\"Estou empolgado para a festa de aniversário\", \"positivo\"),\n",
    "    (\"Não aguento mais essa dor de cabeça\", \"negativo\"),\n",
    "    (\"Estou ansioso para o lançamento do novo filme\", \"positivo\"),\n",
    "    (\"Estou preocupado com o meio ambiente\", \"negativo\"),\n",
    "    (\"Estou maravilhado com a beleza desse lugar\", \"positivo\"),\n",
    "    (\"Não consigo parar de pensar naquela situação\", \"negativo\"),\n",
    "    (\"Estou radiante com a notícia do casamento do meu amigo\", \"positivo\"),\n",
    "    (\"Não gostei do final desse livro\", \"negativo\"),\n",
    "    (\"Estou feliz por ter terminado meus estudos\", \"positivo\"),\n",
    "    (\"Estou irritado com o barulho constante\", \"negativo\"),\n",
    "    (\"Adorei a comida desse restaurante\", \"positivo\"),\n",
    "    (\"Estou preocupado com a saúde da minha família\", \"negativo\"),\n",
    "    (\"Que bom te ver depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Estou cansado de tantas brigas\", \"negativo\"),\n",
    "    (\"Estou encantado com a cultura desse país\", \"positivo\"),\n",
    "    (\"Não gostei do resultado do jogo\", \"negativo\"),\n",
    "    (\"Estou ansioso para o final de semana\", \"positivo\"),\n",
    "    (\"Estou empolgado para a festa de aniversário\", \"positivo\"),\n",
    "    (\"Que bom encontrar você aqui\", \"positivo\"),\n",
    "    (\"Estou decepcionado com o resultado da eleição\", \"negativo\"),\n",
    "    (\"Esse filme me deixou emocionado\", \"positivo\"),\n",
    "    (\"Estou frustrado com o atraso no projeto\", \"negativo\"),\n",
    "    (\"Não vejo a hora de conhecer meu sobrinho que está a caminho\", \"positivo\"),\n",
    "    (\"Estou feliz por ter encontrado um novo emprego\", \"positivo\"),\n",
    "    (\"Estou radiante com a notícia do meu amigo\", \"positivo\"),\n",
    "    (\"O tráfego está caótico hoje\", \"negativo\"),\n",
    "    (\"Estou feliz por ter começado a fazer exercícios\", \"positivo\"),\n",
    "    (\"Não suporto mais essa discussão\", \"negativo\"),\n",
    "    (\"O concerto de música clássica foi incrível\", \"positivo\"),\n",
    "    (\"Estou desanimado com o futuro da economia\", \"negativo\"),\n",
    "    (\"Que maravilha, vou me formar na faculdade\", \"positivo\"),\n",
    "    (\"Estou triste com a perda do meu animal de estimação\", \"negativo\"),\n",
    "    (\"O pôr do sol na praia é magnífico\", \"positivo\"),\n",
    "    (\"Estou irritado com o comportamento dessa pessoa\", \"negativo\"),\n",
    "    (\"Adorei o show que fui ontem à noite\", \"positivo\"),\n",
    "    (\"Estou feliz por ter encontrado um novo emprego\", \"positivo\"),\n",
    "    (\"Que bom te ver depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Estou cansado de tantas brigas\", \"negativo\"),\n",
    "    (\"Estou radiante com a notícia do meu amigo\", \"positivo\"),\n",
    "    (\"Não gostei do resultado do jogo\", \"negativo\"),\n",
    "    (\"Que alegria, vou passar o final de semana na praia\", \"positivo\"),\n",
    "    (\"Estou irritado com o barulho constante\", \"negativo\"),\n",
    "    (\"Estou feliz por ter terminado meus estudos\", \"positivo\"),\n",
    "    (\"Estou preocupado com a saúde da minha família\", \"negativo\"),\n",
    "    (\"Estou encantado com a cultura desse país\", \"positivo\"),\n",
    "    (\"Não consigo parar de pensar naquela situação\", \"negativo\"),\n",
    "    (\"Estou maravilhado com a beleza desse lugar\", \"positivo\"),\n",
    "    (\"Não suporto mais essa situação complicada\", \"negativo\"),\n",
    "    (\"Estou emocionado com o nascimento do meu sobrinho\", \"positivo\"),\n",
    "    (\"Estou irritado com o comportamento dessa pessoa\", \"negativo\"),\n",
    "    (\"Estou feliz por ter encontrado um novo emprego\", \"positivo\"),\n",
    "    (\"Não gostei do atendimento dessa loja\", \"negativo\"),\n",
    "    (\"Que bom te ver depois de tanto tempo\", \"positivo\"),\n",
    "    (\"Estou chateado com o cancelamento do evento\", \"negativo\"),\n",
    "    (\"Estou empolgado para a festa de aniversário\", \"positivo\"),\n",
    "    (\"Não aguento mais essa dor de cabeça\", \"negativo\"),\n",
    "    (\"Estou ansioso para o lançamento do novo filme\", \"positivo\"),\n",
    "]\n",
    "\n",
    "# Cria o DataFrame\n",
    "data = pd.DataFrame(tweets, columns=['tweet', 'label'])\n",
    "data[\"label\"] = data[\"label\"].map({\"positivo\": 0, \"negativo\": 1})\n",
    "df = data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee6594b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:24.238176Z",
     "start_time": "2024-04-13T12:37:24.220001Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu amo o meu cachorro</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu odeio acordar cedo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A comida deste restaurante é incrível</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estou cansado de estudar</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Que dia lindo para passear no parque</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   tweet  label\n",
       "0                  Eu amo o meu cachorro      0\n",
       "1                  Eu odeio acordar cedo      1\n",
       "2  A comida deste restaurante é incrível      0\n",
       "3               Estou cansado de estudar      1\n",
       "4   Que dia lindo para passear no parque      0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e63aac9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:25.251601Z",
     "start_time": "2024-04-13T12:37:24.240796Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>bias</th>\n",
       "      <th>pos_freq</th>\n",
       "      <th>neg_freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu amo o meu cachorro</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu odeio acordar cedo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A comida deste restaurante é incrível</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estou cansado de estudar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Que dia lindo para passear no parque</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O filme que vi ontem foi excelente</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Não vejo a hora de encontrar meus amigos</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Estou muito feliz com os resultados</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Que tristeza ver essa notícia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Estou decepcionado com o serviço dessa empresa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A chuva estragou meu dia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fiquei surpreso com o presente que recebi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Esse livro é incrível, não consigo parar de ler</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Estou preocupado com o futuro do país</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Adorei o novo restaurante que experimentei</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>O trânsito está terrível hoje</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Essa música me faz sentir feliz</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Perdi o ônibus e vou me atrasar para o trabalho</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Estou ansioso para o feriado chegar</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Que vergonha, esqueci meu aniversário de casam...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  label  bias  pos_freq  \\\n",
       "0                               Eu amo o meu cachorro      0     1        37   \n",
       "1                               Eu odeio acordar cedo      1     1         0   \n",
       "2               A comida deste restaurante é incrível      0     1        51   \n",
       "3                            Estou cansado de estudar      1     1        30   \n",
       "4                Que dia lindo para passear no parque      0     1        31   \n",
       "5                  O filme que vi ontem foi excelente      0     1        53   \n",
       "6            Não vejo a hora de encontrar meus amigos      0     1        82   \n",
       "7                 Estou muito feliz com os resultados      0     1        33   \n",
       "8                       Que tristeza ver essa notícia      1     1        24   \n",
       "9      Estou decepcionado com o serviço dessa empresa      1     1        40   \n",
       "10                           A chuva estragou meu dia      1     1        43   \n",
       "11          Fiquei surpreso com o presente que recebi      0     1        55   \n",
       "12    Esse livro é incrível, não consigo parar de ler      0     1        64   \n",
       "13              Estou preocupado com o futuro do país      1     1        58   \n",
       "14         Adorei o novo restaurante que experimentei      0     1        48   \n",
       "15                      O trânsito está terrível hoje      1     1        25   \n",
       "16                    Essa música me faz sentir feliz      0     1        24   \n",
       "17    Perdi o ônibus e vou me atrasar para o trabalho      1     1        68   \n",
       "18                Estou ansioso para o feriado chegar      0     1        43   \n",
       "19  Que vergonha, esqueci meu aniversário de casam...      1     1        78   \n",
       "\n",
       "    neg_freq  \n",
       "0         33  \n",
       "1          3  \n",
       "2         16  \n",
       "3         31  \n",
       "4          9  \n",
       "5         28  \n",
       "6         39  \n",
       "7         38  \n",
       "8         16  \n",
       "9         75  \n",
       "10        22  \n",
       "11        60  \n",
       "12        39  \n",
       "13        95  \n",
       "14        28  \n",
       "15        36  \n",
       "16        12  \n",
       "17        64  \n",
       "18        32  \n",
       "19        36  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Exemplo de dataframe de tweets\n",
    "# data = {'tweet': ['Este é um tweet não tóxico', \n",
    "#                   'Este é um tweet tóxico com palavras negativas',\n",
    "#                   'Outro tweet não tóxico com palavras positivas'],\n",
    "#         'label': [0, 1, 0]}\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "positive_words = df[df[\"label\"] == 0][\"tweet\"].to_list() # Lista de palavras positivas\n",
    "positive_words = word_tokenize(' '.join(positive_words))\n",
    "\n",
    "negative_words = df[df[\"label\"] == 1][\"tweet\"].to_list()# Lista de palavras negativas\n",
    "negative_words = word_tokenize(' '.join(negative_words))\n",
    "\n",
    "# Função para mapear a frequência das palavras do subconjunto de palavras positivas (label = 0)\n",
    "def positive_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())  # Tokenização e conversão para minúsculas\n",
    "    freq = FreqDist(tokens)\n",
    "    positive_freq = sum([freq[word] for word in positive_words])\n",
    "    return positive_freq\n",
    "\n",
    "# Função para mapear a frequência das palavras do subconjunto de palavras negativas (label = 1)\n",
    "def negative_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())  # Tokenização e conversão para minúsculas\n",
    "    freq = FreqDist(tokens)\n",
    "    negative_freq = sum([freq[word] for word in negative_words])\n",
    "    return negative_freq\n",
    "\n",
    "# Aplicar as funções aos tweets e adicionar os resultados ao dataframe\n",
    "df['bias'] = 1  # Coluna de bias com valor 1\n",
    "df['pos_freq'] = df['tweet'].apply(positive_words_frequency)\n",
    "df['neg_freq'] = df['tweet'].apply(negative_words_frequency)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2352020b",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84643a",
   "metadata": {},
   "source": [
    "O pré-processamento geralmente envolve as etapas de limpeza e preparação dos dados. O texto é normalmente convertido em minúsculas, removidos caracteres especiais, removidos números e pontuações, e realizada a tokenização do texto para obter as palavras individuais.\n",
    "\n",
    "Exemplo em python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb873564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:26.173403Z",
     "start_time": "2024-04-13T12:37:25.253769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exemplo', 'texto', 'limpo', 'tokenizado']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/joaoag/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def text_cleaning(text):\n",
    "    # Extrai as stopwords em português\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"Este é um exemplo de texto que será limpo e tokenizado!\"\n",
    "tokens = text_cleaning(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d819d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:26.349789Z",
     "start_time": "2024-04-13T12:37:26.175409Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>bias</th>\n",
       "      <th>pos_freq</th>\n",
       "      <th>neg_freq</th>\n",
       "      <th>ttweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Eu amo o meu cachorro</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>amo cachorro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eu odeio acordar cedo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>odeio acordar cedo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A comida deste restaurante é incrível</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>comida deste restaurante incrível</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estou cansado de estudar</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>cansado estudar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Que dia lindo para passear no parque</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>dia lindo passear parque</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>O filme que vi ontem foi excelente</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>filme vi ontem excelente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Não vejo a hora de encontrar meus amigos</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>vejo hora encontrar amigos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Estou muito feliz com os resultados</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>feliz resultados</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Que tristeza ver essa notícia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>tristeza ver notícia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Estou decepcionado com o serviço dessa empresa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>decepcionado serviço dessa empresa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A chuva estragou meu dia</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>chuva estragou dia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Fiquei surpreso com o presente que recebi</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>fiquei surpreso presente recebi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Esse livro é incrível, não consigo parar de ler</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>livro incrível consigo parar ler</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Estou preocupado com o futuro do país</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>preocupado futuro país</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Adorei o novo restaurante que experimentei</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>adorei novo restaurante experimentei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>O trânsito está terrível hoje</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>trânsito terrível hoje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Essa música me faz sentir feliz</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>música faz sentir feliz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Perdi o ônibus e vou me atrasar para o trabalho</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>perdi ônibus vou atrasar trabalho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Estou ansioso para o feriado chegar</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ansioso feriado chegar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Que vergonha, esqueci meu aniversário de casam...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>vergonha esqueci aniversário casamento</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  label  bias  pos_freq  \\\n",
       "0                               Eu amo o meu cachorro      0     1         2   \n",
       "1                               Eu odeio acordar cedo      1     1         0   \n",
       "2               A comida deste restaurante é incrível      0     1        15   \n",
       "3                            Estou cansado de estudar      1     1         0   \n",
       "4                Que dia lindo para passear no parque      0     1         4   \n",
       "5                  O filme que vi ontem foi excelente      0     1        13   \n",
       "6            Não vejo a hora de encontrar meus amigos      0     1        18   \n",
       "7                 Estou muito feliz com os resultados      0     1        13   \n",
       "8                       Que tristeza ver essa notícia      1     1        11   \n",
       "9      Estou decepcionado com o serviço dessa empresa      1     1         0   \n",
       "10                           A chuva estragou meu dia      1     1         1   \n",
       "11          Fiquei surpreso com o presente que recebi      0     1         6   \n",
       "12    Esse livro é incrível, não consigo parar de ler      0     1        11   \n",
       "13              Estou preocupado com o futuro do país      1     1         3   \n",
       "14         Adorei o novo restaurante que experimentei      0     1        23   \n",
       "15                      O trânsito está terrível hoje      1     1         1   \n",
       "16                    Essa música me faz sentir feliz      0     1        18   \n",
       "17    Perdi o ônibus e vou me atrasar para o trabalho      1     1         7   \n",
       "18                Estou ansioso para o feriado chegar      0     1         7   \n",
       "19  Que vergonha, esqueci meu aniversário de casam...      1     1         9   \n",
       "\n",
       "    neg_freq                                  ttweet  \n",
       "0          1                            amo cachorro  \n",
       "1          3                      odeio acordar cedo  \n",
       "2          1       comida deste restaurante incrível  \n",
       "3          8                         cansado estudar  \n",
       "4          1                dia lindo passear parque  \n",
       "5          1                filme vi ontem excelente  \n",
       "6          2              vejo hora encontrar amigos  \n",
       "7          0                        feliz resultados  \n",
       "8          4                    tristeza ver notícia  \n",
       "9         13      decepcionado serviço dessa empresa  \n",
       "10         3                      chuva estragou dia  \n",
       "11         0         fiquei surpreso presente recebi  \n",
       "12         8        livro incrível consigo parar ler  \n",
       "13        13                  preocupado futuro país  \n",
       "14         1    adorei novo restaurante experimentei  \n",
       "15         6                  trânsito terrível hoje  \n",
       "16         0                 música faz sentir feliz  \n",
       "17         8       perdi ônibus vou atrasar trabalho  \n",
       "18         0                  ansioso feriado chegar  \n",
       "19         4  vergonha esqueci aniversário casamento  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessando o texto\n",
    "df[\"ttweet\"] = df[\"tweet\"].apply(text_cleaning)\n",
    "df[\"ttweet\"] = df[\"ttweet\"].apply(lambda x: \" \".join(x)) # retornando a lista como string\n",
    "\n",
    "# novo conjunto de palavras\n",
    "positive_words = df[df[\"label\"] == 0][\"ttweet\"].to_list() # Lista de palavras positivas\n",
    "positive_words = word_tokenize(' '.join(positive_words))\n",
    "\n",
    "negative_words = df[df[\"label\"] == 1][\"ttweet\"].to_list()# Lista de palavras negativas\n",
    "negative_words = word_tokenize(' '.join(negative_words))\n",
    "\n",
    "# Função para mapear a frequência das palavras do subconjunto de palavras positivas (label = 0)\n",
    "def positive_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())  # Tokenização e conversão para minúsculas\n",
    "    freq = FreqDist(tokens)\n",
    "    positive_freq = sum([freq[word] for word in positive_words])\n",
    "    return positive_freq\n",
    "\n",
    "# Função para mapear a frequência das palavras do subconjunto de palavras negativas (label = 1)\n",
    "def negative_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())  # Tokenização e conversão para minúsculas\n",
    "    freq = FreqDist(tokens)\n",
    "    negative_freq = sum([freq[word] for word in negative_words])\n",
    "    return negative_freq\n",
    "\n",
    "# Aplicar as funções aos tweets e adicionar os resultados ao dataframe\n",
    "df['bias'] = 1  # Coluna de bias com valor 1\n",
    "df['pos_freq'] = df['ttweet'].apply(positive_words_frequency)\n",
    "df['neg_freq'] = df['ttweet'].apply(negative_words_frequency)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fbdded",
   "metadata": {},
   "source": [
    "Mas além disso também podemos fazer **Stemming** e **lematização**. Elas são técnicas de pré-processamento de texto com o objetivo de reduzir as palavras em sua forma base ou raiz, simplificando o processo de análise de texto.\n",
    "\n",
    "**Stemming** é um processo mais simples e rápido de normalização de palavras, que envolve a remoção de sufixos com o objetivo de transformar uma palavra em sua raiz, ou seja, em sua forma básica. Um exemplo de algoritmo de stemming é o Porter Stemmer, que é amplamente utilizado em NLP. O ponto positivo do stemming é sua simplicidade e velocidade de processamento, o que pode ser útil em projetos com grandes volumes de dados. No entanto, o stemming pode produzir algumas palavras raiz que não são facilmente reconhecidas, o que pode ser um problema em alguns casos.\n",
    "\n",
    "Já a **lematização** é um processo mais complexo de normalização de palavras, que envolve a análise do contexto da palavra para determinar sua forma básica. A lematização utiliza um dicionário de palavras ou um algoritmo para mapear a palavra para sua forma base. O ponto positivo da lematização é que ela produz palavras que são facilmente reconhecíveis, o que pode ser importante em projetos que exigem maior precisão na análise de texto. No entanto, a lematização é um processo mais lento e computacionalmente mais caro do que o stemming, o que pode ser um problema em projetos com grandes volumes de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6116403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:26.360195Z",
     "start_time": "2024-04-13T12:37:26.352257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exempl', 'text', 'limp', 'tokeniz', 'stemmed']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "\n",
    "def text_cleaning_s(text):\n",
    "    # Extrai as stopwords em português\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza o stemming dos tokens\n",
    "    stemmer = SnowballStemmer('portuguese')\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "text = \"Este é um exemplo de texto que será limpo, tokenizado e stemmed!\"\n",
    "tokens = text_cleaning_s(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9ea281",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:28.250660Z",
     "start_time": "2024-04-13T12:37:26.361873Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/joaoag/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/joaoag/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['este', 'é', 'um', 'exemplo', 'de', 'texto', 'que', 'será', 'limpo', 'tokenizado', 'e', 'stemmed']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "def text_cleaning_l(text):\n",
    "    # Extrai as stopwords em inglês\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza a lematização dos tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "# text = \"This is an example of text that will be cleaned, tokenized, and lemmatized!\"\n",
    "text = \"Este é um exemplo de texto que será limpo, tokenizado e stemmed!\"\n",
    "tokens = text_cleaning_l(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c464aaf",
   "metadata": {},
   "source": [
    "## Logistic Regression Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbde6b3",
   "metadata": {},
   "source": [
    "A **regressão logística** é frequentemente utilizada em análise de sentimento porque é um algoritmo de classificação simples, rápido e eficiente para problemas de classificação binária, ou seja, quando há apenas duas classes, como positivo e negativo. Além disso, a regressão logística é facilmente interpretável, o que significa que é possível entender como o modelo chegou a uma determinada previsão.\n",
    "\n",
    "Outra vantagem da regressão logística é que ela lida bem com problemas de dados desbalanceados, que é comum em análise de sentimento, onde muitas vezes há mais exemplos de uma classe do que outra. A regressão logística usa uma função sigmoide para calcular as probabilidades de cada classe, e essa função é bem adequada para casos de classes desbalanceadas.\n",
    "\n",
    "Outro motivo para a popularidade da regressão logística em análise de sentimento é que ela é relativamente fácil de implementar e ajustar. É possível utilizar diferentes técnicas de regularização, como a regularização L1 ou L2, para evitar overfitting e melhorar o desempenho do modelo.\n",
    "\n",
    "No entanto, é importante ressaltar que a regressão logística pode não ser adequada para problemas de classificação multiclasse, ou seja, quando há mais de duas classes, como em análise de tópicos. Nesses casos, outros algoritmos, como Árvores de Decisão, SVMs ou Redes Neurais, podem ser mais apropriados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24aa2a8",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/logistic_regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6143e66d",
   "metadata": {},
   "source": [
    "O modelo é treinado usando um conjunto de dados rotulados, onde cada texto é rotulado como tendo um sentimento positivo ou negativo. O objetivo do treinamento é ajustar os parâmetros do modelo para que ele possa fazer previsões precisas em novos dados.\n",
    "\n",
    "Durante o treinamento, o modelo utiliza a função logística para calcular a probabilidade de um texto ter um sentimento positivo ou negativo com base em seus vetores de características. A função logística produz um valor entre 0 e 1, que representa a probabilidade de um texto ter um sentimento positivo. Se a probabilidade for maior que 0,5, o modelo classifica o texto como tendo um sentimento positivo. Caso contrário, o texto é classificado como tendo um sentimento negativo.\n",
    "\n",
    "Após o treinamento, o modelo pode ser usado para fazer previsões em novos dados. Para cada novo texto, o modelo converte-o em um vetor de características e utiliza a função logística para calcular a probabilidade de ter um sentimento positivo ou negativo. Em seguida, classifica o texto como tendo um sentimento positivo ou negativo com base no valor calculado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5e201",
   "metadata": {},
   "source": [
    "Note que conforme $\\theta^T x^{(i)}$ se aproxima cada vez mais de $-\\infty$, o denominador da função sigmoidal fica cada vez maior e, como resultado, a sigmoidal se aproxima de $0$. Por outro lado, conforme $\\theta^T x^{(i)}$ se aproxima cada vez mais de $\\infty$, o denominador da função sigmoidal se aproxima de 1 e, como resultado, a sigmoidal também se aproxima de $1$.\n",
    "\n",
    "Dado um tweet, podemos transformá-lo em um vetor e passá-lo pela sua função sigmoidal para obter uma previsão da seguinte forma:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0467663",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/logistic_regression2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c619dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:28.259394Z",
     "start_time": "2024-04-13T12:37:28.253032Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_cleaning_l(text):\n",
    "    # Extrai as stopwords\n",
    "    stopwords_list = stopwords.words('portuguese')\n",
    "    \n",
    "    # Remove caracteres especiais\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove números e minimiza\n",
    "    text = ''.join(word for word in text if not word.isdigit()).lower()\n",
    "    \n",
    "    # Converte para minúsculo e tokeniza as palavras\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove as stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords_list]\n",
    "    \n",
    "    # Realiza a lematização dos tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return lemmatized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d113a1a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:28.266634Z",
     "start_time": "2024-04-13T12:37:28.261862Z"
    }
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Criação das frases aleatórias\n",
    "# np.random.seed(42)\n",
    "# frases = np.random.choice(['Esta é uma frase positiva.',\n",
    "#                            'Esta é uma frase negativa.'], size=100000)\n",
    "\n",
    "# # Criação das labels aleatórias\n",
    "# labels = np.random.choice(['positivo', 'negativo'], size=100000)\n",
    "\n",
    "# # Criação do DataFrame\n",
    "# df = pd.DataFrame({'frase': frases, 'label': labels})\n",
    "\n",
    "# # Visualização das primeiras linhas\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfa71ee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:28.278570Z",
     "start_time": "2024-04-13T12:37:28.272630Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53de06cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:29.279616Z",
     "start_time": "2024-04-13T12:37:28.281521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.groupby('label').agg({'tweet': 'count'}).reset_index(drop=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82b1d12b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:29.308718Z",
     "start_time": "2024-04-13T12:37:29.281698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classificação: 0.9807692307692307\n",
      "Precisão da classificação (Classe 0): 1.0\n",
      "Precisão da classificação (Classe 1): 0.9655172413793104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaoag/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/joaoag/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# divisão em treino e teste\n",
    "X = df[['pos_freq', 'neg_freq']]\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# criação do modelo de regressão logística e treinamento\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# avaliação do modelo nos dados de teste\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# avaliando o modelo\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "print(\"Acurácia da classificação:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precisão da classificação (Classe 0):\", precision_score(y_test, y_pred, pos_label=0))\n",
    "print(\"Precisão da classificação (Classe 1):\", precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fea5ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:29.423261Z",
     "start_time": "2024-04-13T12:37:29.310414Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classificação: 0.7692307692307693\n",
      "Precisão da classificação (Classe 0): 0.7142857142857143\n",
      "Precisão da classificação (Classe 1): 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pré-processamento das frases\n",
    "# df['frase_processada'] = df['frase'].apply(text_cleaning_l)\n",
    "df['ttweet'] = df['tweet'].apply(text_cleaning_l)\n",
    "\n",
    "# vetorização das frases\n",
    "vectorizer = CountVectorizer(tokenizer=lambda text: text, lowercase=False)\n",
    "X = vectorizer.fit_transform(df['ttweet'].apply(lambda tokens: ' '.join(tokens)))\n",
    "\n",
    "# divisão em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# criação do modelo de regressão logística e treinamento\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# avaliação do modelo nos dados de teste\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# avaliando o modelo\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "print(\"Acurácia da classificação:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precisão da classificação (Classe 0):\", precision_score(y_test, y_pred, pos_label=0))\n",
    "print(\"Precisão da classificação (Classe 1):\", precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9e15ac8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:29.457649Z",
     "start_time": "2024-04-13T12:37:29.426277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia da classificação: 0.7115384615384616\n",
      "Precisão da classificação (Classe 0): 0.6216216216216216\n",
      "Precisão da classificação (Classe 1): 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Mesmo exemplo mas com o TF-IDF como vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pré-processamento das frases\n",
    "# df['frase_processada'] = df['frase'].apply(text_cleaning_l)\n",
    "\n",
    "# vetorização das frases\n",
    "vectorizer = TfidfVectorizer(tokenizer=lambda text: text, lowercase=False)\n",
    "X = vectorizer.fit_transform(df['ttweet'].apply(lambda tokens: ' '.join(tokens)))\n",
    "\n",
    "# divisão em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.3, random_state=42)\n",
    "\n",
    "# criação do modelo de regressão logística e treinamento\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# avaliação do modelo nos dados de teste\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# avaliando o modelo\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "print(\"Acurácia da classificação:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Precisão da classificação (Classe 0):\", precision_score(y_test, y_pred, pos_label=0))\n",
    "print(\"Precisão da classificação (Classe 1):\", precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579bf0c",
   "metadata": {},
   "source": [
    "## Logistic Regression Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258badda",
   "metadata": {},
   "source": [
    "A função de custo da regressão logística é usada para avaliar quão bem um modelo de regressão logística está performando em relação aos dados observados. Ela é frequentemente utilizada em problemas de classificação binária, onde o objetivo é prever se uma observação pertence a uma das duas classes possíveis. Aqui está uma explicação de cada etapa da função de custo da regressão logística:\n",
    "\n",
    "1. **Hipótese da Regressão Logística:**\n",
    "   A primeira etapa é a formulação da hipótese do modelo de regressão logística. A hipótese é uma função que mapeia as entradas para uma probabilidade estimada de pertencer a uma das classes. A função logística (também conhecida como função sigmoide) é comumente utilizada como a função de hipótese na regressão logística e é dada pela seguinte equação:\n",
    "\n",
    "   $$ h_{\\theta}(x) = \\frac{1}{1 + e^{-\\theta^Tx}}$$\n",
    "\n",
    "   Onde:\n",
    "   - $ h_{\\theta}(x) $ é a estimativa da probabilidade de \\( x \\) pertencer à classe positiva,\n",
    "   - $ \\theta $ são os parâmetros do modelo,\n",
    "   - $ x $ é o vetor de entrada,\n",
    "   - $ e $ é o número de Euler (aproximadamente 2.71828).\n",
    "\n",
    "\n",
    "2. **Função de Custo Logístico:**\n",
    "   A função de custo é uma medida de quão bem a hipótese do modelo se ajusta aos dados observados. Para a regressão logística, a função de custo (também conhecida como função de perda ou função de erro) é definida usando a técnica de máxima verossimilhança. A função de custo logístico para um único exemplo de treinamento é dada pela seguinte equação:\n",
    "\n",
    "   $$ J(\\theta) = -y \\log(h_{\\theta}(x)) - (1 - y) \\log(1 - h_{\\theta}(x)) $$\n",
    "\n",
    "   Onde:\n",
    "   - $ J(\\theta) $ é a função de custo,\n",
    "   - $ y $ é a classe verdadeira do exemplo (0 ou 1),\n",
    "   - $ h_{\\theta}(x) $ é a estimativa da probabilidade da classe positiva dada pela hipótese.\n",
    "\n",
    "\n",
    "3. **Função de Custo Médio (ou Função de Custo Regularizada):**\n",
    "   Para avaliar o desempenho do modelo em todo o conjunto de dados, a função de custo médio é calculada. Isso é feito tirando a média dos custos individuais de todos os exemplos de treinamento. Além disso, uma penalidade de regularização pode ser adicionada para evitar overfitting. A função de custo médio (ou função de custo regularizada) é dada pela seguinte equação:\n",
    "\n",
    "   $$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m}[-y^{(i)} \\log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2 $$\n",
    "\n",
    "   Onde:\n",
    "   - $ m $ é o número total de exemplos de treinamento,\n",
    "   - $ n $ é o número de características,\n",
    "   - $ \\lambda $ é o parâmetro de regularização,\n",
    "   - $ \\theta_j $ é o j-ésimo parâmetro do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b49ed3",
   "metadata": {},
   "source": [
    "# Week 2 - Sentiment Analysis with Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cadd187",
   "metadata": {},
   "source": [
    "## Objetivos de aprendizagem\n",
    "- Error analysis\n",
    "- Naive Bayes inference\n",
    "- Log likelihood\n",
    "- Laplacian smoothing\n",
    "- conditional probabilities\n",
    "- Bayes rule\n",
    "- Sentiment analysis\n",
    "- Vocabulary creation\n",
    "- Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b4e35",
   "metadata": {},
   "source": [
    "## Recaptulando...\n",
    "\n",
    "1. **Probabilidade:**\n",
    "   A probabilidade é uma medida numérica que quantifica a incerteza associada a um evento. Em termos simples, é a chance de que algo aconteça. A probabilidade de um evento é sempre um número entre 0 e 1, onde 0 indica impossibilidade absoluta do evento ocorrer e 1 indica certeza absoluta de que o evento ocorrerá.\n",
    "   \n",
    "<img src=\"./imgs/prob_positive_tweet.png\">\n",
    "<img src=\"./imgs/prob_positive_tweet2.png\">\n",
    "\n",
    "2. **Probabilidade Condicional:**\n",
    "   A probabilidade condicional é a probabilidade de um evento ocorrer dado que outro evento já ocorreu. É denotada por $ P(A|B) $, que lê-se como \"a probabilidade de A dado B\". A fórmula para calcular a probabilidade condicional é:\n",
    "\n",
    "   $$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$\n",
    "\n",
    "   Onde:\n",
    "   - $ P(A|B) $ é a probabilidade de A dado B,\n",
    "   - $ P(A \\cap B) $ é a probabilidade da interseção de A e B,\n",
    "   - $ P(B) $ é a probabilidade de B.\n",
    "\n",
    "   Em palavras simples, a probabilidade condicional é a proporção de vezes que o evento A ocorre quando o evento B ocorre.\n",
    "\n",
    "<img src=\"./imgs/conditional_prob_tweet.png\">\n",
    "\n",
    "\n",
    "3. **Regra de Bayes:**\n",
    "   A regra de Bayes é uma ferramenta fundamental na teoria das probabilidades que permite atualizar as probabilidades de uma hipótese à luz de novas evidências. Formalmente, a regra de Bayes é expressa como:\n",
    "\n",
    "   $ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $\n",
    "\n",
    "   Onde:\n",
    "   - $ P(A|B) $ é a probabilidade de A dado B (posterior),\n",
    "   - $ P(B|A) $ é a probabilidade de B dado A (likelihood),\n",
    "   - $ P(A) $ é a probabilidade de A (prior),\n",
    "   - $ P(B) $ é a probabilidade de B.\n",
    "\n",
    "   A regra de Bayes nos permite calcular a probabilidade de uma hipótese (A) ser verdadeira dada uma evidência observada (B), usando a probabilidade da evidência dada a hipótese (likelihood), a probabilidade a priori da hipótese e a probabilidade marginal da evidência.A diferença principal entre a probabilidade condicional e a regra de Bayes é que a probabilidade condicional é uma medida da probabilidade de um evento ocorrer dado que outro evento já ocorreu, enquanto a regra de Bayes é uma ferramenta para atualizar a probabilidade de uma hipótese à luz de novas evidências. A regra de Bayes utiliza a probabilidade condicional como um de seus componentes para calcular a probabilidade posterior.\n",
    "\n",
    "<img src=\"./imgs/bayes_rule_tweet.png\">\n",
    "\n",
    "A probabilidade é a chance de um evento ocorrer, a probabilidade condicional é a probabilidade de um evento ocorrer dado que outro evento já ocorreu, e a regra de Bayes é uma ferramenta para atualizar a probabilidade de uma hipótese à luz de novas evidências."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b7b386",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "O algoritmo Naive Bayes é um método de classificação probabilístico baseado no teorema de Bayes, com uma suposição \"ingênua\" de independência condicional entre os recursos.\n",
    "\n",
    "1. **Suposição de Independência Condicional:**\n",
    "   A primeira etapa do algoritmo Naive Bayes é a suposição de independência condicional entre os recursos. Isso significa que assumimos que os recursos são independentes entre si, dado o valor da variável de classe. Apesar de ser uma suposição forte e muitas vezes não ser verdadeira na prática, ela simplifica os cálculos e torna o algoritmo computacionalmente eficiente.\n",
    "\n",
    "2. **Construção do Modelo de Probabilidade:**\n",
    "   O próximo passo é construir o modelo de probabilidade. Isso envolve calcular a probabilidade de cada classe e a probabilidade de cada valor do recurso dado cada classe. Em outras palavras, para cada classe, calculamos a probabilidade a priori da classe ( $ P(C_k) $ ) e a probabilidade de cada recurso ( $ P(X_i | C_k) $ ).\n",
    "\n",
    "3. **Classificação:**\n",
    "   Depois que o modelo de probabilidade é construído, podemos usá-lo para fazer previsões sobre novos exemplos. Dada uma nova instância com valores de recursos $ x_1, x_2, ..., x_n $, queremos calcular a probabilidade de pertencer a cada classe e, em seguida, atribuir a classe com a maior probabilidade como a classe prevista para a instância. Isso é feito usando o teorema de Bayes:\n",
    "\n",
    "   $$ P(C_k | x_1, x_2, ..., x_n) = \\frac{P(C_k) \\cdot \\prod_{i=1}^{n} P(x_i | C_k)}{P(x_1, x_2, ..., x_n)} $$\n",
    "\n",
    "   Onde:\n",
    "   - $ P(C_k | x_1, x_2, ..., x_n) $ é a probabilidade da classe $ C_k $ dado os valores dos recursos,\n",
    "   - $ P(C_k) $ é a probabilidade a priori da classe $ C_k $,\n",
    "   - $ P(x_i | C_k) $ é a probabilidade de cada valor do recurso dado a classe $ C_k $,\n",
    "   - $ P(x_1, x_2, ..., x_n) $ é a probabilidade dos valores dos recursos.\n",
    "\n",
    "4. **Estimação de Parâmetros:**\n",
    "   Durante a etapa de construção do modelo, precisamos estimar os parâmetros do modelo, ou seja, as probabilidades a priori das classes e as probabilidades condicionais dos recursos para cada classe. Isso geralmente é feito usando técnicas como a frequência relativa de ocorrência dos dados de treinamento.\n",
    "\n",
    "5. **Suavização de Laplace (Opcional):**\n",
    "   Em alguns casos, para evitar probabilidades condicionais iguais a zero para recursos não observados em uma classe particular, pode ser aplicada a suavização de Laplace, adicionando uma pequena quantidade aos contadores de frequência de cada valor de recurso para cada classe durante a estimativa dos parâmetros.\n",
    "\n",
    "Essas são as etapas principais da função do algoritmo Naive Bayes, desde a suposição de independência condicional até a classificação de novas instâncias usando o teorema de Bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6188f3a4",
   "metadata": {},
   "source": [
    "## Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917266a3",
   "metadata": {},
   "source": [
    "Para construir um classificador, começaremos primeiro criando probabilidades condicionais, dada a tabela a seguir:\n",
    "\n",
    "<img src=\"./imgs/naive_bayes_intro1.png\">\n",
    "          \n",
    "Isso nos permite calcular a seguinte tabela de probabilidades:\n",
    "<img src=\"./imgs/naive_bayes_intro2.png\">\n",
    "\n",
    "Depois de ter as probabilidades, podemos calcular a pontuação de probabilidade da seguinte forma\n",
    "<img src=\"./imgs/naive_bayes_intro3.png\">\n",
    "\n",
    "Uma pontuação maior que 1 indica que a classe é positiva, caso contrário é negativa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30595f9",
   "metadata": {},
   "source": [
    "Costumamos calcular a probabilidade de uma palavra dada uma classe da seguinte forma:\n",
    "\n",
    "$$\n",
    "P(w_i \\mid \\text{classe}) = \\frac{\\text{freq}(w_i, \\text{classe})}{N_{\\text{classe}}}\n",
    "\\quad \\text{classe} \\in \\{\\text{Positivo}, \\text{Negativo}\\}\n",
    "$$\n",
    "\n",
    "No entanto, se uma palavra não aparecer no treinamento, ela automaticamente recebe uma probabilidade de 0. Para corrigir isso, adicionamos **suavização de laplace** da seguinte forma:\n",
    "\n",
    "$$\n",
    "P(w_i \\mid \\text{classe}) = \\frac{\\text{freq}(w_i, \\text{classe}) + 1}{N_{\\text{classe}} + V}\n",
    "$$\n",
    "\n",
    "Observe que adicionamos um 1 no numerador e, como há V palavras para normalizar, adicionamos V no denominador.\n",
    "\n",
    "Onde:\n",
    "- $N_{\\text{classe}}$: frequência de todas as palavras na classe.\n",
    "- $V$: número de palavras únicas no vocabulário.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183fd509",
   "metadata": {},
   "source": [
    "Para calcular a log-verossimilhança (log likelihood), precisamos obter as razões e usá-las para calcular uma pontuação que nos permitirá decidir se um tweet é positivo ou negativo. Quanto maior a razão, mais positiva é a palavra:\n",
    "\n",
    "<img src=\"./imgs/log_likelihood1.png\">\n",
    "\n",
    "Para fazer inferência, você pode calcular o seguinte:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\prod_{i=1}^{m} \\frac{P(w_i \\mid \\text{neg})}{P(w_i \\mid \\text{pos})} \\right) > \\frac{1}{P(\\text{neg}) P(\\text{pos})}\n",
    "$$\n",
    "\n",
    "A expressão acima começa com um logaritmo, $\\log$, que é aplicado a um produto de probabilidades condicionais. O produto é denotado pelo símbolo $\\prod$ e representa o produto de todas as probabilidades condicionais das palavras $w_i$ dadas as classes \"negativo\" e \"positivo\". Isso significa que estamos multiplicando a probabilidade de cada palavra $w_i$ ocorrer, dado que o tweet é negativo, e dividindo pelo mesmo para tweets positivos.\n",
    "\n",
    "A expressão é comparada com o inverso do produto das probabilidades das classes \"negativo\" e \"positivo\". Isso é feito usando o sinal $>$, indicando que queremos que a expressão à esquerda seja maior do que o inverso do produto das probabilidades das classes.\n",
    "\n",
    "Em termos de interpretação, isso significa que estamos comparando a probabilidade conjunta de todas as palavras em um tweet serem negativas (ou positivas) com a probabilidade de um tweet ser classificado como negativo (ou positivo), independente do conteúdo do tweet. Se a probabilidade conjunta de todas as palavras serem negativas (ou positivas) for maior do que a probabilidade do tweet ser classificado como negativo (ou positivo) independentemente do conteúdo, então a inferência seria que o tweet é mais provavelmente negativo (ou positivo).\n",
    "\n",
    "Essa expressão é uma maneira de inferir a polaridade (positiva ou negativa) de um tweet com base na probabilidade condicional de cada palavra em relação às classes \"negativo\" e \"positivo\", em comparação com a probabilidade marginal das classes. Se a probabilidade conjunta das palavras sendo negativas (ou positivas) for maior do que a probabilidade do tweet ser classificado como negativo (ou positivo) independentemente do conteúdo, então a inferência seria que o tweet é mais provavelmente negativo (ou positivo).\n",
    "\n",
    "Conforme o número de palavras do tweet (m) aumenta, podemos ter problemas numéricos, então introduzimos o logaritmo, que nos dá a seguinte equação:\n",
    "\n",
    "$$\n",
    "\\log \\left( \\prod_{i=1}^{m} \\frac{P(w_i \\mid \\text{neg})}{P(w_i \\mid \\text{pos})} \\right) \\Rightarrow \\log \\prod_{i=1}^{m} \\frac{P(w_i \\mid \\text{neg})}{P(w_i \\mid \\text{pos})} + \\sum_{i=1}^{m} \\log \\frac{P(w_i \\mid \\text{neg})}{P(w_i \\mid \\text{pos})}\n",
    "$$\n",
    "\n",
    "Utilizando a propriedade do logaritmo de um produto, podemos reescrever a expressão como a soma dos logaritmos dos fatores dentro do produto. Esta transformação nos permite calcular a log-verossimilhança de forma mais eficiente e robusta, evitando o custo computacional de calcular o logaritmo de um produtório. A expressão agora é uma soma de logaritmos individuais, o que é mais estável numericamente. A nova expressão nos permite calcular a log-verossimilhança somando os logaritmos das razões das probabilidades condicionais de cada palavra em relação às classes \"negativo\" e \"positivo\". Isso nos dá uma medida da probabilidade de observar as palavras em um tweet dado que ele é classificado como negativo, em comparação com a mesma probabilidade para tweets positivos. Essa transformação simplifica o cálculo da log-verossimilhança e reduz a chance de problemas numéricos, tornando a inferência mais eficiente e precisa.\n",
    "\n",
    "O primeiro componente é chamado de log prior e o segundo componente é a log-verossimilhança. Introduzimos ainda $\\lambda$ como segue:\n",
    "\n",
    "<img src=\"./imgs/log_likelihood2.png\">\n",
    "\n",
    "Ter o dicionário $\\lambda$ ajudará muito ao fazer inferência, mas uma vez que computemos o dicionário $\\lambda$, se torna simples fazer a inferência, simplesmente somando os lambdas e comparando com os thresholds de negativo, neutro e positivo.\n",
    "\n",
    "<img src=\"./imgs/log_likelihood3.png\">\n",
    "\n",
    "O resultado foi **3.3**, sendo > 0, classificaremos o documento como positivo. Se tivessemos um número negativo, classificaríamos como a classe negativa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab17d324",
   "metadata": {},
   "source": [
    "## Training Naïve Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4004827b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:29.632630Z",
     "start_time": "2024-04-13T12:37:29.459569Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.9709302325581395\n",
      "Precisão da classificação (Classe 0): 0.6216216216216216\n",
      "Precisão da classificação (Classe 1): 0.9333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaoag/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/home/joaoag/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:623: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Pré-processamento\n",
    "df[\"ttweet\"] = df[\"tweet\"].apply(text_cleaning)\n",
    "df[\"ttweet\"] = df[\"ttweet\"].apply(lambda x: \" \".join(x))\n",
    "\n",
    "# Listas de palavras positivas e negativas\n",
    "positive_words = df[df[\"label\"] == 0][\"ttweet\"].to_list()\n",
    "positive_words = word_tokenize(' '.join(positive_words))\n",
    "\n",
    "negative_words = df[df[\"label\"] == 1][\"ttweet\"].to_list()\n",
    "negative_words = word_tokenize(' '.join(negative_words))\n",
    "\n",
    "# Funções para mapear frequências de palavras\n",
    "def positive_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())\n",
    "    freq = FreqDist(tokens)\n",
    "    positive_freq = sum([freq[word] for word in positive_words])\n",
    "    return positive_freq\n",
    "\n",
    "def negative_words_frequency(tweet):\n",
    "    tokens = word_tokenize(tweet.lower())\n",
    "    freq = FreqDist(tokens)\n",
    "    negative_freq = sum([freq[word] for word in negative_words])\n",
    "    return negative_freq\n",
    "\n",
    "# Aplicando as funções e adicionando colunas ao dataframe\n",
    "df['bias'] = 1\n",
    "df['pos_freq'] = df['ttweet'].apply(positive_words_frequency)\n",
    "df['neg_freq'] = df['ttweet'].apply(negative_words_frequency)\n",
    "\n",
    "# Modelagem com Naive Bayes ou Regressão Logística\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Criando e treinando o modelo\n",
    "model = MultinomialNB() # Exemplo: usando Naive Bayes\n",
    "model.fit(df[['bias', 'pos_freq', 'neg_freq']], df['label'])\n",
    "\n",
    "# Fazendo previsões\n",
    "predictions = model.predict(df[['bias', 'pos_freq', 'neg_freq']])\n",
    "\n",
    "# Avaliando o desempenho do modelo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(df['label'], predictions)\n",
    "print(f\"Acurácia do modelo: {accuracy}\")\n",
    "print(\"Precisão da classificação (Classe 0):\", precision_score(y_test, y_pred, pos_label=0))\n",
    "print(\"Precisão da classificação (Classe 1):\", precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a13b10e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:29.649776Z",
     "start_time": "2024-04-13T12:37:29.634424Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.8857142857142857\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Separando os tweets e seus sentimentos\n",
    "X = [tweet[0] for tweet in tweets]\n",
    "y = [sentimento for _, sentimento in tweets]\n",
    "\n",
    "# Dividindo os dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Criando uma matriz de frequência de palavras\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_counts = vectorizer.fit_transform(X_train)\n",
    "X_test_counts = vectorizer.transform(X_test)\n",
    "\n",
    "# Treinando o classificador Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train_counts, y_train)\n",
    "\n",
    "# Fazendo previsões\n",
    "predictions = clf.predict(X_test_counts)\n",
    "\n",
    "# Calculando a acurácia do modelo\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Acurácia do modelo:\", accuracy)\n",
    "# print(\"Precisão da classificação (Classe 0):\", precision_score(y_test, y_pred, pos_label=0))\n",
    "# print(\"Precisão da classificação (Classe 1):\", precision_score(y_test, y_pred, pos_label=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2b8871",
   "metadata": {},
   "source": [
    "Visão geral de como o algoritmo funciona:\n",
    "\n",
    "- Treinamento: Durante a fase de treinamento, o algoritmo calcula a probabilidade de cada classe (por exemplo, positivo, negativo) ocorrer, bem como as probabilidades condicionais de cada feature (palavra) dado cada classe. Essas probabilidades são estimadas a partir dos dados de treinamento.\n",
    "\n",
    "- Previsão: Durante a fase de previsão, o algoritmo utiliza o Teorema de Bayes para calcular a probabilidade de cada classe dado um conjunto de features (no caso, as palavras em um tweet). A classe com a maior probabilidade posterior é então atribuída como a classe prevista para a instância.\n",
    "\n",
    "- Suavização de Laplace: Para evitar problemas quando uma palavra não aparece no conjunto de treinamento para uma determinada classe, é comum aplicar uma técnica de suavização chamada Suavização de Laplace, que adiciona uma contagem pseudocount para todas as features durante o cálculo das probabilidades condicionais.\n",
    "\n",
    "- Vantagens e Desvantagens: O Naive Bayes é fácil de implementar, eficiente em termos de tempo de treinamento e pode funcionar bem mesmo com conjuntos de dados pequenos. No entanto, sua suposição de independência entre as features pode não ser realista em muitos casos, o que pode levar a resultados subótimos em certas situações."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68967e26",
   "metadata": {},
   "source": [
    "## Testing Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0de7b9f",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/naive_bayes_test.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed05828c",
   "metadata": {},
   "source": [
    "## Applications of Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c710d",
   "metadata": {},
   "source": [
    "O algoritmo Naive Bayes é amplamente utilizado em uma variedade de aplicações em Data Science e Machine Learning devido à sua simplicidade, eficiência computacional e bom desempenho em muitos cenários. Como em:\n",
    "\n",
    "1. **Classificação de Texto:** Naive Bayes é frequentemente usado para classificar documentos de texto em categorias, como spam vs. não spam em e-mails, classificação de sentimentos em redes sociais, detecção de tópicos em artigos de notícias, entre outros.\n",
    "\n",
    "2. **Filtragem de Spam:** É um uso clássico de Naive Bayes, onde o algoritmo é treinado com uma base de dados de e-mails rotulados como spam ou não spam, e então usado para prever se novos e-mails são spam ou não.\n",
    "\n",
    "3. **Análise de Sentimento:** Naive Bayes é eficaz na análise de sentimentos em dados textuais, como comentários de clientes, análise de feedbacks de produtos e análise de redes sociais, ajudando a determinar se um texto é positivo, negativo ou neutro.\n",
    "\n",
    "4. **Classificação de Documentos:** Além de filtragem de spam, Naive Bayes é usado para categorizar documentos em diferentes classes, como classificar notícias em categorias como política, esportes, entretenimento, etc.\n",
    "\n",
    "5. **Sistemas de Recomendação:** Pode ser utilizado em sistemas de recomendação para classificar e sugerir itens com base no histórico de interações do usuário, como classificar produtos em sites de compras ou recomendar filmes ou músicas em plataformas de streaming.\n",
    "\n",
    "6. **Diagnóstico Médico:** Na área médica, o Naive Bayes pode ser aplicado para auxiliar no diagnóstico de doenças, utilizando características dos pacientes para prever a presença ou ausência de certas condições médicas.\n",
    "\n",
    "7. **Análise de Risco Financeiro:** Naive Bayes é utilizado em análises de risco financeiro para prever riscos de crédito, detectar fraudes em transações financeiras e realizar análises de mercado.\n",
    "\n",
    "8. **Previsão de Churn:** Na área de negócios, o Naive Bayes pode ser usado para prever a probabilidade de um cliente cancelar um serviço (churn), com base em dados históricos de comportamento do cliente.\n",
    "\n",
    "Sua simplicidade e eficiência o tornam uma escolha popular em uma variedade de cenários."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f07acdb",
   "metadata": {},
   "source": [
    "## Naïve Bayes Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855c9a5c",
   "metadata": {},
   "source": [
    "O algoritmo Naive Bayes é baseado em algumas premissas importantes, que são fundamentais para o seu funcionamento. Como:\n",
    "\n",
    "1. **Independência Condicional:** Esta é a premissa mais crucial e é de onde o \"Naive\" em \"Naive Bayes\" vem. O algoritmo assume que as features (ou atributos) usadas para a classificação são independentes entre si, dadas as classes. Em outras palavras, ele assume que a presença ou ausência de uma característica não está relacionada à presença ou ausência de outras features, dado o resultado da classe. Embora esta premissa raramente seja verdadeira na prática, o Naive Bayes muitas vezes funciona bem mesmo quando ela é violada, tornando-o muito eficaz em muitos casos.\n",
    "\n",
    "2. **Presença de Dados de Treinamento:** O Naive Bayes requer um conjunto de dados de treinamento que inclua exemplos rotulados. Ou seja, para cada exemplo, deve-se saber a que classe ele pertence. Esses exemplos de treinamento são essenciais para estimar as probabilidades necessárias para a classificação.\n",
    "\n",
    "3. **Distribuição de Features:** O Naive Bayes assume uma distribuição específica para as features. Embora seja comum assumir uma distribuição de Bernoulli para features binárias, o Naive Bayes também pode ser aplicado com **distribuições multinomiais** (para features categóricas) e distribuições gaussianas (para features contínuas).\n",
    "\n",
    "4. **Probabilidades Condicionais:** O algoritmo Naive Bayes usa a teoria das probabilidades para calcular a probabilidade de uma instância pertencer a cada classe com base nas features observadas. Ele assume que as probabilidades condicionais de cada classe dado o conjunto de features podem ser calculadas facilmente.\n",
    "\n",
    "Essas premissas são simplificações significativas da realidade, e é por isso que o algoritmo é chamado de \"Naive\" (ingênuo). No entanto, apesar de suas simplificações, o Naive Bayes muitas vezes funciona surpreendentemente bem em uma variedade de problemas de classificação, desde que as premissas sejam razoavelmente satisfeitas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7bfd05",
   "metadata": {},
   "source": [
    "# Week 3 - Vector Space Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9a2d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T22:22:55.561680Z",
     "start_time": "2024-04-12T22:22:55.556434Z"
    }
   },
   "source": [
    "## Objetivos de aprendizagem\n",
    "- Covariance matrices\n",
    "- Dimensionality reduction\n",
    "- Principal component analysis\n",
    "- Cosine similarity\n",
    "- Euclidean distance\n",
    "- Co-occurrence matrices\n",
    "- Vector representations\n",
    "- Vector space models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576d2459",
   "metadata": {},
   "source": [
    "## Introduction to Vector Space Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c80008f",
   "metadata": {},
   "source": [
    "Os modelos de espaço vetorial (VSMs) são uma técnica fundamental em processamento de linguagem natural (PLN) para representar o significado de palavras e documentos em um espaço matemático contínuo. Eles são usados para capturar e representar semanticamente o contexto e a similaridade entre palavras e documentos. Os VSMs representam palavras como **vetores** matemáticos em um **espaço de alta dimensionalidade**. Cada dimensão do vetor **pode representar um aspecto diferente da palavra**, como seu contexto, uso ou significado. Essas dimensões são geralmente aprendidas automaticamente a partir de grandes conjuntos de dados textuais usando técnicas como word embeddings. Ou seja, **é codificado uma representação numérica de palavras que captura o significado relativo de forma contextual**. Exemplo:\n",
    "- A palavra \"gato\" pode ser representada por um vetor como [0.2, -0.1, 0.5, ...], onde cada valor numérico representa um aspecto diferente da palavra.\n",
    "\n",
    "Além de representar palavras, os VSMs também podem representar documentos inteiros. Nesse caso, cada documento é representado como um vetor onde cada dimensão pode representar a frequência ou importância de uma palavra ou conceito dentro desse documento. Exemplo:\n",
    "- Um artigo de notícias sobre tecnologia pode ser representado por um vetor onde a dimensão correspondente à palavra \"tecnologia\" tem um valor alto, enquanto as dimensões correspondentes a palavras irrelevantes têm valores baixos.\n",
    "\n",
    "Os VSMs são úteis para **medir a similaridade semântica entre palavras e documentos**. Isso é feito **calculando a proximidade entre os vetores no espaço vetorial**. Palavras ou documentos semanticamente semelhantes estarão mais próximos no espaço vetorial. Exemplo:\n",
    "- Os vetores das palavras \"cachorro\" e \"animal\" provavelmente estarão mais próximos no espaço vetorial do que os vetores das palavras \"cachorro\" e \"carro\".\n",
    "\n",
    "Eles são amplamente utilizados em tarefas de PLN, como recuperação de informações, classificação de texto, agrupamento de documentos, tradução automática e muito mais. Eles formam a base para muitas técnicas e algoritmos modernos de PLN. Exemplo:\n",
    "- Um sistema de recomendação de filmes pode usar VSMs para encontrar filmes semelhantes com base nas sinopses dos filmes já assistidos pelo usuário.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed20a10",
   "metadata": {},
   "source": [
    "## Word by Word and Word by Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065b202b",
   "metadata": {},
   "source": [
    "Construir vetores usando uma matriz de coocorrência com um design de word by word é uma abordagem para criar representações vetoriais de palavras com base em sua coocorrência em um corpus de texto. Aqui está uma explicação detalhada:\n",
    "\n",
    "**Construção da Matriz de Coocorrência**:\n",
    "   - **Definição do Contexto**: Para cada palavra no vocabulário, é necessário definir um contexto em torno dela. Por exemplo, para uma palavra \"cachorro\", o contexto pode ser os N tokens à esquerda e à direita da palavra em um texto.\n",
    "   - **Contagem de Coocorrências**: Para cada palavra no vocabulário, conte quantas vezes ela ocorre no contexto de outras palavras. Isso resulta em uma matriz de coocorrência, onde cada célula (i, j) representa quantas vezes a palavra i ocorre no contexto da palavra j.\n",
    "\n",
    "**Construção dos Vetores**:\n",
    "   - **Normalização**: Antes de construir os vetores, a matriz de coocorrência pode ser normalizada para considerar fatores como a frequência geral das palavras no corpus e a frequência dos próprios contextos.\n",
    "   - **Construção dos Vetores**: Cada linha ou coluna na matriz de coocorrência representa um vetor para uma palavra específica. Isso pode ser feito tratando cada linha ou coluna como um vetor.\n",
    "\n",
    "Suponha que temos o seguinte texto como nosso corpus:\n",
    "\n",
    "```\n",
    "O cachorro correu no parque.\n",
    "O gato dormiu no sofá.\n",
    "O cachorro e o gato são amigos.\n",
    "```\n",
    "\n",
    "Para construir a matriz de coocorrência para um contexto de duas palavras à esquerda e à direita, teríamos algo assim (excluindo palavras de parada como \"o\", \"e\", etc.):\n",
    "\n",
    "```\n",
    "          cachorro correu gato dormiu são amigos\n",
    "cachorro     0       1     1      0     0     1\n",
    "correu       1       0     0      1     0     0\n",
    "gato         1       0     0      0     1     1\n",
    "dormiu       0       1     0      0     1     0\n",
    "são          0       0     1      1     0     1\n",
    "amigos       1       0     1      0     1     0\n",
    "```\n",
    "\n",
    "Para construir vetores para cada palavra, normalizamos esta matriz e tratamos cada linha ou coluna como um vetor.\n",
    "\n",
    "**Uso dos Vetores**:\n",
    "   - Uma vez que os vetores tenham sido construídos, eles podem ser usados para várias tarefas em PLN, como encontrar palavras semanticamente semelhantes, calcular distâncias entre palavras, etc.\n",
    "\n",
    "Essa abordagem é uma maneira simples e eficaz de construir representações vetoriais de palavras usando a coocorrência em um corpus de texto. No entanto, ela pode ser limitada em capturar nuances semânticas mais complexas, especialmente em corpora muito grandes ou com vocabulários extensos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51daeecf",
   "metadata": {},
   "source": [
    "Construir vetores usando uma matriz de coocorrência com um design de word by document é uma abordagem para criar representações vetoriais de palavras com base em sua coocorrência com documentos inteiros em um corpus de texto. Aqui está uma explicação detalhada:\n",
    "\n",
    "**Construção da Matriz de Coocorrência**:\n",
    "   - **Definição do Contexto**: Neste caso, o contexto é o documento inteiro em que uma palavra ocorre. Cada documento é tratado como uma \"janela\" em torno das palavras.\n",
    "   - **Contagem de Coocorrências**: Para cada palavra no vocabulário, conte quantas vezes ela ocorre em cada documento. Isso resulta em uma matriz de coocorrência, onde cada linha representa uma palavra e cada coluna representa um documento.\n",
    "\n",
    "**Construção dos Vetores**:\n",
    "   - **Normalização**: Assim como na abordagem word by word, a matriz de coocorrência pode ser normalizada para considerar fatores como a frequência geral das palavras no corpus e a frequência dos próprios documentos.\n",
    "   - **Construção dos Vetores**: Neste caso, cada linha na matriz de coocorrência representa um vetor para uma palavra específica. Cada elemento do vetor pode representar a frequência ou a importância relativa da palavra em cada documento.\n",
    "\n",
    "Suponha que temos o seguinte corpus de três documentos:\n",
    "\n",
    "1. Documento 1: \"O cachorro correu no parque.\"\n",
    "2. Documento 2: \"O gato dormiu no sofá.\"\n",
    "3. Documento 3: \"O cachorro e o gato são amigos.\"\n",
    "\n",
    "A matriz de coocorrência seria algo como:\n",
    "\n",
    "```\n",
    "          Documento 1 Documento 2 Documento 3\n",
    "cachorro      1            0           1\n",
    "correu        1            0           0\n",
    "gato          0            1           1\n",
    "dormiu        0            1           0\n",
    "são           0            0           1\n",
    "amigos        0            0           1\n",
    "```\n",
    "\n",
    "Para construir vetores para cada palavra, normalizamos esta matriz e tratamos cada linha como um vetor. Uma vez que os vetores tenham sido construídos, eles podem ser usados para várias tarefas em PLN, como encontrar palavras semanticamente semelhantes, calcular distâncias entre palavras, classificação de texto, entre outros.\n",
    "\n",
    "Esta abordagem é útil quando estamos mais interessados nas **relações entre palavras e documentos** do que nas relações entre palavras individuais. Ela pode capturar a essência de como palavras diferentes estão distribuídas em diferentes contextos de documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e2b0a3",
   "metadata": {},
   "source": [
    "No exemplo abaixo, podemos verificar a relação entre palavras e documentos, e temos que as palavras \"data\" estão mais relacionadas a Economy e ML do que Entertainment. Assim como a palavra \"film\" está mais relacionada à Entertainment que aos demais documentos. Aqui, a categoria Entertainment pode ser representada como um vetor v = [500, 700], assim como Economy como um vetor v = [6620, 4000] e ML como um vetor v = [9320, 1000]\n",
    "\n",
    "<img src=\"./imgs/vsm_corpus.png\">\n",
    "<img src=\"./imgs/vsm_vector.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a270c",
   "metadata": {},
   "source": [
    "## Euclidean Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c65e113",
   "metadata": {},
   "source": [
    "A Distância Euclidiana é uma medida de distância entre dois pontos em um espaço euclidiano. É derivada do teorema de Pitágoras e é frequentemente usada em diversas áreas, como geometria, análise de dados, reconhecimento de padrões e aprendizado de máquina. Aqui está uma explicação detalhada:\n",
    "\n",
    "A fórmula para calcular a distância euclidiana entre dois pontos $ P $ e $ Q $ em um espaço $ n $-dimensional é dada por:\n",
    "\n",
    "$ \\text{EuclideanDistance}(P, Q) = \\sqrt{\\sum_{i=1}^{n} (q_i - p_i)^2} $\n",
    "\n",
    "Onde:\n",
    "- $ P = (p_1, p_2, ..., p_n) $ e $ Q = (q_1, q_2, ..., q_n) $ são os pontos no espaço $ n $-dimensional.\n",
    "- $ p_i $ e $ q_i $ são as coordenadas dos pontos $ P $ e $ Q $ ao longo da dimensão $ i $.\n",
    "\n",
    "**Características**:\n",
    "- **Positividade**: A distância euclidiana é sempre não negativa, ou seja, $ \\text{EuclideanDistance}(P, Q) \\geq 0 $.\n",
    "- **Identidade de Indiscerníveis**: A distância entre dois pontos é zero se e somente se os pontos são idênticos.\n",
    "- **Simetria**: A distância entre dois pontos $ P $ e $ Q $ é a mesma que a distância entre $ Q $ e $ P $.\n",
    "- **Desigualdade Triangular**: A distância de um ponto a outro ponto é sempre menor do que ou igual à soma das distâncias de um ponto a um terceiro ponto intermediário.\n",
    "\n",
    "\n",
    "Suponha que tenhamos dois pontos no plano 2D: $ P(2, 3) $ e $ Q(5, 7) $. Para calcular a distância euclidiana entre esses dois pontos, usamos a fórmula:\n",
    "\n",
    "$$ \\text{Euclidean Distance}(P, Q) = \\sqrt{(5-2)^2 + (7-3)^2} = \\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5 $$\n",
    "\n",
    "Portanto, a distância euclidiana entre $ P $ e $ Q $ é 5 unidades.\n",
    "\n",
    "A distância euclidiana é amplamente utilizada em algoritmos de agrupamento (como k-means), classificação (como k-NN), redução de dimensionalidade (como PCA), reconhecimento de padrões, entre outros. É uma métrica fundamental em muitos problemas de otimização, onde minimizar ou maximizar a distância entre pontos é um objetivo. Ela é é uma medida intuitiva e útil para quantificar a distância entre pontos em um espaço multidimensional, com aplicações em várias áreas da ciência e da engenharia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717dbd11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T11:41:29.254147Z",
     "start_time": "2024-04-13T11:41:29.249421Z"
    }
   },
   "source": [
    "<img src=\"./imgs/euclidean_distance.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa20e9f8",
   "metadata": {},
   "source": [
    "Podemos generalizar a encontrar a distância entre dois pontos $(x_1, y_1)$ e $(x_2, y_2)$ para a distância entre um vetor $\\mathbf{v}$ de $n$ dimensões da seguinte forma:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{v}, \\mathbf{w}) = \\sqrt{\\sum_{i=1}^{n} (v_i - w_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0569ec4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T11:44:03.471039Z",
     "start_time": "2024-04-13T11:44:03.465662Z"
    }
   },
   "source": [
    "<img src=\"./imgs/euclidean_distance_n.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00bed52",
   "metadata": {},
   "source": [
    "Para calcular a distância euclidiana entre dois vetores n-dimensionais em Python, podemos usar a biblioteca NumPy, que fornece funções eficientes para operações matemáticas em vetores e matrizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c782a0df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:29.660451Z",
     "start_time": "2024-04-13T12:37:29.651948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A distância euclidiana entre os vetores é: 6.4031242374328485\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calcula a distância euclidiana entre dois vetores n-dimensionais.\n",
    "\n",
    "    Argumentos:\n",
    "    vector1 (np.array): O primeiro vetor.\n",
    "    vector2 (np.array): O segundo vetor.\n",
    "\n",
    "    Retorna:\n",
    "    float: A distância euclidiana entre os dois vetores.\n",
    "    \"\"\"\n",
    "    if len(vector1) != len(vector2):\n",
    "        raise ValueError(\"Os vetores devem ter o mesmo número de dimensões.\")\n",
    "\n",
    "    # Converte as listas em arrays do NumPy para cálculos eficientes\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "\n",
    "    # Calcula a diferença entre os dois vetores\n",
    "    difference = vector2 - vector1\n",
    "\n",
    "    # Calcula a soma dos quadrados das diferenças\n",
    "    squared_difference = np.sum(difference ** 2)\n",
    "\n",
    "    # Calcula a raiz quadrada da soma dos quadrados\n",
    "    euclidean_distance = np.sqrt(squared_difference)\n",
    "\n",
    "    return euclidean_distance\n",
    "\n",
    "# Exemplo de uso\n",
    "vector1 = [2, 3, 5]\n",
    "vector2 = [5, 7, 1]\n",
    "\n",
    "distance = euclidean_distance(vector1, vector2)\n",
    "print(\"A distância euclidiana entre os vetores é:\", distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a2cd7023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T12:37:29.668311Z",
     "start_time": "2024-04-13T12:37:29.662960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A distância euclidiana entre os vetores é: 6.4031242374328485\n"
     ]
    }
   ],
   "source": [
    "# Ou podemos simplesmente\n",
    "\n",
    "distance = np.linalg.norm(np.array(vector1) - np.array(vector2))\n",
    "print(\"A distância euclidiana entre os vetores é:\", distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dfb90e",
   "metadata": {},
   "source": [
    "## Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3aaf05",
   "metadata": {},
   "source": [
    "A similaridade do cosseno é uma medida de similaridade entre dois vetores em um espaço vetorial, frequentemente usado em mineração de texto, recuperação de informação e aprendizado de máquina. A medida é baseada no ângulo formado entre os dois vetores no espaço vetorial e não na distância euclidiana entre eles. A similaridade do cosseno entre dois vetores $ \\mathbf{A} $ e $ \\mathbf{B} $ é dada pela fórmula:\n",
    "\n",
    "$$ \\text{CosineSimilarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{\\mathbf{A} \\cdot \\mathbf{B}}{\\|\\mathbf{A}\\| \\|\\mathbf{B}\\|} $$\n",
    "\n",
    "Onde:\n",
    "- $ \\mathbf{A} \\cdot \\mathbf{B} $ é o produto interno (ou escalar) entre os vetores $ \\mathbf{A} $ e $ \\mathbf{B} $.\n",
    "- $ \\| \\mathbf{A} \\| $ e $ \\| \\mathbf{B} \\| $ são as normas (ou magnitudes) dos vetores $ \\mathbf{A} $ e $ \\mathbf{B} $, respectivamente.\n",
    "\n",
    "**Características**:\n",
    "- **Intervalo de Valores**: A similaridade do cosseno varia de -1 a 1, onde 1 indica que os vetores têm a mesma direção, 0 indica que os vetores são ortogonais (semelhantes de forma nula) e -1 indica que os vetores têm direções opostas.\n",
    "- **Independência de Escala**: A similaridade do cosseno é independente da escala dos vetores. Isso significa que os vetores podem ser normalizados antes do cálculo sem afetar o resultado.\n",
    "- **Eficiência Computacional**: O cálculo da similaridade do cosseno é computacionalmente eficiente, especialmente para vetores de alta dimensionalidade.\n",
    "- **Apropriado para Texto**: É amplamente utilizado em PLN, pois lida bem com vetores de alta dimensão, como vetores de termos de documentos.\n",
    "\n",
    "Suponha que temos dois vetores $ \\mathbf{A} = [2, 1] $ e $ \\mathbf{B} = [3, 4] $. Para calcular a similaridade do cosseno entre esses dois vetores, usamos a fórmula:\n",
    "\n",
    "$$ \\text{CosineSimilarity}(\\mathbf{A}, \\mathbf{B}) = \\frac{(2 \\times 3) + (1 \\times 4)}{\\sqrt{2^2 + 1^2} \\times \\sqrt{3^2 + 4^2}} = \\frac{6 + 4}{\\sqrt{5} \\times \\sqrt{25}} = \\frac{10}{5 \\times 5} = \\frac{10}{25} = 0.4 $$\n",
    "\n",
    "Portanto, a similaridade do cosseno entre $ \\mathbf{A} $ e $ \\mathbf{B} $ é 0.4.\n",
    "\n",
    "A similaridade do cosseno é amplamente utilizada em tarefas de recuperação de informação, onde é usada para encontrar documentos semelhantes com base em vetores de termos de documentos. Também é usado em **sistemas de recomendação** para calcular a similaridade entre perfis de usuários e itens. Além disso, é usado em classificação de texto e agrupamento de documentos. A similaridade do cosseno é uma medida eficaz de similaridade entre vetores que é amplamente utilizada em várias aplicações, especialmente em processamento de linguagem natural e mineração de texto.\n",
    "\n",
    "Se os documentos tiverem tamanhos diferentes, a métrica de **Euclidean Distance** não é a ideal. **Cosine Similarity** não é enviesada pelo pelo tamanho da diferença entre os vetores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d9ddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T11:54:48.685615Z",
     "start_time": "2024-04-13T11:54:48.680427Z"
    }
   },
   "source": [
    "<img src=\"./imgs/cosine_sim.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae43abb4",
   "metadata": {},
   "source": [
    "Para calcular a similaridade de cosseno entre dois vetores em Python, você pode usar a biblioteca NumPy para facilitar os cálculos vetoriais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8aa63b8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T14:34:13.631682Z",
     "start_time": "2024-04-13T14:34:13.624244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A similaridade de cosseno entre os vetores é: 0.8944271909999159\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Calcula a similaridade de cosseno entre dois vetores.\n",
    "\n",
    "    Argumentos:\n",
    "    vector1 (np.array): O primeiro vetor.\n",
    "    vector2 (np.array): O segundo vetor.\n",
    "\n",
    "    Retorna:\n",
    "    float: A similaridade de cosseno entre os dois vetores.\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    similarity = dot_product / (norm_vector1 * norm_vector2)\n",
    "    return similarity\n",
    "\n",
    "# Exemplo de uso\n",
    "vector1 = np.array([2, 1])\n",
    "vector2 = np.array([3, 4])\n",
    "\n",
    "similarity = cosine_similarity(vector1, vector2)\n",
    "print(\"A similaridade de cosseno entre os vetores é:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f8e55",
   "metadata": {},
   "source": [
    "## Manipulating Words in Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b65e97",
   "metadata": {},
   "source": [
    "Manipular vetores usando aritmética para prever países e suas capitais é uma aplicação interessante de vetores em aprendizado de máquina e processamento de linguagem natural. Aqui está uma explicação passo a passo de como isso pode ser feito:\n",
    "\n",
    "**Representação de Palavras como Vetores:**\n",
    "Primeiro, você precisa de uma representação vetorial para as palavras envolvidas - neste caso, países e suas capitais. Uma maneira comum de fazer isso é usar embeddings de palavras pré-treinados, como os disponíveis em modelos como Word2Vec, GloVe ou FastText. Esses embeddings atribuem a cada palavra um vetor numérico denso em um espaço vetorial, onde palavras semanticamente semelhantes estão próximas umas das outras. Mas podemos usar os embeddings que criamos também.\n",
    "\n",
    "**Vetores de Países e Capitais**\n",
    "Para cada país e capital, você precisa de seus respectivos vetores. Se estiver usando embeddings de palavras pré-treinados, você pode simplesmente pegar os vetores correspondentes às palavras \"país\" e \"capital\" e somá-los aos vetores de palavras correspondentes ao país e à capital real. Por exemplo, para prever a capital do Brasil, você somaria o vetor correspondente a \"Brasil\" ao vetor correspondente a \"capital\", obtendo assim um vetor que representa a capital prevista.\n",
    "\n",
    "**Operações Vetoriais:**\n",
    "Agora que você tem vetores para países e capitais, você pode usar operações vetoriais para fazer previsões. Por exemplo, para prever a capital de um país, você pode calcular a diferença entre o vetor do país de interesse e o vetor de outro país conhecido com sua capital conhecida. Em seguida, você adiciona esse vetor de diferença ao vetor da capital conhecida para obter a capital prevista.\n",
    "\n",
    "**Exemplo:**\n",
    "Por exemplo, suponha que você tenha embeddings de palavras onde \"Brasil\" é representado pelo vetor $ \\mathbf{v}_{Brasil} $ e \"capital\" pelo vetor $ \\mathbf{v}_{capital} $. Se você quiser prever a capital do Brasil, você faria o seguinte:\n",
    "$$ \\text{Capital prevista} = \\mathbf{v}_{Brasil} - \\mathbf{v}_{Outro\\ país} + \\mathbf{v}_{Capital\\ conhecida} $$\n",
    "\n",
    "Outra forma de prever tokens usando as relações vetoriais, é usar as representações conhecidas. Sabendo que a capital dos USA é Washington, podemos ver a distância entre os dois vetores e somar ao vetor que representa Russia para descobrir sua capital, Como:\n",
    "\n",
    "<img scr=\"./imgs/manipulating_words.png\">\n",
    "\n",
    "**Avaliação e Ajuste:**\n",
    "Depois de fazer as previsões, podemos avaliar a precisão do seu modelo usando um conjunto de dados de teste que contenha pares de países e suas capitais reais. Se a precisão não for satisfatória, você pode ajustar os vetores ou usar técnicas mais avançadas de aprendizado de máquina para melhorar o desempenho do modelo. Manipular vetores usando aritmética para prever países e suas capitais (ou qualquer relacionamento parecido) envolve representar países e capitais como vetores, realizar operações vetoriais para fazer previsões e avaliar o desempenho do modelo. É uma aplicação interessante de vetores em processamento de linguagem natural e aprendizado de máquina."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5404fe",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb5fba6",
   "metadata": {},
   "source": [
    "A Análise de Componentes Principais (PCA) é um algoritmo não superisionado, uma técnica de redução de dimensionalidade amplamente utilizada para simplificar conjuntos de dados complexos, mantendo as informações mais importantes. Funciona encontrando as direções (ou componentes) de maior variância nos dados e projetando os dados nesses novos eixos, chamados de componentes principais. Ele funciona da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/pca1.png\">\n",
    "\n",
    "<img src=\"./imgs/pca2.png\">\n",
    "\n",
    "**Centralização dos Dados:**\n",
    "Antes de aplicar o PCA, os dados são geralmente centralizados subtraindo a média de cada variável. Isso garante que o centro dos dados esteja na origem do espaço de características.\n",
    "\n",
    "**Cálculo da Matriz de Covariância**:\n",
    "Em seguida, é calculada a matriz de covariância dos dados centralizados. A covariância entre duas variáveis mede como elas variam juntas. A matriz de covariância captura as relações lineares entre as variáveis originais.\n",
    "\n",
    "**Decomposição da Matriz de Covariância**:\n",
    "A seguir, a matriz de covariância é decomposta em seus autovetores e autovalores. Os autovetores representam as direções dos eixos principais (ou componentes principais) dos dados, enquanto os autovalores representam a quantidade de variância explicada por cada componente principal.\n",
    "\n",
    "**Seleção dos Componentes Principais**:\n",
    "Os autovetores são ordenados de acordo com seus autovalores associados, do maior para o menor. Os autovetores com os maiores autovalores capturam a maior parte da variância nos dados e são selecionados como os componentes principais mais importantes.\n",
    "\n",
    "**Projeção dos Dados**:\n",
    "Finalmente, os dados são projetados nos novos eixos definidos pelos componentes principais selecionados. Isso reduz a dimensionalidade dos dados, substituindo as variáveis originais por uma combinação linear dos componentes principais.\n",
    "\n",
    "**Exemplo**:\n",
    "Vamos considerar um conjunto de dados bidimensional com duas variáveis, como altura e peso de uma população. O PCA encontrará a direção ao longo da qual a variabilidade dos dados é máxima. Suponha que a direção seja dada pelo autovetor \\( [0.8, 0.6] \\), com um autovalor associado de 10. Isso significa que 80% da variabilidade dos dados está ao longo dessa direção. Os dados podem ser projetados nessa direção, reduzindo-os de duas dimensões para uma dimensão.\n",
    "\n",
    "**Uso**:\n",
    "- Redução de dimensionalidade: PCA é usado para reduzir a dimensionalidade de dados complexos, preservando o máximo de informação possível.\n",
    "- Visualização de dados: PCA pode ser usado para visualizar dados de alta dimensionalidade em um espaço de menor dimensão.\n",
    "- Pré-processamento de dados: PCA é frequentemente usado como uma etapa de pré-processamento antes de aplicar algoritmos de aprendizado de máquina, para reduzir o tempo de treinamento e evitar a maldição da dimensionalidade.\n",
    "\n",
    "O PCA é uma técnica poderosa e amplamente utilizada para redução de dimensionalidade e análise exploratória de dados, que ajuda a simplificar conjuntos de dados complexos, mantendo as informações mais importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37f6da",
   "metadata": {},
   "source": [
    "Para reduzir vetores de alta dimensionalidade para visualizá-los em duas dimensões usando Principal Component Analysis (PCA) em Python, podemos seguir estas etapas:\n",
    "\n",
    "- Passo 1: Importe as bibliotecas necessárias, incluindo NumPy e sklearn para PCA.\n",
    "- Passo 2: Gere um conjunto de dados de exemplo com vetores de alta dimensão.\n",
    "- Passo 3: Aplique o PCA para reduzir a dimensionalidade dos vetores para 2 dimensões.\n",
    "- Passo 4: Plote os vetores reduzidos em um gráfico de dispersão para visualização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "729a7afd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T14:14:13.010671Z",
     "start_time": "2024-04-13T14:14:12.679586Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfAAAAGDCAYAAADUGkKJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAADjeklEQVR4nOz9eZhc2V3YjX/OXWpfem+1WrtkydLIM57NY9mDM4M99oATILwYhzc2JCyGkJgwMOElkMQQwvIjEyZ5nTcBBxKMnReDeW0wNpY9YyPGi2bfrFGPNNql3tfa7627nN8f51apqru6u7rVrVZL9/M8/UhVdZdzz733fM/5rkJKSUhISEhISMjmQtvoBoSEhISEhISsnFCAh4SEhISEbEJCAR4SEhISErIJCQV4SEhISEjIJiQU4CEhISEhIZuQUICHhISEhIRsQkIBvo4IIV4TQjywzueQQoh9wf9/Xwjxb9fxXLcJISaEEP+XEOIRIcT3rdFxdwXXYazF8UJaI4Q4JoT4yWs8xq8IIf5wid8vCCHecy3nCLmxEUJ8lxDi1Ea3oxGh+JYQ4kkhxD4hxJ9sdJuuB6EAXyVCiK8IIf59i++/XwgxJoQwpJS3SSmPXa82SSl/Rkr5G+t4iu8CfhzoBb4POLaO51o1QohBIYQrhNjb4rfPCyEeW2b/XxNCfHr9WtgeDRObYvB3QQjxyxvZJinlb0kpr2kSsBkQQvyxEKIa9PuMEOIJIcSbG37fL4T4rBBiSgiRE0K8KoT4BSGE3rBNMtj/bzbmKkAI0SeE+FMhxEjQzm8JIe5bYvtfE0I4QohC8HdaCPFfhRADtW2klN+QUh64PlfQNtuAF4D/AvwZEArwkCX5Y+DDQggx7/sPA/9bSule/yatL1LK35dSflFK+aiU8kEpZX6j29QKKeUw8DXUvagjhOgCvhf45Hqefx00CR1SyhTwQ8C/FUI8tMbHD2nN7wb9vg2YQL3zBBPDZ4DLwFuklFngA8A9QLph/x8CbOC9jQLwOpMCngPuBrpQz/6XhBCpJfb5MyllOtj+HwJbgBc28BqWRUp5WUr5c1LKv5ZS3i2lfHKj23RdkFKGf6v4A+JADnhXw3edgAXcEXy+ALwn+P/bgOeBPDAO/F7w/QPAlXnHnr/fcWAOGAX+KxBp2FYC+4L//zHwH4L//zVQbPjzgX8S/PZfUINPHjVr/a6G4+nArwBngULw+/Y29osC/xkYCf7+MxBdpO904DFgCjgH/PPgOozg963AF4AZ4AzwUw37tuzHFuf4P4Gz8777WeDFhnP8f8AkcB74ueD7h4Eq4AT99kobbfo14C+ATwft+kkgC/xRcM+Ggf8A6MH2+4C/Qz0/U6gBs9U17Grsl+C7Z4F/1fD5x4EhYBb4CrCz4beHgNeD8/zX4Jw/2dDmT7c6F3Bk3rNjARcW2e/DwEVgGvhVmp/dRZ8JoAf4Iuq5ngG+AWiL9MNtwBPBduPAr7Rx/AeAK8AvoYTvKPADqAnc6eBYv7LE+/3HBO9S8Pn9QDH4/6eBL7UxRnwd+E3gReDRZbZ9c8M1ngJ+eF5b/hvw5eB+fAslVP9zcN9fB+5cwdiVB+5e5Lem+9vwvr4CPNZqzAru+b8CXgVKqOe+P2hvAXgS6GzY/u3At4N7/wrwQMNvx4DfCK6xAHwV6Al+iwV9Px3s+xzQ38b7qQG/jBrTpoE/B7qWO+Zm+NvwBmzmP+B/AH/Y8PmngZcbPl/g6mB2HPhw8P8U8Pbg/00vQ4v97g4eeAM1yA4BP9+wbUsBPu94D6MGuJog/hDQHRzzF4ExIBb89q+A7wAHAAHcAXS3sd+/B54G+lAq9m8Dv7FIv/0MatDZjprl/y3NAvzvUANWDHgrSsi+e6l+bHGO2gTr/obvjgM/H7zQLwD/DogAe1ATifcF2/0aCwexpdr0ayiB/wPBsePAXwJ/ACSDPnkW+Olg+z9FCTstON79i1zDrnn98nagDPzD4PMPoAarg8E9+TfAt4PfelAD9Q8BJvAI4NKGAJ/XBhM1qP72/P2AQyiB8i6UMP294By1Z3fRZwL4beD3g+ObKPOMaNEHaZTw/cWgr9LAfW0c/4GgLf8uOP5PBffs/w2OcRtqYrJnkb7/Y65OhlPBft8IPo8B/3SZsWEHatJ8KGj7q0tsm0RNjP9pcB/vQk3sbmtoyxRqLIihJgbngR9FCdf/APxtm2PWW4Przi7ye9Nz0fD9vweeaTVmocarp1FCexA1YXoRuDN4Lr4OfCzYdhAlLL8X9fw/FHzuDX4/hhK0+1Hv0THgdxrG178GEsF13w1k2ng/fz5o37agPX8A/Olyx9wMfxvegM38B9yPEhLx4PO3gEcafr/A1cHsKeDXCWaTDds0vQzz92txzp8HPt/weUkBHrwIEzSsllscc5arWoNTwPe3ef2N+50Fvrfht/cRrNpa7Pd14GcaPr+Xq6u/7YAHpBt+/23gj5fqx0XO84fAJ4L/vwm1su4D7gMuzdv2XwP/K/j/r9Es3JZr068BTzX81o9SncYbvvsRgkEWZZ/7BLBtmfbvCvplDqgE/3+MQNChVjg/0bC9hhLwO1GD+9MNvwnUinSlAvy/A18iWB3TLMD/HfCZhm2TQR/XnvlFnwmUQPgrgmd3iT74EeClRX5b6vgPBH1W03qkg+u7r2H7F4AfWOTYf4wSdHMogf0FYG/wmwM8vEy7/w3BZB61OvRYZJUMfJBgctDw3R9wVej9MfA/Gn77KDDU8PktwFwb70MGNTn/10ts0/RcNHz/M8AbDX07X4D/44bP/x/w3+e19y+D//9fwKfmHfsrwI8F/z8G/JuG334WOBr8/8dRk7Tb5+2/3Ps5RCDMg88DwT00FjvmZvkLbeDXgJTym6iZ3vcLIfYA96Jm6q34CZQwfV0I8ZwQ4u+3c47AWeaLgWNcHvgt1OqqnX2zqEHy30opv9Hw/S8KIYYCp5Y5lLq3dsztqIGx1fGW2m8rSpVa42LwXSu2olYcjds2/jYjpSzM+30w+P9K+vGTwA8LIWIoVe9RKeUESsBtFULM1f5QZoP+Jdq7VJuYdz07Uau+0Ybj/wFq8gBKrSuAZ4NIhR9f4hpA9XEKeBQ1eJoN5/kvDeeYCY47yLw+lmrkamzjsgghfjo43/8ppfRbbDL/HCXUaqrx98Weif+I0h58VQhxbgnnvEWfx2WODzAtpfSC/1eCf8cbfq+g+nUxHpNSdkgpt0gpv09KWWvHNEoILMWPAv8bQEo5gloh/tgi2+4E7pv3PP5jlJq8xvx2r+Q6EELEUSvNp6WUv71M21sxiHq+FqPd9u0EPjDvWu+nuT/HGv5fbtj3Uyhh/5nAKe93hRAmy7+fO4HPN5xvCCXw+5c45qYgFODXzp+gXtYPA1+VUo632khK+YaU8kdQg/j/D/gLIUQSZTNK1LYLvFh7G3b97yh185uklBmUoJnvOLcAIYSGmkz8rZTyDxq+/y7ULPiHUXapDpQWoXbMy0Ar7+3l9htBvSg1dgTftWIUNTA3bltjBOgSQqTn/T4MS/bjAoJJyzTw/Sj1f80z9TJwPhica39pKeX31nadd6gl29Rin8uoFXhPw/EzUsrbgnaNSSl/Skq5FaXC+28iCAVcDCmlJ6X8T6hV4c82nOen511HXEr5beb1ceBs2djnTc8dzcKidr9/A6WNyS3SrPnnSKBMLDUWfSaklAUp5S9KKfcA/wD4BSHEu1uco+XzuNzx15kngf9jsR+FEO9AaXz+dTDxHkNpfX5kEQfHy8DfzbuPKSnlP1uLxgohoiiTzjDqeVvp/hrqHn1juW3b4DJqBd54rUkp5e8st6OU0pFS/rqU8hDwDuDvo8be5d7Py8D3zDtnTEo5vMQxNwWhAL92/gR4D8rGtqh3sxDiQ0KI3mAlMxd87aEcamJCiPcHM79/g7LT1EijbJnFIIyl3Zf6N1EqzX857/s0yjY4CRhCiH+HUq3V+EPgN4QQbxKK24UQ3W3s96fAvxFC9AohelDq1cVCsf4c+DkhxDYhRCfKwQRQ3qQoldZvCyFiQojbUavu/w1L9uNi/AlK0HegViCg7NF5oeLZ40IIXQhxWAhxb/D7OLArGLiWbdN8pJSjKOeb/ySEyAghNCHEXiHE3wuu4QNCiG3B5rMo4b/UNTTyO8AvBVqF30cJiduC42aFEB8ItvsScJsQ4gcDofFzNAvpl4F3CSF2BJqaf137QQixHRWK86NSytNLtOUvgL8vhLhfCBFBqcUbx5RFnwkhxN8XKl5XoJ5vb5E++CKwRQjx80KIqBAiLa6GQa3kmVtLPga8QwjxH4UQWwCCa/m0EKIDtdJ+AmX/fmvwdxg1YfqeFsf7IrBfCPFhIYQZ/N0rhDh4rQ0NxpS/QK2Cf3QRTcqi+wZt+FPUs/N719oe1P35B0KI9wXvXUwI8UDD+7BUex4UQrwlWOTkUWpwr4338/eB3xRC7AyO0yuE+P6ljrkG13ldCAX4NSKlvIB6eJIoO9liPAy8JoQoory5/5GU0gpWNz+LEpzDqJXRlYb9HkV5VBdQTnN/1mbTfgTl9DQrrsYR/2OUuujLqInDRdSKrlG1+nsoAftVlBPOH6GcSZbb7z+gvMNfRdnZXgy+a8X/CI73SrDd51q0fRdqZv15lC3wieC3lv24RD/8CWo2/mdSShvUaha1ongryhloCtX/2WCfzwb/TgshXmyjTa34UZSD3EmUkP4LrqoJ7wWeCa7hC8C/lFKeX+JYjXwpON5PSSk/j5qcfEYo88oJAgEhpZxChTb9DkoL8SaUjwbB70+gnqVXUbbgLzac492oAfsvGp6d1+Y3REr5GiqC4P9FrcZnaX52l3om3oRayRZRzoX/TbbImRCoRR9C3a8x4A3gwTaOv24EqvQjqOfhNSFEDmX3fR4lAH4Y+Higaan9nUepaxeo0YNrfC/wj1DP1xjqvkbnb7sKaqvK9wJzDffzu5bY54PBszmHej6nUV7r16zdCITt96M0iZOoMeRf0Z4s2oJ6j/IoNfjfcXXCttT7+V+C6/iqEKKAcmi7r41j3vDUnGFCQhYghPg88ONSytmNbktISEhISDPhCjxkAYHqLIqagd+9wc0JCQkJCWlBKMBDWtGFCj27H6WeDAkJCQm5wQhV6CEhISEhIZuQcAUeEhISEhKyCQkFeEhISEhIyCZkU9Vf7unpkbt27droZrSkVCqRTLbMJ7LpCa9tc3IzXxvc3NcXXtvmZD2u7YUXXpiSUva2+m1TCfBdu3bx/PPPb3QzWnLs2DEeeOCBjW7GuhBe2+bkZr42uLmvL7y2zcl6XJsQ4uJiv4Uq9JCQkJCQkE1IKMBDQkJCQkI2IaEADwkJCQkJ2YSEAjwkJCQkJGQTEgrwkJCQkJCQTUgowENCQkJCQjYhoQAPCQkJCQnZhIQCPCQkJCQkZBMSCvCQkJCQkJBNyKbKxBYSEhIS0h5DozmOnhhneK7CYEechw/3c3Agu9HNCllDwhV4SEhIyE3G0GiOTzx1nlzFYSAbI1dx+MRT5xkazW1000LWkFCAh4SEhNxkHD0xTjZuko2baELU/3/0xPhGNy1kDQkFeEhISMhNxvBchXSs2UKajhkMz1U2qEUh60EowENCQkJuMgY74hQst+m7guUy2BHfoBaFrAehAA8JCQm5yXj4cD+5ikOu4uBLWf//w4f7N7ppIWtIKMBDQkJCbjIODmT5yLt2k42bjOYssnGTj7xrd+iFfpMRhpGFhISE3IQcHMiGAvsmJ1yBh4SEhISEbEJCAR4SEhISErIJCQV4SEhISEjIJiQU4CEhISEhIZuQUICHhISEhIRsQkIBHhISEhISsgkJw8hCQkLWhbAaVkjI+hKuwENCQtacsBpWSMj6EwrwkJCQNSeshhUSsv6EAjwkJGTNCathhYSsP6EADwkJWXPCalghIetPKMBDQkLWnLAaVkjI+rNhAlwIERNCPCuEeEUI8ZoQ4tc3qi0hISFrS1gNKyRk/dnIMDIb+G4pZVEIYQLfFEJ8WUr59Aa2KSQkZI0Iq2GFhKwvGybApZQSKAYfzeBPblR7QkI2mjBuOiQkZCVsqA1cCKELIV4GJoAnpJTPbGR7QkI2ijBuOiQkZKUItRDe4EYI0QF8HviolPLEvN8+AnwEoL+//+7PfOYz17+BbVAsFkmlUhvdjHUhvLb1Z6Jg4/kSXRP172qf+9LRVR3zRrm29eJmvr7w2jYn63FtDz744AtSynta/XZDCHAAIcTHgJKU8rHFtrnnnnvk888/fx1b1T7Hjh3jgQce2OhmrAvhta0/j372FQayMTRxVYD7UjKas3jsA3es6pg3yrW1Yi3MBTfy9V0r4bVtTtbj2oQQiwrwjfRC7w1W3ggh4sB7gNc3qj0hIRvJrRQ3HZoLQkLWho20gQ8AfyuEeBV4DmUD/+IGtickZMO4leKmwzSrISFrw0Z6ob8K3LlR5w8JuZGoxU03qpU/eO+2m9ILfXiuwkA21vRdmGY1JGTlhOVEQ0JuEG6VuOnBjji5ikM2bta/u1nNBSEh60mYSjUkJOS6ciuZC0JC1pNQgIeEhFxXwjSrISFrQ6hCDwkJue7cKuaCkJD1JFyBh4SEhISEbEJCAR4SEhISErIJCVXoISEhda5HQZWwaEtIyNoQrsBDQkKA65MhLczCFhKydoQCPCQkBLg+GdLCLGwhIWtHqEIPCQkBVp4hbTWq8DALW0jI2hGuwENCQoCVFVRZrSr8ViraEhKy3oQCPCQkBFhZhrTVqsLDLGwhIWtHKMBDQkKAlWVIG56rkI41W+DaUYWHWdhCQtaO0AYeEhJSp90MaddSkCTMwhYSsjaEK/CQkJAVE6rCQ0I2nlCAh4SErJhQFR4SsvGEKvSQkJBVEarCQ0I2llCAh4SEbHrC9KwhtyKhAA8J2QBCgbN21GLSs3GzKSY9VOmH3OyENvCQkOtMmA98bQnTs4bcqoQr8JCQ60yjwAHq/x49MR6uGOfRjqYiTM8acqsSCvCQkOvMrSRwrsVU0K5q/Fpi0kNCNjOhCn2TMzSa4/EnTvPoZ1/h8SdOh2rYTcCtkg/8Wk0F7arGw5j0kFuVUIBvYkJb6ubkVhE412qbbjddaxiTHnKrEqrQNzGhLXVzUhM4jarlD9677aa7Z9dqKliJajyMSQ+5FQkF+CbmRrKlhmFRK+NWEDjXapt++HA/n3jqPKCe64Llkqs4fPDebevS3pCQzUaoQt/E3Ci21FCVH9KKazUVhKrxkJClCVfgm5gbZYWykar8la78529/EG9d23crsxamgltBUxESslpCAb6JuVFsqRulyl9pBq5W2095VYZGc6GQWCdCARwSsn6EAnyTcyMMkOsRh9vOynqlK/9W2+tlcUs4/YU+CiEhNx+hDTzkmlnrsKh2berthhkttb0mxE2ZQKWR0EchJOTmJFyBh1wza63Kb3dlPdgR58JUkbG8Td5yyMRMtmSi7OpJtTxuK02BL+VNl0BlPmG4YUjIzUkowEPWhLVU5bdrU9/fn+RzL14hGTVIR3VyFYeRuQrvva31yr+V01+vL2+6BCrzuZHCDUNCQtaOUICH3HC0a1M/PV7izu0djBVsipZLJm6yvy/F6fES729x3Faagh4i130Ver3t0bdyrvDQ9h9yMxMK8JAbjnbD44bnKuzsSbK796rK3JdyyZXlfE3BsWMja9z6pdmI2tU3Srjh9WY1UQqhsA/ZTGyYE5sQYrsQ4m+FEENCiNeEEP9yo9oScmPRbgKPGyWRzUrYiNrV8/vTcT3ipsYfffPCTV0AZyV9HTr6hWxGNnIF7gK/KKV8UQiRBl4QQjwhpTy5gW0KuQ60s9Jpx6a+GVeWG22PzlWqXJ6usL8/xc6e5HXRAGwUK+nr0NEvZDOyYQJcSjkKjAb/LwghhoBBIBTgNzjXo8ZzO9woiWxWwkbYoxv7PFd2QMDpiSKpmEFvWgm4tRRUN4oqeiV9vdETq5CQ1XBD2MCFELuAO4FnNrgpIctwrQJ4rVc6N0IimxrtCK710hrUzn1yNEeu4pKJGby3y65/X+vzgq1+s12fM5MletOxtgVVO9e3ETb+xVhJX9/Kjn4hmxchpdzYBgiRAv4O+E0p5eda/P4R4CMA/f39d3/mM5+5zi1sj2KxSCrVOv54s9N4bRMFG8+X6Jqo/1773JeOLnusK7MVTF1DXN0dKcHxfLZ1XttgaTkeecul6vpEDI1MzCBm6kvucy33rfF8QoDrSSKGhiYEvpR4vqQnFVnQhtW0c7l2TBWrSCmxXB8pJZ4PvTGfWUdHF4Jk1EAIKNouUkoEqo2ZuNnW/audQ9fEktd3rc/HSmjn3rXb1+1e3/XiVhlPbjbW49oefPDBF6SU97T6bUMFuBDCBL4IfEVK+XvLbX/PPffI559/fv0btgqOHTvGAw88sNHNWBcar+3Rz77CQDaG1iCBfSkZzVk89oE7lj3W40+cXrDSqX1+5KH9q25j48qvcbW13Mrv2LFj9B+4c8Uq3/nne+r0JAXL5cjeLnpSsTW9ruXaVuvTodE8c+UqU8UqAvjJN1n85ViWyYLNO/d2s7s3xVTR4oWLcwBkYgaHtmbb6qd279u1Ph8rYa3fuRtF9Q+3znhys7Ee1yaEWFSAb6QXugD+CBhqR3iH3Bhcq+f3WqddrbFa727L8VblfTz/fI4nSUV1zkyU6ttcqw11pSll85ZDwXIxNEHEEEiphHQionN6vEiu4tCVjHKgX60QMkH721Fvt5u2djNGBtQ4OJDlkYf289gH7uCRh/bfMKaZkJDF2Egb+DuBDwPfEUK8HHz3K1LKv9m4JoUsx7XacNfL8Wy1Tkh5y12VTX7++VIxA7vqkrec+neNgqtxdRfVBRKoenLJld5KUsrmKiqV7FjOIm7qeL6PJsB2fQayMbIJdZzhuQq7elL8zAN7V9Tn7diIh0ZzTBUsvnFmms6EycGBNDHTuOEjA0JCNisb6YX+TUAsu2HIqlhrdWDj8eKmhuN6jObcNavxfK3tXa0TUtX1SSfbL4iy2Pn29SZ55twMqZiBL2XTxKZR3W7qcOzUJKWqR2fS5I1xk1evzPFLDx9YcL3tTkpqk6otmShvjBcoV726Ldd2fXZ2Jdjdm2pSdQ+N5nj8idNt9/dyE7fGa3znvi5OjhT49tkZ7t/XvWlD1G4klXpISCvCamQ3IWudlGK+mjli6JQdn5+4f9eaqBrXor2rVc1HDG1VKt/554sYOju6ExzemlmQfKZxJf2dK3lKjoehCSzHA+DSdJlPHb+44BztqqNrWo1dPSn2b0mja4K4qaNrgv19KXRda+qH1fT3csl1Gq+xLx3ngQN9fPeb++hNxzal0AsTu4RsBm6IMLKQtWWtQ7VWq2Zez/a2Wh2tRjWfiRnkZpTaeyUmgVamgFaraGheSY/lLSK6wNA0qp6vPJyl5KXLcwv2W4m5olGrUeubSPEsuztSC1aOq30+lgrZu9niqMPELiGbgVCA34Ss9WC6WjVzu6y0vUvFGq/U4ztm6qu2ybcbg75AvS9VaFXE0GofES2sSav1F6i169ixET70wML+WA9he7PFUd9sE5KbmVvZ1BEK8JuQtR5Ma2rm9RqcV9rejUwGs5rBonElvSUT5eJ0BUMTbE3HsByPou1xZE9XW21rx3Zda2NvscLjT5xmf3+S0+Ol+j4RXaz5/WzHRr6ZBtmbbUJys3IjJQ7aCEIb+E3IWodqZWLGuoR+rba97YY0rTWrtYs22o+3ZOP0pCL0pqN4vsrBsLsnyYeO7FyT8zduY+oa5yeL/M6XT3FhqljfZzxvc3G6tKb3s5WN/D0Hezl6Ypyf/ORzPPJnrzS14Ua3J69XuGPI2rIRxYFuJMIV+E3IWodqXYuaeT3au1Gro2tZ+beyUa90NdrO+Ru3ERaMFWySUYOxvM2unpTapyuB43r1sLK1up/zr7G2MspXlI/BqXGVf72W6OZGtidvxjz7tyK3uqkjFOA3KWudI3y9c46v5PgrjUVfK/XtWg0Wq+3Lds4/f5ui5ZKO6k3x6emYwWjOvaYMccvROJEo2t7V/OsTJXpSa5t/fb24kfLsh7TmVjd1hCr0kE1Hu/XCYW3DgTY6y1g755+/TSpmULA9MrHrO8A1mjlSgfCOGlp9ItFOG8JQrpDluNVNHeEKfJOz2ZyD1op2V0dr6fC2nvXH16qSWeM2W4Et6SijcxUO9KcWJJhZT2orI8fzKNsul2bLRDSNLZlofZBdrg2L3btPHb9IbzpWd9K7Hs/8ZnvPNlt7V8utbuoIV+CbmHCFsjxr6fC2kpV/IzXP8Uc/+wqPP3F6wf1p9z62c/7GbRzPZ3dvil/+ngOkogZfenWEL746wsuXZvn08Yvr+pw8fLifyzNljp+dQRPQn4rg+pKpUhXH9Vadf91yXL55ZrrupHc9nvnN9p5ttvZeK7dyDvtwBb6JCZNNLM9a28hWahddKswF1L366skxIrrG4cEMmjCXvI/tnH9+HPjQaI6/eGEYgWAwG0MCx8/NMJa3efR96zPgHRzI0p+JMlW0cTxJZyrGvbu7iRg62bi5uvh5YGi0QGfiqpNeNrb+z3ztPau6Hs+cz1O0XExd8KnjF/mtH7x9Xc55LYTjwq1DKMA3Mbe6B2Y7rKfaux2WUgNXHF99liCl5IWLc9y9s2NFTl7ttmGmVCXVUAtbCMFU0W4a1Nda7Vr1JO/a37ugtGi711W7d7Mlm9GcxXSpylTR5h3zYubX+5kfnqtgaPDy5RxRQyMV1bEdj2+emWZoNHfDCcVwXLh1CFXom5iNdqpaLcuplNeS1aq914rFVPgvXZ6rC/Z03EQIQdTQ6uVI1/I+Ds9VqAZOZADlqstkweb8VJEnTo4zNJpbF7XrtTyftcnEaK7CN89MMzJn0Z2M0J+OcWq8xFTRWvExV8tgR5yh0QJRQyNm6gghQAg6EzdmvPFmHRdCVk64At/EbPTqcjUslzlpPZxvrlc4UKu2D3bEOT9ZZKxgU7RcUjGDLekoAlEX7Pt6k7x4aY6oLshVqm07ebXLYEecN8YL2K6PLyUjc0r4xUwdUxd84qnzJExtTdSu88umjuQsdnYnV/R8Nj4jvi/pTUcB2Nen6pg/c26GE8N5bt/BmvdVKx4+3M/nXxqmI24gpcR2fWzX584d2RtyVbsZx4WQ1REK8E3MZvPAHBrN8bEvnGSmWKUrFWFfb5Le9NWkHsANkxZxpROJxSYmb96S5KXLcySjhorHrjiMzlV4y2CGS9MlxvI2ectR1clcH11TgnQt7+PDh/t59cocl6bLFGwHKSW+hHTM5LatGSKGzrPnZ3j3wb6m/ZZTu87vo/39SZ4cmqz3QcFy0YSgusLSs41mh4LtXo0hnyxxZE839+7u5JUrORzPJ5ta275qxcGBLN+1r5sTI/mgPSaHBzOYuk5f2lz+AOvIYs/pZhoXQlZPKMA3OZsl2URNwE0XbboSJrbj8eKlOe7a0UF3KsrwXOW6Ot/MzxfeKKBXk195sbZ/7fUp7trRURfU2bjJgf4Ujuvz4qWrgr1ge5SrHr/8Pft4/+2Da35tv/TwAf7vJ9/g66cmAEEmZnBwS5redAxfSiRyRfnRG/vI0ODYqQn++FsltmRj3L2z86ozXleCbNysJ41pJ5d7ow03EzOxHI+ooVEM1MIx0+C9h7awzWxdrGU9+NCRnfXrvVFWtcs9pyt9Z26V0LObiVCAh1wXagKuJxXFcry6M9WZyRIRQ2ewI35NzjcrGXwaB74+oYTP518a5v593Xz4yM5VTSQWa/t43uK+3V3s6knVv/el5GtDE9y5vaOuWs/ETfb3pTg9XuL9y17t0v1Qu7atDWFW7znYSyxi8OYtGaSUCCE4P12mMxkhYujcub2DXMVpchgzdY2Pfvfeludp9MyuOXcJAbPlan1i1ptudsZrd2LU6H2+ry/JCxfnsF2/KSf/B+/dxvipkWvoqZVxLava9RKMaznhvdWLgmxWQie2kOtCzZlrX18S2/WxHI+ILpgpVuuZk1brfLNSB6zawOd4atUL0Bk3eG0kzyeeOs9rI7kVx44v1vb+TKzl9xLJzp4kR/Z089Chfo7s6WZnT/KabapNudAF9f9/8vglsnGl+q16qohKVBe8NpInV3H48JGdvOdgL6fGisyUHLqTEfb3pXhyaLJlP9bu55nJUt25KxExcDypnPEmFzrjtVt4ojG7VlcyyoF+NfnJBNtvlFBZTbzxesZkr2WOg1u9KMhmJVyBh1wXaquqnlSMu3d2cGaixFTRpjsVbRqQV+N8s9KVSG21/Oz5PHu6lDOXlJKifbXAx0rLbS7mOPRjR3bw5NAkM0WbsbzFTEnZu3f3JBac4+JUibGCzaOffWXVK7XlNAGaMOv9n6tU0YRW7/+jJ8Z5+97upjblKk7Lfqzdz6LlkooqbUoqqmzVlarL8FyF6aLdtIpvV8Myf7W7qyfFzzywd1OuBNfTLLSWOQ7C0LOV00qzcr0JBXjIdaFRwHUloxwc0MlVnCbhvVo15UoHn9rAl7ccBCpG2XZ9UjGDdMwgG1eq2tpxasL43l0di9pvl2v7x79+Fsfz6U5GGMjGKNoeF6dLdQ/ti1MlXrqsVM+mvlCtv5Q5oPGc0UVqfaejBk+dnsTxJKmYwb7eJBEj05RUZSX9WLufpi6wHQ+EQNMEBwdSnBguIIDuZIQtmRhPDk2ypze1QOBMFS1ODOepev4CP4SN8u1Ya3X3egrGtfQ2v9WLgqyUxUwO7+vyrms7QgEecl1oVzivZuBeavBpNSDXBr6IruFLieV42K7PbVszFCyXQwNZHj7c37Tfvbs6mjysW9kIF2v76fESb9+zcGXbWNJzrGBz144OUjGDFy7OETW0JrV+K7Vxq0FkJGepxCldCbYG57k8UyYR0ZkqVjE0uDBlcXIkR08ywr96+MCi/ThZsHhtJI/jSR5/4jT7+5OcHi/V++Q9B3v5lqnxzTPTdCZM7tyR5eRIgVTUIJtQVcjGCjZb0lGOnhhvEji26/LMuVkEcO/uzhvC5roeduD1FIxr6W0ehp6tjMU0K/l55rL1ZlkBLoQwpZTOvO96pJRT69eskJuR9VpVLTb43LurY9EB+SPv2s2nj1/Ec2YBeOv2LBFDrw9a89v6+BOnV60KXWwV1ljS89HPvhKo9WfqNuVGtX6r87QaRHZ2J6kGEwOnqMKsJnTBlOtjux6jgXNaIqITMfT66vhgMGmp9aPluDx3fhYJ3Lenk/OTRT734hXu2tHBju4kuYrDk0OTfORdu3nnvm4+efwS3z47Q65cJW7qGJpKTGM7HqfHi5Qdj0ce2l8XOE+cnCEdMxjsiHF+qkzecojoGp8+fpHf3KD0pOuh7l5vwbhW71QYerYyFnunqyX/urZjUQEuhHgQ+BQQFUK8BHxESnkh+PmrwF3r37z1IQyXuLlYbPBZakB+5KH9/OYP3s7RJyZ5oKuP4bnKkvHX16IKbWcV1qjW1wScn6owV1YlEq/MluhJRRc8p0tNDB4+3M93nlcOec9dmKE/HUVKSETUK9+biiIRTZODxn585vw0qZjB4cEMPakYxyenSUYNxvI2u3pS9Wv59PGLlB2fQwMZ7tvdxZ8/d5mi7dETeLrHTB3b9esrk9p5hucqmDq8dEl5sKejBpbj8Y0l0pPOf28Psjbqytpx//LlYfrTUfb1per5Ca5V3b2ZBONmCUm9EVjsnd5uXF+/8KVW4L8LvE9K+ZoQ4oeAJ4QQH5ZSPg2IJfa7oQnDJTae65Vt7Y++eWFZoRszdR5pI5b4WlShKykD6nmSy3MVbNfD88HQwXJ8JvI2j33ldFPxkcXaVLIcHvnMK/zgVoeTI3mQMFGo4vk+EnA9yYVplz29qQX90ShgB7Kxeh7zqYJFpeoxmlPb7utL0pWM8rWhGd62u6vehmSQJ3wib7OzWwlvKSXZePNQM9gR59ipibq2AVR+9lp60nbMBVNe9ZpzkTcetz8dJW+5HD+rJiuelKrIzNbMqo8PoWC8GVnsnc50XV+r9FLThYiU8jUAKeVfAD8AfFII8Q8BeR3ati6E4RIbx9Bojl/93Kv89Kde5NipCQyNdS11uJY5oWuhTecnixw/O8WXXh3l6bPT7O9PLrvvYvnYgXpO+KMnxnnPwV6ihkbVU4LW1MHUVXx1xNDqxUfmtylXUSv1XMXh4nSJ18cKIMDQBBXHQ0I9ZM71fDQBjicpVBwuTZda9kdj300WLHIVl4rjk4yolfILF+e4NF1CIptCmXrSMXozUTwpKdguMVPnzVvSHJonwB4+3M9s2QEpkQ1+CAcH0i1XvK3eW10TS7637eTcbzzum/pT2I7HVLHKRMEiogmKlstIzrppS3GGrI7F3unaZPR6sdR0wRFCbJFSjgEEK/F3A18EWmd32ASE4RIbQ22lc26ySEewGnv5co67dnQsauO9VtbS/nhwIMt7DvY2eZMnTI2Pf+0sf/nySN3xbbFrmL8Ka7WifHJokv5sjILtMJ638Xwfx5UIDSqOR65cXbBanq+e3ZqNcWmmTCZm4Po+RcvFkxIpldpMIpBAJmYQixicGi/yMw8sfJ0b++7MRJF0VGeyaGO7cGW2jOtJRnMVupMRnjo9yW1bM/SmY+zrTfJMwaYzYZKOGkwXq+TKDu+9rTnE5uBAlvv3dfPaSJ6i7ZGKGfW0ro0ahRqt3ltNiEXf2y+9OszHv3YW15d0JU2qjscnniovyLlfU5u/qT9FTypGMmpQsl1sVxKNGNw2qHwjwlKcIfNppVkZP3V927CUAP9loB8Yq30hpbwihPh7wL9Y74atF7dSuMSNZOuvZ+7yfNJRQ1V0QmViu29317pMoNba/tjoTT5ZsHjx0hyW4/LacJ7zk2W+cmKMj757b1upUBezzytbfIR82aHkKBW6lBIJzFkuEb3ZejV/EHn0s6/QnYwwlrOoZiRCAL5SmQkBnQkDx1MTmkzMIBNfvDZ3wtR49vwMw3NluhMm2ZiJJ8FyPSzXQxOCA/0pTo2XeObcDPfu7iRmGnQmTaaLVaZL1XrYXKOzXI0PH9nJ7x49xUypSqHi8NpInq5khF9q8Iyv0eq99aVs+d4Ojeb4+NfPgoCupInt+pyeKLK/L9WUc9/zfBzX5/WxAmcnS9y3uxNPSgayMaIRgyN7uuvnCSf4ITciiwpwKeWTi3yfA35z3Vq0ztwq4RI3mq2/toKq5baOmXo9v/V6TqDW0v5Yc7x6+lyes5NFfM/HQ60Et3XGyVsuH//62QWCarFjtdIEZeMGftTg3GQR6fv4QsOXgoguSEV05srVJXOJD3bEcVyPV67MgZT4V5feaAKKtsf33bGV3nRsgUAE9dx8+vhFvhGEht2+LYOpCy5Ol+nPROlIRLgyW8bUNHRNMFN2ObK3ixPDeV65kuO9h7ZwoD+NOdi8kl4sIUzNxi4Dq1xj7fBGWr23vb5smTzj6Inxupak5kgHMJa3iJhqNe15Sqgno8pO7/mSp8/N0JOKUnR8bhu82s6bdYIfsvm55VKpLma7uNnUYzearb9mU21MpWo7HqYu6qlUb3SiuuCZc7NYjvJ+tlyfStXD1ARCqAIhjue31ceL2ecPDWR59H376U1HMXWB4yknsLips7MrzomRwpJpOR8+3E/R9nCDdKmeD74EXaiX3ZfQnYrWbeeN/V6b9J0YyTeZObZmY3i+ZLZURUpJuerhS0lvOkLeUtn13rW/l9u2qlSjttdsF58qqrjzv3x5uMkWffTEONu7EjxwoI/33TbAAwf62N6VaNl/rd7bnlRk0YiB7mQE270a0hM1NGZKTj3n/ljeImpodCWjDGRjJCJ6PQ3sju4EEUOv+xVslucz5NbjlkzkcjN4hS6nHr/RbP21FZTv+xgCLs2U8aTknXu6l5xArcYMsF6mA8nV8IuIrlG4urgFVDa37mSkrT5eShN0cCDL2/d088y5GVIxQ8VTuz6vjRXpTUUWjVOuXfe5ySJa0FBdA0PT0AS4wWRgaDTPbVuzC8wJi5k5pssOO7rjjMxZFGyXREQnFTXQNY1ksLptXKU2qrunihYvXJxT1xnVmzLMTRZt3ryl2cM7HTM4uUjFsvnv7bFjrYuZ1LQQp8aLgBLeecvF0EQ9Qc/Ll+boStY85w10TWlRdnQn+Yn7d61L2NeNZNIKuTm4JQX4Zqcd9fiNZuuvO4EFjkVv6kuxJRND1xdXAq3GDLDWpoPGQfe1kRz7+pLMVlyihhKKtZVazYt6V3di0Qxwjedfzj7fODGo4bo+kXlxprVJWeN1R02NLdkYkiIaSoirkqGC+3Z11lfK82ll5vB8n3OTFWKGhiYE+/tSxCM6z52fpeC5HNqabqoQBs2TkzcCIVqoVCnYHrqmzAHPXZhBFxoJU2+q1HZpusTl6QqDHYlV3z91/jIH+lPNldXefTWf+ldOjJG3rtYab7x3K53gtyOYbzSTVsjNwVKJXP6aJcLFpJTfty4tClmWdjJG3Yi2/tPjpbaLZcDqMmOtZ4nFN8YLnJkosT+ojlV1PaZKDhGpso7t6k6gaRr7+5NtDdZLCYqJok3c1Oqr+YFMjJ3dCWyn+ZWsTcoarzsbjxA1dDRRUnHfvgop29EV4y3bO5fNEV8r4Wk5HuN5C13TSEVN9vZGODVWZHt3nPv2dCEA25P0pZsT4DROTsYLNqmg3nlE14gYgkrV49JMhY64wTfP2ADs6E5SsFxOjRfZ35+6pvvXeH7T0Dmyt2dBnvWPvnsvH//62bqjXe3erVRV3q5gvp617kNuHZZagT923VoRsiLaUY/fiBmgVqrWX40ZYC1NB/MH3cODGY69Psnx8zPs7IzTnYoCgoihkU2Y7OpJ1VW01zJYD43muDxdAQF7epL1FWIyojGWr/ClV0fpSpp1DcYH793WlLSmJoB1TZCKmuzoTmC7Pnfv7FhSC1Ob9GXjKq/514Ym8HzJYGeMO7ZlmxzfGnPFt6JxcnLs1IRaeRuaikd3fISQaEIQNwWnxlSq1UMDWbZ1xtnZ0xxbv5r7t9wq+v23D7KnN3XNKu127/WNZtIKuTlYygv9765nQ25EblSbVbvq8bWw9Q+N5pi4xhKXK233ardf7T6NzFeZ37EtWz+WlFD1fMq2y3DOYksmxgNv7sXUlcd1TS3dTga4pTh6Ypz+TIQTIwXG8xYxU0NIGJnzuGtHB+Wqx3SpSt5y+eh3K7Vw43XXSrbq1TxeoES7c0cWU9eX1MI0T/pculNR7tiWpT9zte/SMYPXRnJcmim3pQ5++HA/n39pGFMTuL5PueohpY8mNKZLVXrSUQ5sSbGrJ8UjD+3n8SdOXzfTz1q8H+0K5hvNpBVyc7CsF7oQ4k1CiL8QQpwUQpyr/V2Pxm0kNdXYUh6/G0WrDFzr4Slb6wPPl9fcB0OjOaYKFl9/fYJjpyYYz1eWbfdqrnOxffb3J5fNyjX/nkd0jefOzzJZsOpx39UgPGmwI47rK+E4f8C+1gxwr43kGM9X6UqaxE0dy/GZrTjETY07tndyZG8Pb9/TRdTQ+K9/e7ZeKazxuk1dJx7R+e1/eJgHDvTheLQVcXFwQNnHH/vAHbz30BZipprjTxYsjp+b5svfGePkaB7P89uKcDg4kOW79nXTmYhgOb7qMxl4xmuCVMTg9bECJ4P7sR7PdjsZ2VZLu/e63euqtfXKbGXN2xpy89FOGNn/Av474AIPAn+CKnJyzQgh/qcQYkIIcWItjreW3GhhWI1cr1C4Wh/omrimPqgJRtPQeee+LgC+fXaGqust2e52UpDOH+Ra7fOeg708OTS57GRs/j0/PJhBAq+N5DkzoZyxDE0jEzfxpQqr+spr4zx1epJoQ4KVxsF6olDh2KkJvv76BJOFpVNyDo3m+JXPvcq3zkxxdqrIldkKpary+taDettA3bNbSiUMa1nc3nOwd0GY1ftvH6wL5EceuppHvR2h1pg+9sWLc+QrDrqmQtJOjxeZLFj1bW3X5YmT4y2P96EjOzk0mOXBA72YusoEpwkl/DqDWO1c5Wqxk1b37+iJ1sdejvWeiLcrmNt5ZxvbauraDbVoCLkxaccLPS6l/JoQQkgpLwK/JoT4BvCxNTj/HwP/FTUpuKG40W1W1yMUrt4H9tXvVtMHzXZCk74DV9WJy11DOylIl6vLvVQp0Dsb8pjMv+c9qRj37enklct55io2/ekoe3oSDI0WGM/bmDp4Hk35smvnrpUr/daZGToTJu/Y20XE0BdVNX/p1WH+49FTTJeqOJ6P66uiI36wYq26vvo/cGaiRDTwRk/Hjfr1nB4vNXmXLxZm1a7jVe06PvaFkzi+T08qyr6+JGcmSuQqDmcmS/SmY0wVLZ45N0s6ZrQ8XqNqPhWdIWpIupMROhLK2x2pUru2uudLtbUdrofzWNzUeOb8NALBnduzi05Kl3tnG9sqLMjGQke3kKVpR4BbQggNeEMI8S+AYaBvLU4upXxKCLFrLY611oQ2q6t90Fh7bjV9sJ6OZY0Dcu3f+T4LS53/zt6r30V1wVOnJ6l6PpmYyb6+JFHD4KFDajVVex4uzVQoVz2qnk88qnPfnq4F+bIPDmTpScf47jf3Lch2Nn9ArqX+LNge8YhOxfHQUKlPXV9lU+tJR7Dd2gqvSlTXsD3JbUGlrMb+rNnxe4tKDTvfb2E5oTbf9yMbN7hvd1dTlrTnL8wyU6ziS8mJ4TwCuG1rpq6pmX+djcLr/GSRsYJN0XJJxQx2diXY3Xs1lGyp+111Pc5NFvnFP3+Vf7LHXrYa2XpOxBsnF+852F+P9FgtN/qiIeTGQ0i5aKSY2kCIe4EhoAP4DSAL/G5QVvTaG6AE+BellIcX+f0jwEcA+vv77/7MZz6zFqddFiuoSlRTH/tS4vmSnlSkZcWZYrFIKtV6ENqs1PogoTk4IrpsHyzGRMHG8yW6dlUA1D73paMratOV2Uq9QlcNKVVbdU20vF95y130/AnhkEqlsByPsbxN1fXRhYqf9KVKAtKfiWK7PhMFGynB830MTQOhfnd9dS6A7Z3xet8s1lbH89nWeXUSNFGwmSs7eL7KXe75qkIXqBKbmgBDV6lLY4YWqJtl/VrVdqo0ancyUn9uY1SxiCy4Z1dmKwhBPYWorqkwOCmhJxVZ8NwXbZeYoRFtuOe24+H4qtxmxfFIRHRMXcP1fKzguALY1tAf0Pxe1eLnXV+Sjhp0JRc+V4196HjKCU4DfKA76lP2zSWfx7V89tb72I3HM30bR4uuWVtvJG7GsbLGelzbgw8++IKU8p5Wvy27ApdSPgcQrMJ/TkpZWNPWLX/+TwCfALjnnnvkAw88cN3OvRIv9GPHjnE923a9GBrN8Z3nn+Y5a8uqvdAbVyr1mHTLWZXdvu6lHGuOJT85lefQQGZBjPkkJg/f089jXznNVNHGdj2ihk5PKsqj79vP+KmXeOCBB9RxTYeq8DgzWaJouZi64LatGd52eGe9+MXZqSJnJ0r4vqQvE8XQBNlEBCklQgj2yFT9uua3dbJg8dpIHseTPJTqr/flo599hUuzJS5OlwFJKSj76XgSQxf0pKLcs7OTlGFQtn18fF6+PMdsyUHTBL0pE08Kdvck2eJHMYOKXoPWecZiu+v9UKt7/qufe5XjZ2dIN2R5K1guR/Z00UOMnN6seTpfKnJ6pMjb93RfvX+O03SdwxUHx1VlRqOGpmYq8/qj8Xn41PGLfDPIt35oa5qoZpCbWfhMNPbh8XPT2EEa25ip839szXNGbGu6tqWePdt1OTlSYLbscP++bj58YGfbYX2txoFHP/tKU810UAlzRnMWj/2DO5Y97lJtPSiuMCS3rfo9uZG5WcdKuP7XtqwAF0Lcg3JkSwefc8CPSylfWOe2bTg3Q8rVa+XgQJbxdHRVA1LjMWo24a8NzSCR3Lm9Y1XHWixBTTZuNOXfrv1eUz/6tRVtYA/w52meaupLTZj0pmP1bUZz1tUUo66H58PO7gRjQYavqKFjGhq6pnH3ThWqVVMdN7Z1dK7MMxdmcYOMXxemivXyllFdMFd2KFgOnoSoIeorr4FsnNsHs2iahoS6OvncZAnL8al6PnMVly3ZGBeny7xyZY6HDvUBVwXwfDVsY0rYGrXMb63UuDt7kpQdj2zcZHiuQlQXxE2NP/rmBQY74uzvT/Lk0CTnJov1amm2J7lrR+tSnAcHVEx5O+aFxj6cLFiULBfL9dnRlcD1fNKJpVXMq/FHaGQpG/xam9kafQWcok82ZW547oaQG5t2vND/J/CzUspdUspdwD9HCfSQkBVRdnzetruL9xzsrw+gK/WwXcyb99BAdtFwnqMnxtnZneSBA32897YtPHCgj53dySZv+qXCgYbnKqRjBmcmlfNYRyLC9q44SmuuinvcvbODnlSsSVjW2lp1PY6fm8EQgl09CXRd49R4Ed/3+fTxi4zkrCBhSpyEqVGp+pi6xo6uBPv6UuzuVavYakOREFdKdnYn2JqNUfV8NCHoTBi4nuSZc7NMFa0F11Gj6knu3d1J1NQp2h5RU+fe3Z1UPbloP9TSrz54oIcXLs7x7PlZLk2XOD9Z5C9eGGYyV+H1sTynxguMzlXY05OgNx1b1IZb69NGWm1b60PH9ZguVvElbO+Ko2uCUtXj0nRpWYHZ6I/wwIE++jPxtiMqlopGWY+Qt1oY37bOeFPUQEhIK9oR4AUp5TdqH6SU3wTWRI0uhPhT4DhwQAhxRQjxE2tx3JAbj7UMy6utbmvC9eiJ8QVx0I2DaSthYTkuXz05Vo+3XWr/mlArWioHesl2mSxU0YLsYhFdoyelVq2tVmBnJktICRFD1MtbRg2N0ZzFS5dz7OxOct+eLjqTUQY6EuzuSbK7N8Vbd6i85TWVbaNwzcRMZstVzk+XsB2fqaLNXMVhW2ccAZwYziODELNGoTI0qpKwPHdhFoFK8HJkTzcx06ivpp8+O82XXh3l+Nkpzk8W6/u3qrP96nCO167McWKsQEfcJBU1qHqSodECU0Vr0RVpu/HTX3p1mI994SRf/M4YhiYwdYGhaXUv/FPjxbYEZrsThpXsd73COUNCFqMdL/RnhRB/APwpSsv2QeCYEOIuACnli6s9uZTyR1a7b8jmYi09bFupNWtx0KfHSwtSx85XdU4WLJ47P0sqZtTjbZfaH1Bx7LpgtmQzWXJAwtZMlIlClarnMJ6vEDONpkxntXbOFKukYzpVVzIyZ7G1I4YvJZcmyvhScnJEZ19fiiN7ulXCmCAP+XyVbaM6uTOhMqI5rq8KcjjKjn3/vm4ycZNnzs+Qq1T54hsjZGIGnz6u8Y593Tw5NMmWdJR8WQn25y/M8uYtaTRN495dHTw5NMmBLVeLgDRme3v8idML6myP5iqUHI+IodObjjGaszA0KNkuJ4bz9KSiVDPRBZn82snV/6VXh/mdL5/C0AS24+JLKFgqAUwmHkEP6rC3IzBXq+5ebr/QzBaykbQjwN8a/Ds/7vsdKIH+3WvZoJCbk7W0Fy4WBjU/DrrGfGHx2kgeicptLkSp7mS22P61ldanjl/ki6+OEtE1tnRE0TUNX4Kpa7xyJcd7D23h3l0dfPr4RV66nGOiYNGZMImaOkIY2K6DAEbnKlRdiaYLtnXEyVsuL16a464dHZyZLIGAnlS0rqmYLdl87Asn2dGVIG5qOK7H62MForqG40lKVZ90TNCZMHn5Sg5dqBrrUsJgNoYEjp+b4Vtnp7l9MMvu3hSpwCQwU6wymrf59e871NSvtQphuYrD6fES76e5znbN69vzJY4nycY1klEVBz5TqpKrVLk4XWI8bzFVjHFwIL0gPny5XP2fPH4JQxPkLVd5xUvwpGS6VOVd+3uJ6yVu62hPeD58uJ/fPXqKmVKValDVrSsZ4ZcePrBg20antaguGMlZ7OxOko4ZXJwqcXpcFXRpFaIXEnI9accL/cHr0ZCQm5vVVEdbzPt3/mp+qmjxxniR8YJdP1erhCS1Yzme5L49nUrtHZiKl9MGHBzI8ls/eDuTRZtc2aFguyRNncODGbqSUUZzFg8fVt7u56dKpKI6ruczWagSN5XKtytpUrJdpooOiYjBO/Z00ZmM1Otln5koMlNS2c729amCHm+M53n63AyOJ0GqlLb5wJFrW2ccT0quzFpUHJ+qW0UiMDQVeuZJiSdVvWshBOcmizx/cZYzkyVSMYN9vUm6d3cxmrM4OJBdNof7YEecqYLF2ckSvpQqzWtVhYEVbY8rs2U6ExGSUZ2yoxPRr4Y/vXw5x107Oupmk9rKdSnhN563qDoehiZIRtUzowVhdq+N5Dk0KFdkb655i8sgP3yj93iN+doddU5B1fUYGi1zZbbCgS0pdnQn6xOS+ZqbUKiHXC+WKif6ISnlp4UQv9Dqdynl761fs0JuNlZaHa1d799aWlGA/nS0rdKdtdCkRtrVBhwayC7QJOQqTt1Zbqpok44ZxEydZFSptn0JqZhJImogpSATlzx0qI++tDrf3Ts76hOQ/kyMgUyUnlSQ4ez8LL5UgtR2fU6NF5EyyNAmBMmIwfYuJZwrrqQnHSVmaMyUqkgJZyeLdCYjxE0N2/XQqmqlfnGqxKmxPP3pGPfs6gSW15Ls70/yuRevkIkbWFWPuXI1qJKmo6Fiw6/MVvB8n3hEp+r6jMxZREyNVESt+O/b3dW22aQ/E+P10TypYAKSjhnkgnj5C9MlxAoq4x49Mc72rgSHB68+E61K2bbU7nQlyMZVdMK2zkTTb7Mlm49/7Sxv39sd1vkOue4stQKv1fRLX4+GhNz8rMReuFS2sMbV/Bvjxfo++/pSTdstdq7G/bdy1dGrnVrpS2kS/uibF8hXqtiuVJ7hQNXz0KWG50sODWTIVRwSpoZpXE080pOKYeo6R4IynbV82G+MF3E85ZHemORkeK5CKqpjuz4AcVMnYuhowufBA728cjnHyNxVL3TPk1wpWkR1HSQM5ywiukAXgomCzXheZTRbTktyerzEnp4Eb0yWKDs+miboT0fpTkWRwFjeQggVpJYwdQQCicTzJFNFG9f3l50oNWpdehImXlDBzNCgYLvYniTwX8NyPH736Km6GnypnA3t+mAst93830ZzFq4v1zVVa0jIYixVTvQPgn9//fo1J+RauFHLn66GpQbSxtX8eEHlKN/Xl6rHb7fjHJcwNZ49P8PggEPVWLqoSo1a/xYsh9PjBcpVl7hpcOd2tV9UF8yW3XpmM5VRLYi5FmpwrwnDxQTl/GtLRQ2SUbWahyDzmyfJxiMcHszwyuUc56ZKVKoeyYjObKnK8FwFy/XxpaTq+ip+3fPRTIPeVISC5eJJlQwlZmps70pw9MQ4jzy0f0ktSa1KWm8qyrYOjTcmitieT8F2+P63quvypeRPn71ELGKQjgtG5pRTmyagZHtLTpQWqK+jBru6k1ycLlG0VUpZjauqb9+HS9Nl/u8n3yAWMZbM7d6uD8Zy283/bbpUpTsZaTrGjZr+9GYaH0IU7SRy+STwL6WUc8HnTuA/SSl/fJ3bFrIC2i1QsVlYiffvSpzjGvvp3Qf7SFVKVCx/2fY07teTinB+soRAcMf2DGYQ025VXVJRg6Lt4no+CJUKNRMz+L0fvqPpPrznYC+fPH6J8bxFfybGjx3ZsWje8NMTRZVy1PMZy9u4nk++4vD6SI7RnIXj+UR0gRTwt6cmlZNWsMD3JZSqHqkg69pcxUVKSdTUSUUMOpORBbHriz0vecuFIGUrQDyiU7Y9ZZ9v6PtUVK30NSEYyEaZLNiqDGts6TCrVlqXe3d3owWV2F4fy6NrymygCTVZSEV1nrs4y9+/feuSq+DFtAv37upQmeQCoVZLSjN/u8UmXqausSXTPNFs9fw1Cs+ILpgrVTk/U64XQPnQkfaywq2Wm218CFG044V+e014A0gpZ4UQd65fk0JWw/WounQ9adfpbaXOcfP7SddEk2PVYjTud3I0TyqIDT43Webte7oBeH0sz/1v6ubEcJ6xvAVS5UYf6GgOdRoazfHk0CSHBjLct7uLguXWhUajM1TBsvnW2WkqjhfYvX0iuk5/JkrRdrk4UyZiCLoSUdIxg9GchR/kUzd1DaMhh3bM1JgtqzKVuiawHZ+iZbGvL9m2/T8bN8iVq1iOR9RQdu1CxUETBr6U9b5/x94eipYbFCyR7OxJsSUdZXdvalWFR/K2y/fdsZXhuQq+72M5XlCxTVKuujgNCW4a92tcBbfywaiFzbUbjgg0HSOqC/Z0J3j5So6LM2UODqQXhBLW7ndNeBoaPHVqkjnLYWs2RszUOX5uhrG8zaPvW7/ELTfb+BCiaEeAa0KITinlLIAQoqvN/UKuI+tdyeh6q9/adXpbqXPcavupcb9ibZUJTBQsnj43Ta5SZapYpWx7vPug8oyeKlqcGM4zPNdcFezoiXF832doNE/ecsjETOKm1uQM9cqlGZ69OEtH3CSiG0yXqvg+RGMaqZhJxfExNFV1pTsVIRExmCjYCAHJiIGhaQihvLYrjofl+ty1PcuV2QqXZysgIGHqnJ0oETWNBROe+fdbJbpxKVddCraLkCAFJCI6npS8Ppbn0EC2aaV6aCDTNKlazmN8vnPimYkSU0Ubx/O5NF2iI25ycaYceNkrNfpo3mZLOkrBcpfVwrRbZnaxcMLGY9SEcl82zjuTJidHCnz77Az37+tesKqdP/lzfEnM0ClVPTqTUYQQTBXtdRWmYaWzm5N2BPF/Ar4thPiL4PMHgN9cvyaFrIb1LH9qOd6GqN/adXprtd1iE47F+imqC371c6/y0uVcPVf7hxvUmo37pWIGtuNRqXrkKy6ZmEdU1+hORnjp8hwAyZjOM+dUtrN7d3c29dnJ0RyXpsvETJ101GC2VOWVmRKappEdNelOmrxwaQ7Xk+Qtl13dSRxfUrQcKo5HzNSpBmrzqieZKVVJRAwMTVB1JeBjxoSq0ub6RA2deERjoDPOWKHKju4EJduj4niM5i1+7j37FmgIGu/3+ckin3vxCnt7EiQjJrbnMVWw6UlFiUZN9ven0HWtaVK3kklVjf39ST7+9bPMlmxyFUdVVDMNdnXFePHSHJpQfgaeVGVWtWACgRB8/fWJq4VRjIWr4FZci1BbSY37+ZM/T0pMXdSdEKOGRt5y1lWYhuWRb07aiQP/EyHEC8CDKH+cH5RSnlz3loWsiNXEWbdLPljdbBb121L2vlqs9ktBZbIPDDqcnFb52KeKVVJR5T39zLkZRnMWv/TwgQWZw/b0JHju/CzTpSp9aeXAZHuSu3d2MjJX4flLs5Rsl5ihc/fODvozVwfJoyfGyVXceiazku2q1bWUGEhmSjZDo3kqjkdUFziez2hOeXfLQHABRAwNx9XQpfLSLtkO5aqLRMVJFyxXHVNTxVdKVY+jJ8aJ6Bq96ShdyShWUNmrlqilsY2N97vmJHditMCWTIyZkoumCWzP58jebnrTsQUhWSvNUFYzK2xJR7gwVcL1wfV9fN/hwgzs6opxcdaiNxOj6vqYusDUy8RNHRC8Y28nQ6MFvnVmhu+atwpeyWTu0nSJ0by9IHPcfGpCebJg1avXpaI6mXnFWWDh5G+6KHA8SdRU7vR2MMlaT2G6nuNDyMbRTi50gNeBzwF/BRSFEDvWr0khq2E98zJXXX9VeaQ3iuXyrs+vTDaRt5gtV0nHDOIRg1jEIBUzmClV6/s09q/rE+QuN9E1VSf7rh0dCKHCihKmwdZsnP5MlHNT5XphkVqfCSmZLNicHi9wfqqI6/nomoYQgnLVI6Kr/7tSBLZsgpAs5RgmpSQVMZBAR9wgbuoMz1kIBL0p5ZRWqy5mOcobfW9PEs/38XyfkbkKsyUVw31wIN0ylMpyXI6fm+avXxnmjYkinq/s8LomVBKZjhgd8ciKPP/buWcVV2LqWiCgBbqu7tKFGYt0zODv7e/jh+7ezve/dRtaoEovOy6vXskTNXXeuk0VLmkU3rWwvMbJXC1srjH//YWpIi9emmNLOrpg2/kMdsS5OFXixUtz2I5HKqqTt1yuzFYWbN94nj09CUxNYLkqaqBSdSlYLj2p6DUVQWlkaDTH40+c5tHPvsLjT5xmaDQX5m2/SWnHC/2jqDSq44DH1cqDt69v00JWynrlZY4YWksbY1QXTR68N0pYylKq0Vplstu3dajvrQq6rhKfdCauhgNFg2ue7wjVslZ10C9Pn5tGCEFXSn22HY+oITgzUaInpbJ6RXRBznJJR3WqnqRku/jSJxPTcX0lwGOGIBKoWOOm8ri2XJ9UzCCmqyIoni+JGYKqB2/qT5CrOGgCDE2re4knzBmMIN3q7t4U56ZKTBZsPM+nWHV5YH9v3dmsccUZ0ZUGIhUzqDieCgGr+mTjOr6UeL7kjYkSvekoU0Wrfm3XsoKs3bO85dQzpelC4EtVMKZouyQiRj0Bj+26RKsq+1s6aqAlTGzH4/R4kXKgWYClnbfmh82N5m3u3N7B7t7Ugm3nP9cPH+7nkc+MgaBeUx3gQH+qZfnU2nmKtsu7DvTWvdBdH47s6VozL/TlvM1vhPczZO1oxwb+L4EDUsrp9W5MyLWxnKPZahzRhkZzOJ7P10832xgvz6hCHKahr6tdvN02N253aaZM1fHqA3HNmazq+ZwcyXPH9gyN9bK7kxFmghVpTfjZQb7spYTSfLXkVNGu1/Eu2y55yw2qdnn1FVjC1DjQn+LUeJFskJGkXPVwfcF9uzt5+UqOouWSTUTY05Pg3FSZQsUhHTf51e99MwD/8egpZiqqvUg4OZLn9m0ZxnJVksmrSjWJshNXA+Fyx/Ys33pjComaODx/YZZcpcrb9zRnEbMdl1pgmONKYoZG0fawXZ/h2QqGBlUXTE00FUO5FnXsYEecC1Oq8pnl+PXVfsRQExlDE2ztiPGeg738/rFznJsq8c8OSCK6wDR0RnM2WztiIIJwt4Dl7NyNQu3Rz77Stk384ECW7d3xelrdTMysp9VdbPvrITxDb/Nbi3YE+GVgZUWbQ647y828VxMHWtvnbTHBO/Z2NdkY+zNRIoaO43k8e155U0d0jU8fv8hv/uDaKGfabfP87RzX48VLc8BCZ7Kh0QLPnJvlyF7l5JX1XUZzFlJKpgo23akIAijaHju6E0uqNQ8OZJviucu2i5AySLsZxdAFk4UqXUmznsTl9544Ta7sYLseectBCOrOWHv70hiaxrMXZkhFDKaKKmnKwS0ZfvCurZweL/GFl4eZLNpYjrIDa5qgUvV47vwM3akYhq7izm1XpXDtTkaJBBMFKVVcuu+DoQnmKlUiukoSUzM1zBRtnr0wW5+QOL5PzNDZkTQZydlIKfClst9WfYlre/ViKIvZnPf3J5fNFV5L0xo1lMnA85S93whs/rdvy9KbivLk0CSaJtjbm0QTVRwfHNfDMDQm8jYdCZNs/OqwthLnrZU6ei2VVnejCL3Nby3aEeDnUOVDvwTYtS/DXOg3FsvNvFczM//08YucmyxyqM/l/GyZw4MZTF0nGzcZnqsgcXnpUo6ooZGOGliOxzfOTNdtbu2w1Aq73TbP365WSWs0b1OcdEnHDG7bmqE3HUMTSj387PkZdKGxe1Aq4dmb5OJ0mYLlEovo3Lenq8kLfbG2N8Zzf/XEGMO5CuWqS9RQoV1dSaUifeSh/QyN5rg8rUK4elNRbNfHdn3e1Jei7ChntXTMYFtnHMeT9ZVzrlLlcy+OsL0rQd4KVqgoe7guBBjg+SrWG65mB4sZKstarb75mYkiUVMnairnuhcvzRHRBGcmS/SmlUPW6fEini9JRw1ETBDRNaSUmIaBqVfRhAa+T9TQqFQ9NCEQUi6wOc/3YL9rRwc7upO8cmmGTz99kUjQth87soP33z7I6fESd27vYKxg47g+06UqJpBNmLxtVxdF2+PVKzkqjtJm9GeiaJogbupUfYnmA0Ly5i3p+v2H9hO4PHy4X3nBf+0sri/pSppsycTQdW1BTHdjQpbxvA1diRvGMSz0Nr+1aEeAXwr+IsFfyA3IcjPvlc7Mh0ZzfOPMNB1xA10ILMfjhYtz3Lkjy/CcGhCOnZogaly1uQqhCmW0q65bboV9Lfmrd3Qn6/nGB7KxetxwbzrGvbs7eXJogmzcRNcE9+xSlcl29ahc6g8f7udTxy/yC3/+ypKZsuZPHDRdsDUbo1T1MHSlVj20NY0dZCo7emKc/f0pTk8UA89jZTsdy9s8/kGVqe3xJ05jGnrTAHzs1ARTRZvDg1m1gpaga2qFqmsCpEQTKrzq8Q/ecTXRiFng0fftr5+7lnb2Tf0pelIxMjGTStWlGKica6VMBzviVD1J1FCreSuYSNQc6DQh0DWNiKZs9mcmS/VJ22Ie7N86O82J4RxjeRtT10BK8hWH3/nyqfo93NmTDMweqi76mYliPZ1s3nKxHJ+uhEnRcrk0XUb2ysBGL+jMqGIxmqY1aU3aTeDy2FdO40u5aC30Vs9rzdN/IlfhmfNXs6ptJDejt3mYAnZx2gkjC3OhbwKWm3mvdGZ+9MQ4nYlg24b0mSdHCjxwoI+HD/fz+ZeG6YwbSCnrq8m3bs+2ra5bboW9FvmrJwsWT52exPFkvYRmzDToS8dUKlX7Aj2xq57UJ0dzvHpljkvTZVJRvV5Leyxv19XYtSxcx89NEzE0svEI+/qSdYFo6Dp37shyZqLEs+dn6U5FGRrN1YVUrRZ30XLJxAwyDbHDrSYjecshX3H46smxetiI0iyr/OCehJihkYoaTQNdBqMprAua087u60ty/OwM6ZjKpDZTrKJrcMf2DgDOTJTIVaromsZ/+uHb+b0nTvPChVmEEOiawGuIxa7ds8b2nx7L88ZEUU0wNEGuXK3HcNuuR3cqSjJq8N+OnSMZNXjm3DSelER1jd5MjC2ZGEf29gBgBolPbMcjHTOYKVfxJSQjOpbrM5yzeGB/T0uTUDsJXF4qKuXi7ds6WtZCX+x5TUcNTk8Uefue7rrA3MgUpStNbHSjE6aAXZqlyon+Zynlzwsh/hrqPi11pJTft64tC1kRy828VzozV0VD0rx8OYeUEiklSMlsxa3PgO/f181rI3mKtsq1fdvWDJF5q8elWG6Ffa3pVO/d1cGJ4Vw9P7dddXnm3Aw7uhPcuT1LIcjtXUPt52I7HpoGU4Fjmy4Eb4wX6pnSTF0J9ZmSQ286UtdO7OlJ8HrBxtSVc5cQAlPT2JKOKuewqstT44X6ZOKt27ML+mv+ZGSyYDFbcjA0QTpq4Ho+uYqD7Ul8CdJVVc+kr8p6NoZLTXnVJnPG/H4ydZ3dPUn6M6qeeVcqUi9lCqpKWmNykkMDWV4fzWM5PrPlKqCEdyZu8NqICl06OZLnjfECW7Mxnr0wS5AsDil9HF/FrXqej24qb/pMVGeiWOXeXZ3MldVkwdZUyNroXIX3He7nb09NMZCNsa83yYuX5shbDqmIDrh4wNZsDEPXuDBdaQr7W8lzZ7tePayw1bO42H5nJ4uM5SyePT9TnyC2k5q3kVYrzNXuV5us3CzCLXTKW5ql4sA/Ffz7GCob2/y/kBuI5eI8F/sdWBAzCkqQxEwjiG8WFG0PhOD+fd31Y374yE729KZ42+4u7tvdRcTQ20qZWUPl+3abvptfsKSd2NXFtvvWmWmmijYgmS5VyVkOqZjB1myMDx3ZSa6iakvXbMS5ikMmpvJ9z5Qc3FqRECRjeYuirULpzk2WSccMetMRZkoqrCmiC4bnLHb3JDF0Dc+HTNzkrp0qLMnzfE6OFShYLoZGfTJxcbrU1F/zY5NfG8mTjOj1euCdiQhdSZVTW5UEVdoRKQQjcxWGRvPMlOy6eaAm0Bbrp0fft5/f+sHbeewDd/Dr33cITdPq5661o9a+hw/3KwHr+KSjBumYgSdhZLbCcxdm+eKrIwgk00Wb4+dnqDoeMUNDlS4PHOkAn1q6Vxgr2CSjOhXHZ2tHnGRUx5fg+JK7dnRwerxUf0560zHu2tGhtA9CYGiCd+7pxjT04Dz+krHbSz13UUOvO/u1ehZb7TdZsLgU5KNPRXVsRzlPWo7bthZqsTh1qyEUbiX7LXXdm5HhucqmykFxvVmqnOgLQggd+Ckp5YeuY5tCVslyM+/5vy+Xsaz2WxKDt+3uIldx+PCRnU3HuxZ1Xe0cM0WbsbzFTLDS/Oi797Z9TUtd2zfPTNMZN+hpcBg7OKBs0rW2f+f5cUZzVr3tR0+M850rOQRg6GpAr63Laglg8pZDOmoQNSK4nqrsVag4ICSPvm8/f/TNC012d1C1snVN8PY9XZyZKKlVZDCZWCxmeHiuguNJ3nWgB4FyNpvMWxRsj4ih8ZbBDrqTJuemyozMVfClxHI8vn1mmmTU4Pu3uDxxYbzJZrhUfy53Pw8OZDm0Jc23z83g+JK4qRMzBKM5i2RUpzsZUSFfuobnObhSkjAMulMRRnI2hgBXQkRTSVosx1dJgqIGr4/lSUYMupIRBjv0IAogyfBchZ+4f1ddc9CditYFaTrmMZ1ziAaCNxuP4Hge5yaL/MKfv8J7D21paS9tpbHpSUXrk5bad5dnylQz0XqM/PxKZa+N5NE1jc5EpJ5ZTz17yszU+Cyu1FEzP2+CMZ9bZWUaOuUtzZI2cCmlJ4ToFUJEpJTV69WokOtDu0kunKJPNmUuWkxktQNGLQzr418/i+P5xA2Nqif5jS8N8d+OnWMgG+O2rdklnVYWGxwbbfiLDa4HB7KMp6M89g/uaDrmnz13KciOplaMVVcSN1UVL4BMzKyvkHozMY7s6W5SNbcadGZKDt3JCD2pWF1F7UvJaM5q2S+1652fLCYf5AhPm8rr/5nzRbqTkfoq1peS2bJDueqhbQFTF0vaDFv132KFPAASMZPveUs/5ybL5C21Qo9HdHRNNPVz0XIQnnIyA2WrjugarueRiBjK0U8I4hEN09BJRpSGYTRn0Z2MEDG0uu/C0RPjTRXCbtuaYTxvI4RFoeJgBjnhBztivHBxjoiu9PaL2UsbJyonR3PkKsoXYWsmRtX1GM2pJEW+lEQMne6U0bJSmeNJjuzp5Px0pV6hTUrJXGBmqvXvahw1q6WlS9zeKuFiN6NT3lrSjhf6BeBbQogvAKXal2EY2ean3SQXx46N8KEHFh/Ua6zGW/T0eIm37+nG8ZQdWQiwqh5jroXnSRKmzieeKrcUQEsNjsNzFQ5tTfPSJaVSbDW4tuLgQJZ3v7mP5y7MMhPYebd3xulORRjLqSIbe3oT9djygwPpuqp5MX+Di1MlZstVbNfl6XPTdCdNpksOU0W77uAGNIUnCVR+9aguGMlZ7OxO8uqVOaYKNmVbOXNVHA/L8dEEdCYiFCplXh/NUyvP7fiy7pfQamW2Ggeh2uSkVkL1qyfH8Dy/yUnG9X1KVY+oodOXjjBXdsiVq0gEXUmTnd1JBrIxTo0X6U9HGC+oNLZWwcaXMF6wSEcMIqbOfXs668Jzfn7z7zz/dL3m+t07ldNgbTWejhtLrkprny/NlBnsaA4Dqwn3xmiA2r+Nlcpqk6uuVLSuVYnoGt/VYGZaraPmdmPpLNe3ysr0ZnPKW2vaEeAjwZ8GpNe3OSHXk6gueOr0JFXPJxMz2deXxNQXL6qwlIBerbdobRLx7Pk8UUNjqmgT0TV8IGpqjBVsDg1kWg7CSw2OtQFuT0+CV67kKNaKi+zoWPbl/9CRnYzkLOIRnarrowWry4++ey+nx0ucHC2TjOgUbZdXruS4c3tHS3+DoyfGeW0kx5XZCm/ZmmYsX2WiYHFyJKcqeRk6W9JRfvfoKTQh2N6VwNDgmXMzSOBAf5I3cjbjeYvL0yWGcxYxQyNiqGIYFccDKZkLJhqu7+MFCzchVFGT2VKVff0L85039t9KkvHMn5xEdA1T0zAMrb4KHc/ZRE2dI7u7mC47KqY9SP2qCY2h0QKnxguYmjJP6JpAIOhImFiOT65SpTsd5fBgpq6tqLW3sY/H01F+74fv4BNPncfUdXKVKlFdI2+5JH3JV0+OqTSrAh5/gqbV9m1bs0wVrEWfn3ZWuI1mprcFdd1zFYcPNZiZVuuomelaemi+lVamN5NT3lqz5FMihLgTeA14TUo5dH2aFHI9GBrNMZKz6lWUKlWX42dn2N2T5IPvW7jaXk5Ar9YmVxO0Nbuy7fpB2UiNqKFRtNxFVYNLDY4/cf8uPvZXr3E6CGOKBgUybE+2lWimZr+u5eXWhGBPb4o9vSkuzZS5e1fzqm0+tUHn8SdOs60zQTZuEh/L880zU1iuz2TR5rv29bC7N8WxUxMAHB7McnI0Tyqm1OPPXphjW2ec/nSUsbxN1NARAlJREz+oQuZ4oGmCglUlFTWpaL5KxBIz0ITFK8M5+rPxBZOyodEcT5wcZ65sM1t2MHUVhmZo8MVXRzk7WcRHkI0bHBrINnk4N66IDm/NMJKzSEeNuh9D1fN5x94u9m/JAHD83DSmLhjL2+zuidOZNLk8XaHg+HQlfSKGuu9v39NNxNB59vwM79rf2+RDsFRK01p7NKFhuVfTsCYMnYmgTTFTD6q6CfJlh4Sp8/KVHO/c10VjWt3aeQY74pyfLDJWsNU7EjPYko7W0/POP/diq8PlVsqLHWP81FzTvWo1cQ5XpiFLhZH9O+BDwAvA7wohfltK+T+uW8tC1pVaUY+BbKwek5yOGfRnoi0HgaUENMATJ8fxpV+Pie5JxdqyydVWEhFdreB0oVTHfWnlEJUKhORqUl9OFCwMIUCoFbSha6SirdXJNYZGc3zsCyeZLqp617VVYK1cZu3a252oNJadPD9dxtA1eqI6lis5P12mMxmh6vr1iUJtQjVZsPGlVB7mUmK5Hn2pCJdmKmQTJqaukYyqymDv2NPF116fxJeSZETH9nyVIU1AobJwZVabjLmeyngGgqrrU8INsshp9WIluXJ1gRmjlcPg0RPjREydI3vjTBWsehKdqaLFuckihYoTeOcrG30sWH5Plxy2RQyiuuC1kTx7elP1EL921cO19jx8uJ9H/uwVYhGjniSnYHt0pyKcnijSm4oSM3Usx2OsYNOZMDk5UqDvQHzBeWqpXQ1dYAVFU14fhdsG0y3PvRjtrJRbHWP8VPO9CouThLRiqRX4B4G3SinLQohu4CgQCvCbhJpg0YRZLwm5mFNV4/aN1BKfXJopY+oCpFaPib57Zwemri9bsay2kvj08Ysq81tCxVW7vsT1JDu7EouqBpcaHI+eGMfQdPb0xhBCULJdJgs23zo7zemJYkv7fG2wnClW6UqYTdfSWKRiJc5DtUnGmUlln01EdCxHOcjNlqp89eQ4uhBkgvzdqZhRt2/HGwqrpKK1UqeqXnnVk+hCsKMzwZZsgt50lExMTSzKVZeZoMZ4Om4sMGPUJmOmoSElGDogoVR10YUS5q7vIoSqCHZ2ssQ9u7oWnaQ0CpKh0RyfOn6Rr78+QczQqLoeUqpnKxp4rHu+KhSTihpETZXJL1dRaVproY2rUQ8fHMiyrTNOvuLUcxNk4wbdSSXAtwUTgJpm5/ZtGb59dqbJ87zx+dnbm+Q7w3l8KVVbDY3PvzjKu/b3tS04r3WlfKt4m4esjqU8JSwpZRkgqETWbu3wkE3AcjHY7W6fq6iV0m1bM/WUoRFdcGI4z+WZMiM5a9lY1YMDWX7zB2/nDz58Fw8fHmD/ljSZuMm2rji7e1OL2tGXihMfnqsElcB8SnZQsASJ6/l4nuSRP3uFn/zkc0wU7CYnsmzcpCsVoeqp1W/U0DgzUar3zUr7rRbXPVOsEtEFqahBpepiO369PYYuKFgu5yeL7OlJULRcZC3LWJD7O25qnJsqowtB3NToSUXpSJjs7U2Sqzj82JEdlGwlgGKGRjpmogvBr37vmxf0XS22VkrIxPSgPrBK0Ro1hEq4EsSZSySXZsttxTbXJkARQ+cde1XY4VTJoTNhkIiaaEJDF1D1fKqeJBnVSUXUxKWW3Ga5+7oct23NcmhrlocO9XNkTzc96RgF2yMZmGeAumYnZhrcv6970eenXPXY1hlnf3+a7V1qkuT6sim2vh0ODmR55KH9PPaBO3jkof0rErxhHHTIUiy1At8beJ6D8jVp/BxmYtvkrNQJZv72l6ZLnBovKgcqKXlTf4q7dnRwZrJUj4muVSxrd/WwGpXgYvsMdsSpOh6nJ4rMllSKUM9XFbkczyeq6eSDRC6NnuuNGb8qVY+C5VAMBONHv3sve3pTbfVbo90ybmrETY2ZskNPKornSwqWQ9WTJKIGDxxQdblH8zY7zAT37elirlzlxEgBUVWOaoZu0JM0iRg6ecshGRNs7YizqydV1yZs60zUK6P1Z2JsycZ4/+2DDI3m+PTxi7x0OReUGBU4rlqhen6MqaJygtM1Zb7QBMRNHSGUN3zc0BbENreicbWYjZt0p6L0axCPGOzvT/PM+VkqjofnSarSZyyIH+/wo00Z665FPTz/Od2SjjI6V2F/n8pxXivBWtPsLDYxGOyI89KlWbqTV8s/2K5PV9K8rsLzVvE2D1kdSwnw75/3+bH1bEjI9WWlqr35sbOXpyvs708xlrfIW25d1dwYEz08V6E7tf6rh1ZOPmogL7O/L8W3z00Hcd0avWmVfzsa1LjWNVFPfdnouJQvV5kNYq4zMZP9fSkVyhRoBJbqt1ZFL3rS0bqn+TPnpunPxKh6krt3dtCTitGVVLXVH/vAHU3Hqdnjs3Gzvm2tf+fHa7//9kHef/tg/fOxY8cYGs3x2FdOc36qRCqq1O/TJYenz01zcEuaPEp9nys7JCM6M2WH3pSJ46tEKyDpTkaYLauMbEtFIsw3s6RiBnbVJW9dDTv79rkZErrG1myM0ZxFyfboTgnu2JalN62u7VPHL9Kbjq2qeMX853p3b4r3He7n9HgJy53m0kwZ35eMFWze/eYejp4Y54++eWHBeR4+3M9XXhsjH+SrryUC2tmVuK7C81byNg9ZOUtlYvu769mQW4EbrarOalc5w7MWUVNTuZ9jqXrt7TfGi0E4z1U74nqvHpZy8qkN5G9MFonoGocHM7x4aa7u4FRT2dYmFQ8e6OFzL14hGTVAqPSavpS8dXuW3b2puiPbcmrQVnbLnd1Jqq6nvmuIXa6FSbXql4MDWXZ0Jbhvd1dbHtmLtWWqaJOOGfUkK91C1RAvu5JtXXHylsu+vhSHBrK8MZ7nzESJ4blyEBGgkqLct7sLYEmHqvmrxX29SZ45p3KE+1IyPGfRlYhwZG8XPakYXz05RkQTRCNG3Q9jdK7M8fMz9KaidCcjOK63aB6AxWj1XO8JfDXevCVT1yB9+unL3Lm9g509yZbOYR/97r18/GtnmSk5Kn69K4Gua0vmEVhr2p1o32hjS8j1oZ048JA1YLNX1Wlsvy99kBovXprjrh0dSnUelH48Er+ase3cZHHZ+srXynLZ5GreybVY4XTUYLJok6+4pGM6xW6XS8USu3pSnB4vcdeODsbyNqO5SpCDXCVdeROLC875g+drIzkODmSatknHDEZzLo88tL+pPb6US66qrlWFOjxXwXY9DE1wZdbGdlV+96ips6Mr0bTiB/jSq8P8+78+ia5pdCUNXE9VmpsqVfn4195geM6i6qkCL0JAperzsS+c5MeO7GCqYPGNM9N0JkwODqSJmQY7uhP11bbjSe7b01mftMwvZzpZsHjm/CwRTaunZT01XuRAf2rVTlu1e/PEyXFMXXDb1gyaMBnL2ySjBmMFm929qZbmnfffPsie3tSGC8blJtqbfWwJWT2hAL9ObHZv0mb7ZiRI2qHycx8J4nePNKh1h0ZzPDk0uWR95dXypVeH67beku1yz84OHO9qNqx01EDTmr3faykwNU0wHXiZ96aieH6ZFy/N8d7bVNWrHd1JdvWkkIAdJCbJWyrOu5XgbBw8DU3V7r4wXeLSdJl7dnXWV5YFyyXS4JEfNzWcIG3nUuaL1ahQa0Krt1jh1KjOyJxFparyp0d0wazt43pVUpH8gpj40+MlsvEIElWuMxbR6UlFyZUdTo0V6EsrdXreUl7qgx0xRubK/M6XT3Hn9g7eua+LkyMFvn12htsG0mzNxrA9yWBHnLLtcHKkwMtejkzMpDtp8nrBrpczfW0kj+tLugM7c60S3NmJYj0sbSUsNelUz4lenzzU+nf+BG0p4bmeq95cxeGH/+B43Z/hx47saDKPNLLZx5aQ1dO2ABdCJKWUpeW3DGnFZs9d3Nj+fX3Jes7pQpBGdL5QaRxUFquvvBq+9Oowv/PlUySjBn2pCOctl6dOT5GNm/RlYvUV9nSxSlQXVByfly/N8ZWGIil9qSjnpkqcny7jdPiULIfHvnKa3kyMqXyFsiuZKljkKi7pqE5fJtbyGhuvs+p6vHw5R9TQ6E9HmSjYPHNuhnt3dxIzDS5Ol9CEIGLodbt4oxPV0GiuZbjdSn0VvvTqcF3r8Y+2e4zlKpSqLhrguB7lKhiaIGpoWK63YKU2PFdB1wS7upOIQG1fsh3OTBRxXcmloOymoQukhOE5m4SpoWsa3z43zd7eFIcHMwzPVnj2wixxU1Uj83yfQsUlFtUxNMHoXAUQbOuMcdvWTH2FviUTYaqoCpREdA3X97kwXWZHd2JB/yzHp45f5NxkEcdTWg4vopOIqFrsmZipMp6tUrOxlqve+ROBRERAziJfidKXipCvOPzOl1VgeCshvtnHlpDVs6wAF0K8A/hDIAXsEELcAfy0lPJn17txNxOb3Zu0sf09qRh37+zgxHAehCQbX1joZL0GlU8ev0QyejXP9ZZsjNPjBeYqDtu7EtiuT77ikoxonBgpsK0zTlfSJG+5fPzrZ9nWGacnFVFq/UQEQUWtyks2XQmDZ4eLdMRNpATLcSlYDulgItJKcNau85kgFWwt9Mz1VVjUK1dyvPfQFrZmYy1za9dCkpZL1rGczXN/f5Jvn5nmi98ZVZXUNOU1nbc9dEDXNapBGJXnS2KmamPNga92/MGOOG+MF7Bdn5ipU666XJm18CWYpsDzwfF9ogg0TeB4PkXbpyNhIhH1amjTJRu/ltYVKNoumgb5shKahq5i0IuWy4eP7Kxnrfub74wE5d9USKLvgybg5Eievky8qX/e17V4yc3GanSpqI7nG4zmLPrTkqrrs78/xUjgnb6cGaMVa7XqbTUR+PTTI/z0fhqOrSJ4P3n8UksBvtnHlpDV005s9+PA+4BpACnlK8C71rNRNyPz6zzPr7V8ozO//aaus6c3xe/9cOvY1pXGS7fLlZkyuXKVs5NFrsyWEYJ6zemi7RE1ddIxFQJVy2QmhCATM3A8n7zlMjRaUF7oVRchBLqmETd1RgtVOuIms2UHy1VZ5dR1OEwVLP7omxeaaqYPBY5RX/7OGOcmi7je1Tjj3nSMd+3v5batKgbY9mTLeN6Tgaf5q1fmmmp51wRrK+bXgj4/WeR3vnyK5y/M4vs+FcejYKs4cyEknlSpYA1dEDOU/VsiyFdcbLc5vvvhw/10JSMULRer6jKes/B9STxiYOo6mZihkr14EhCYulp9S6lCzqaKNsNzFfKWR7nqYTkuQihhXMsrH4vo7O1Nsbc3iaHr9et8+HA/tivpTZromkbFUUVSelIRDF1NfjQh6v2zVMnNejW6IAufGaTSvTxrMVlU9u9f/p4D7O5NrTjWHNYuPrtxIlC7tqrrq5rnjceO6oznWydZ2uxjS8jqaUuFLqW8LBq8YIGlq823iRDiYeC/ADrwh1LK31mL496IbPbcxStt/7XYbuerkWvZvY6fm2Ysb2EIyCZULe6ROZXfuiNu8NAhNWA9fW6a18fyJCJX7aa269OdjJCNG5zKWXTEDaqOEriuLxnIRrk8a5EIMoPt71cpM4uWw6WZCidG8rxrf2999feeg708OTTJQEalG1XqZIu+tETTBIcHM1ycKjFWsHn0s69waaZM1fGacmlfmi5xeboSaAMWz/w2v28uzZQZyETrK66xghJIU0UbiUATatWKFGgINA1s18PUNSQSPRCoPakIJ0ea47sPDmT5pYcP8KnjF3np8hy257O9O87OrgTPnJ8FIBvXyVvqeJ7vYeo6FUdNhjxf4gQJfSSqwErRrhVXgVjkqiZgflz1wYEs9+/rDmzhDgSTs5mSw2DHQmG5VMnNxmp0larHdEndo4ih8c693VQcnz29qfqKtta/rULKWj2Xa7XqXUxT5Uu76buC7dGfad6uxmYfW0JWTzsC/HKgRpdCiAjwc8A1FzYRQujA/wM8BFwBnhNCfEFKefJaj32jstlzF6+k/SsdVBazKb7nYC9/8cIwl6bLlGyXmC6ouKoCVzZu4vlKAG/vitdTYm7JRBkahZih8ojXYnh3dSfY1ZOiLxXlxEg+UNXC1o4YmhAkA6emVNSgZLvMlqtMF1USGJXqU9QH7E8ev8ShgQzZuKlU5ZdzXJguMVep8u6DfRQqLi9dVg5TA9kYjuvVw+129iQpWC6nxosqlr5gM1uyKVfVqvXYqUnu3tFZF/bz++blS3PkylVSMYOeVEzlsY/qTBWVoARwPQlIKo6PECAkSA1sVxI1IG7AXLlK0fY4vDXT5Mx2cCDLbwXVyObXI3/m/CyOB+mYcm4by1tBchqfgu1SsFx0Ta34PV/i+aAb4HpBO4SqUW45Xsu46g8f2cnvHj1F0XLpTUaQqFrquYrLZMFqcgpcquRmVBecGMlTdT0mCjYCSER0BjtV8pvGePNaXoP+TOSqz8SJMT767quJe1o9l08OTQLXFp/daiKwvy8JsqSe56hOwfYo2coBdDE2+9gSsjraUaH/DPDPgUGUoH0rsBb277cBZ6SU56SUVeAzLEweE7KJqYVwDXbEGZ6rcPTE+II0qjVaqRKzcZPfP3aOkyN5ZspVCpZLMmaSjmhI1KokEdE5vDXDb/zA4XpKzF09Kf75g3uImjrTpSpRQ+NAfwpNUzG8Hzqykz29Kd65txtdU6U5bcfnTb1J1ZggJ7zt+PhSYmiCguUxWVAqzHTMYDxv1VWoUkIiatCbjuJLmCxUGSvY3LWjg109KTQh2NWTYm9PgucvzfKFV0Y4OZonGzPY2ZOkO2EynrexHZ+YodKqvnR5jv39yZZ905WKIITgzITyKU3FDAq2x5ZMjIiu168JVDrUiKZWw4YG2biBL+FKrspMqYoZCLrfPXqq5b1pVM/u7Uvzzr3dbM3GuWtnFw8fHuC3/uFhbhvMIoTGzq4EMVMnGTGIR3SSEV1lvnN9QPDmfuUYJyVMFmyKlsPpiWL9OmvPzNZsjFTMoOpLpFSagrlyla8NTTCer5CrOFycLuF6Po9+9pUmswY0V9pLRw0MLTCTRAzeMqiEnOW4fPPMtLq2soPterx0OcdsuUpXUsXqf/zrZ/n08Ystn8vT46VVp3tdrH9r6u/OVIy+TJRM3GSiWCUTN/nl7zmwqBd6yK2LkFIuvYEQ75RSfmu571Z8YiF+CHhYSvmTwecPA/dJKf/FvO0+AnwEoL+//+7PfOYz13LadaNYLJJKpZbfcBOy2muzHI+pYhVdU/boWpGSdNSgKxmpJxYBuDJbwdQ1Gi01juczV3bQgsQnXmAYVElNZFCRy0DXBH3paMvz5y2XqquKZ2QakpnUftM9m5yrqxzjER1NKAFcSzkqpaoTpmy9Kpe558t6sRFfSspVD00oQS4E6Joq4qHs68rjG6BcVYU9VFiTDGqUazi+xPMknpRIKRFCkIzoRE2dvnSUi9NltZoNymQamsB2/fqxKlWXclVllROCQEUt6I9LZqrKWQ1ASqXerxUXAVXYQ9eUc1oy2roWfK2vKlVPtSHoq1p/Wo7HSM7C9SS+lAgB0gdP1u6XqvndkYigCZgtO6qIiiYwdWWj7kldfR5qz4Ln+5Qa+tbxlA09bqqJQdpwcUQUX0o8X9aPMVGw8XzVFtv1sQNTiamLutd5IQgNTAfe6F4wWdAEmIaa8bi+SiubjplNz2WtLds6F++rVs/cYuQqDtOlKo7nY+oqBl737HA82YSsx7U9+OCDL0gp72n1Wzsq9I8Dd7Xx3UoRLb5bMJuQUn4C+ATAPffcIx944IFrPO36cOzYMW7Utl0rq722x584TU53qAbq46ihKZW24yM0wbbOOLdtVav0l2pZ22JXVYnHTk0wVq+OpgRpruIGg6rBQDbOniWKnSzH0GiO7zz/NCedLQxmr9o9f+pPniNXdijYqjpXyValVlUikm5yllNXoZ6bLFKuuuQrLhVHlfy0XEnVUytiIZSwNTRR9/p+e5dKK3qhXOTUcBHXl3QmDKpB0pQ9PQmm5qqMF2zu39vFt875xEyjKaXnlnSEsivJWAZXZiskIxqjOZuC5WI5Hn0pkx/ba/GJN2JUXQ/fl1Q9H0PXVDU0oeqI96SjdCUiTBRUnvB/8o6tS1Zqy8bNqyrjmeYwuE88dZ7Zss0LF2fRNQ1dg3TUwJPUV5CPP3GanO/geF49bj+iaxzemuE356nth8bzWI5XnyRETZ1DAxlOjuY5NJDhkLjCSExVL8tVHCYxeeSB/Tz62VeCSntqiJkqWjx/YRbPh+95yxYKlsvXz0/wzn1d9MXiPD0yzenxQn0ytbc3FeQ50CjYLvft7m56LnMVh2zK5EMP7F+8j5IL+2ixZ/CvnjqP5/n1euqGJvhnbyYcTzYh1/valqoHfgR4B9ArhPiFhp8yKKeza+UKsL3h8zZgZA2OuylZSVKIzZI2sVWIVcl2GM1bbO1QZR+XsinOlh3euj3LqbECc0EilISpBasylVWrFoI0n+X6qDbYvi0mF4RuHRrIcmGqiMjbqnBI1MB2fDRdLAgn+9XXJyhaDhFT5fcuWB62q6pfVT1VQjOqCy7NVuhLR9nXd1VdvKM7SdnxGJ2zODNZUqvUuKm85E2d/rSy1buuxEIJlFoa2LFClcc/eAdHT4yTjOicGi/Sm46yrTPOuckSJUdpELZ1xpks2kwVbKKmUms7noOhgaFrlO2gMppUGo1cxeF3j55qSsDy8OH+ZcOmDg5kec/BXn7zb17HkyB9n2TEoC8bJ24IPnn8En97aorXRnLs7IpzYbpC1NBIRw0sx+MbZ6brdviaA+RU0a4799muz21bM3XzxX27u6DBz6vRA7yVXVlKmKsoNfyd27N8177uenKYfX1JzkwUqTh+UO5Vna8zblCuCr7++kRTdrlGW/dSzoXthJYdPTGO5/mcnigSNbR6yGOtSt6N+F6H3DgsZQOPoGK/DSDd8JcHfmgNzv0c8CYhxO7AOe4fAV9YZp+bkvlhQYuV3Vzptqtty+NPnG5pW1wptVCyouXW1ciThSpxU6lfi7a3pE3x/n3dRA2dVMwEqRyXXF+yvz/N//qn9/JbP3h7k6dwrd2/8rlXeewrp5fso5pA0jXRZNs8emKc/f2qGlmu4pCK6Li+pOx4/IsH99ZD5moJUyzHwzAEMUNntuxS9VT+cAncvbODmKljez66JnjzlnQ9jSio6+lLRekNVsGD2Rj5isNUsYrteOzrS1H1fLpTEVWAxdQp2h6ZmMG2zni97OVozqpPkIQQ9GeigTe4SptaK9lpBNeqCRULrgtRV6/7UtU5r7oel6bLnBjJN/XdayO5JcOmhkZzfO7FESpVj5ghiGiCUtVjeLbMiZECI7NlBrIxIrrG0+dn8Xy/3l4hBJ2Jq2FzNQfI7lSU2bJL1NS5a0cHvWmVAKc/E1syRLHRrjxRqHD87AyeL3nPwT7etruLsuPzjn3d9W26klHeMpjBl0plHjU0tqQjnJ0qs7c3yTv3qTzw3z47Q9X1Fmgdas/ZTLHK62MFpopWyz5qxfBchbF88/3LBKVeV1q2NOTWY7liJn8nhPhjKeXFtT6xlNIVQvwL4CuoFf3/lFK+ttbn2Qy0Wt3MFG0+9oWT7Ai8dGsryPVMm7iYJ/hSCTOW4mrOb4HteCAEFcdje1e8ZTGR+Z60jVnX3tSXrHvj/rMH9ixZ/eup05MULJct2SiaMFv2UT18Z5FV3J3bOxgr2BQtl0xcVSOrZZEbGs3x8a+fVQVPTA2r6lGqukR1jbLjIbhqY357UJ2t6npUHL/uKV/zWo6bGju7kwxkY5yZLGF7knhE2fZ707F6vnAp4UhQ0atxddmq7KWha+zojgPVegnTvkyEodEipaqaAPgSSlW3brfujJu8ZTDLmUlVtaw2Eamd5/R4gcmCXa/bva83ScS4ajP/9PGLvD6Wp+r6SmUf2JNt1yNqGFQcn+mizeHBDGcniuQrDp2JSN0k8Nbt2SZBd3Agy69/36EmtX1N4P7YkR08OTSJF5Mtk7DUtAGfPH6JU2N5YobO3Ts76M9ctVnXJoy11fMd2zv5wD3bOD1eqq+ma06IAH0Hrq7qa8/Q/HexK8iadmaitGSRmsbn9tJMmaHRAqmYTncyWq9bbmgizKQWsizt2MCjQohPALsat5dSfve1nlxK+TfA31zrcTaaodEcE0G872pU2vNjQScLFqfHizi+z327u5rUu+uZNnGxycFSCTOWoraS+tTxiyorVsJke2ccz1epNW/bqgp+LDbINRYXyVtq8DzQn1qQjnV+ux1PkorqTQPp/D6qqVkbPTFq7XhtJEe+4lC0vbqw6k5djcs+emIcx1Nx5QXLQAcq7lWHq5ipY2gaz1+Y5c1b0miaxkfetbu+b2NY3R998wLdKQNNmPUQKbvqUg1Wxvv6khw/O1PPFz5fWO3vTzJXdpgs2CSjOumoiaYJ9velyMRL3L6toy4Au5NRTo0XyQZq6N50tB4fbepKQ1K0XAxNFRqpYTku0yWbhKmymtlVlb7W0OBNW9I8/sRpjp2aoGR7gYpfTfi8wJEtEVGe5LW8+Tu7E8pLPOjf27ZmmurGz39+WoUi7ulN8Z3nxxnNWfXvQdnPG8vdFipxTF1wbqpMZzJCTyrW8n2ZLFhMFay62SBXqbKjO9m0zfz95r+L+3qTvHhxjqmivWx2t9qkcyAT5eyEoFL1GXUtupMRtMC5L4JomV43JKRGOwL8s8Dvo9KprkkCl5uJpWyp7b5s8212ZyZLIKAnFW1aBdVqVq9X2sTFJgdLJcxYjlpMcc1W2Di4dqeii+YYr7WnVlykhipLWVmw3WJ1qGs09tHQaI7JgsU3z0yzfbfDhFMhaqgV3r27OvjKa2MAymnMUQ54+/tS9bjs4blKvVpWZyKC5fgI4aNr0JGIkDB1DF1jomAzXary3iDBTKtY3eVKcJq6zu6eJP2ZaJOwqqlwnxya5C1b07x0OcdMyWGmWFUqZtuluzfCR+67KgBTUYO7dnTw8uUc2USE27ZmEAJeuDgHwJmJIqauwtjesi3LZEHZ5t8YLxDRNd68JcV0yWGiYFG0XbJBHfEvvTrKaN4mbgg6YlFKVQ/f9/GkUs1XPZ+C5eAEmer29aXwfHjb7q66NuLidImt2diCSfBi8c0HB7KMp6M89g/uqN/T2mp9dLbCVNFmJF9BA3Qh8IG/PTXJgwd6MXWdiC6aitA8c24GCdy3p5NcxeHydIWEqTc9e/Pfs/n3rjcdq8f1z79X8/l0kKe96vl0xFXFO8fzKdgOb9vVhSdVdb9a7vywwlhIK9qJA3ellP9dSvmslPKF2t+6t2yTsJQttV3mx4LOFKtIKZscnmqz//VMm7hY+tPIEgkz2uXggEop+j9+9F4e/0d3sLs3xetjeU6O5ilYTssY8XbTsc7fbl9vkqLtEdG1eh9dnikzWbD4yU8+xyN/9gol2+Ude5Vt81tnZnAC2+bp8RIH+tWgbbt+3XZ/erzIw4f762rP0ZzFlVlVqnNLJorrSYTQ6EvHODiQUWrsIO96xNAX9VOYfz8jhs6O7kSQXEX1j6Yp4fAT9+9qSltbe/a2diboSkboiKv4a8v16h7YtX7/ift3UXZ8IoYeVOaSvHhpDimVrT4TMxgv2Ny2NcPuniRXZsp85bVxXh/NU7IcYqZaxe7tTdKXjtGbijBXcerZ1HQBFVdSsV1MXXm4AxhChf1Nl6pcmalw9DujFG2Pj757b93foRq015wnrFbif1HrC8fzuDRbQdPAEIKyrcK6PM9numDzVy+NcOzURD0RUDZucm6qTCpmkI4ZnJssk42b7O9PcWq8uOR71upd1HWNX/++Qzz2gdYphkFNNr5xZppK1SVXdpgpq/370upZ2dWTIqILtnclFsSfh3bxkEbaGZn/Wgjxs0KIASFEV+1v3Vu2SViLnMg1VWFtQOtKRVo6PA12xBdsu9oEEq1YbHKQia2u6uxiDnE1T+NU1OTQQIaDA5kFg/bQaI6pgsXXX5/g2KmrCTxaTVaWEoKjOQvH9erf5ytqVX5qvIiuCdIxk+9+cx896VjdKWxHd7LugFawXTIxg+3datJQU3smIjqZuBFkCXNIx0zevruTdx/sZ7qsqmkhBIamMTSa59Urc3zsCycXCKVW9/OXHj7Ah47sJB1T/fPmLQv7B64+e2cmS2Tiqurb/v403ako27sSQd53dQ9+4c9fCSpzeWTjERAqPv3MpDIzHNqa5Z17u+lNxyhVXZ69MIvlqKx06bhJruIhpeTMpAr9ylkOMUOvO151JCIICBz2NDWhAVJRHT/Imx41NeaCe7SnN8UjD+3nsQ/cQW86VhdW00Wbk6N5vnMl17K/FqPeFxMl4qaOQGC7noobj2gUqyouPxnVg+Q1BWxXTfpqTpaNpWN39iTZ1hmv3xfH9YibWlM+/NW+i0dPjBM3NSYKVTxfEjd1TF1jquhw/95uHnloP75kTXKtN7KWDqohNwbtjMw/Fvz7rxq+k8CetW/O5mMpW+pKaFQVNnq3tkrTuF5pExezOY6fmlvxsZYrt7iUMx5crc7VWF/6/n3dLQfIVu3+pYcP1Ld7/InT9UpgNS9u2/U5M1Hijq2tQ5B6UrH6BKqmJm1scyoQFpqw6U5F645VuYpDoeJg6kLVzEYlMelKmMwUqy3VoK3u5+NPnF7WWbHWVpX+VYVEzZUdilWXp89NsXWgyr/9yxM4nmRkToVt5cpVDm3NcG7KaSoH21ju1PclEUNDF4JERFeq3arD5RmPqudjahpzZYeYqXF2shAIS7/u5T6QjTHsV4JVuCAR1diZVpOegu2yszvZ0qFwsmDV8wV0Jgymi3bbauNaX+Qth550hLGccriL6MrT3tAE+/pSJCLKkz8REZwcKdB3IK5MLo6yDtZs/wXLrReiqT3LtVjtxlSr7799cEHO9Fb51BsZnqtgaKKp6pquqQQ8tUQYEUOjYLkt4+VXw1qWPw25cVhWgEspd1+PhmxWap7Wi3nEroaNLE7QSpiMn1r5cZbzll/KGa+xjvN8J6fF+mCpSU3juWqDdeNqa34IUqsiLPfu6uD/+dtz+FJVKdvXl+Tte7rxg7Sr7799kD29KTUBCTLHpaOGqr4VJCLpSkXqE4FWbf3Sq8N88vglxvMWJdvlnp0dZONXt5u/Apvv5V9xlIAZyMaI6hqeL3l9rMDWbFwJYlcyZ7lcmqksKAdbK3fqeB7npkpUXb+eCa0jbpKJG+QrLhMFi2zUJBI4vRUs5SEfMzW6UxFsV7KtM0EmHmFLOlr3ahdC5T/PxMwF4WeXZsq8fGmOsuOSjOjETBX73ZOKLtlfjdT6IqKrZEHdyQjloNqc70nipsZsucrwnEc8ovOWrRmGxpSKfE9PgufOzyKBQ1vTC/wyjp4YZ6ZocWKkEFS404gZOh//+ln29KaaQsraEZC1yIHBbKxuhtCFYEdXIqjypvwvLg+XOT+l+i+iCYqWy0jOWlV8+HpGr4RsHO3UA08AvwDskFJ+RAjxJuCAlPKL6966TUBN2M73iL3Wl2KzFydYzls+ogueOj25ICwpooumOs41J7K3bs9StNv3hp+fYKNWCWxfr4rxtl2fTEylRc1ZzdqN+ZOne3d18OTQJKYuQGpNVcNM/WooVe2e1YTJq1fmWiYiaaUGbQyZ60tFOF2q8rWhSb5zJce2riT7+pJN52psa83L33JctmSiKvY8WM3FgrKp3UnlBGcIGM1VuGdXV1MWu0c/+wo+LsfPzlCperiej+uDxMd2VfsFENE00nGTt+7o4G9PTV5NiappKmY/qjE8V+Gu7R2M5KymEELb9Tk8mKlPmBo9sXPlKkXLra+EdU3j8ODi/QUqbWmjl/Z7DvbybVPjG0HEw7ve1MOZiRLjeVXP3HbUNaQiSnty20A60Mq43LenKzABSPrSzcl6XhvJcWKkgADipo7nS/KWS9z36wJwJQLy4cP9fOW1MTyphHktlG5vb6p+f2OmTn8mylTxaujebYNZIoa+KqG7ntErIRtHOyr0/wW8gMrKBiqD2meBUIAHzPeIvZlpNwvcUt7yQ6M5xvMq7WctLOmZczPs6E6wNRtTdZxRq9haHumh0eayl8u1sXE1NL8SmIrpLqJpKi695kQHtLyWb52ZJhs3uW1rJlDxCiK64MRwnp5UlGom2uQ9DRA3VVazmVKVwY54PRFJruK0NK988vglklGDbNykVJ+oSGYrDt1VJVh39yT54Pua03c2evn/wp+/AhKipq48zO1cPTd6Mmoo1fZsmbLj8+z5Ge7cfvVaBzvifPk7o+QC+70vJY6vvMbzFeXIONgZY6bkMJqzuGdXJ91JVWTE8SXFqhvUYlcpZ01DRxOCrdkYr40W6EyY3Lkji6nrdY3Gx75wkumiTU8qyt7eJAVbJf0pVz0eONBFT2rx/hoazTFVrJLTrybreXJoko+8azcfOrKz/ozetyfKs+dnGMtZ6JqgPx3F0JR6ujGXc286tuiznLdcXN8nGdERAoxALe/5VyMiViIgDw5k+eh37+XjXzvLTMmhK2mysyuBrmtN/h1VT/Ku/b11h0RoHYXRDusZvRKycbQjwPdKKT8ohPgRACllRcwrDh5ya7ASNeFS9cCPnhhne1eCLdlo3b6Xihn19J21Os5APX/6XMVd0tN+qZSWtVCg0bxNxNTZ3ZvitsE0n39xFDcryVcczk8W+cRT5XpK18Zr/OaZad6xt4v+jBLEZyZLFCoOluvSlVSew90pFYb22FdO40vJzu4kDx3q45lzs1SqXpNT4L27OhbE947nLfpSKhnLTLlKzFTOVgXbY6ZUZaZcZTxf4WNfcPmxIztaVqZKx5QTmECVCt2uw2zJIRqksK26qlb3ru6EKnlqufX79/Dhfv7Xt88TCbzHpVROaDWb7L6+FMmoyp5nOz5PvDZOznJUgRQhcH2YDqqb9WeV8xeBY9rPvedNfOr4Rb59doqi7REzNF66NIPnw5ZMFMvxODfl8NZtWc5OlvB86EouHWJ49MQ4WzTRcsU73/v70c++gqnDuckyecshZuoMZKOcGC3Ql43X7/NjXzlNfyZKtSGF7MGBLNm4qmhWdSURo1ZUR2JoWl0ArlRANppcFpsMr6XQXep9DNm8tCPAq0KIOMG7LITYS1P+qpBbhZWoCRvVu08OTSMQ3Lk9y7nJIk+cHG+yJfekYnVbcm3QuntnR5Pzznft6267lvj8etmg8o6bhs5jH7iDL706zL/7q5O4vo8/IJktV8lbLvv7Uk11vmvX2JlQ+cn7M3F607H6SvrkaJ6d3cmmbV8q/v/be/Mwyc76vvfznqX2pdfp6RnNPow0o7GE0MaAkCWQQIkxiUkwzrUxzsUhzk2wQ6w4cbiJ7euHXIKxlQTfxOGGXCvgGEMCsWOMQBIMYhmBdjEzrRlmH03vW+3LWd77x6lTU1Vd3V3dXd3V1fN+nqefma6uOvW+p06d3/v+lu/P+2rcdlMPYHLsgLdTf+X1FO88sr3qjm9cBCWCXkvQZFjzOlnpGo4riJg66aJNqLKjTRcsPvE1LynBN+L+/LfHg6TzXp326dEUtx32duOGJrg8kyega/REAty5p7dOX+DzJy4jgULJJie9L3pAh0RII130duGy0k3O0ARZ26VgOQR0yJe9NjN6ZUk/lipyZNhLtKrdgY6liuhCY2fSy9hOF1yE5rmkeysqcjM5i1u2xxlLL19HfW2+wI5g/T5isR2vf029uaJkB16jnN5IrfiPw8XpHNPZEvcfGlygjW9ZDj8azVS60GlEgiZBQ68uKldjIJcLk7XT6HYyr6bRa3dYyYm0jVYM+G8BTwC7hBB/ArwV+KX1HJRic7ISN+HIWKpOge3IjjjZks0nvnaGkKkRqSR2NcaS/ZtWMmxyz76+6k3rLQf7F1WlWomkpS+DajmeSxSoujHH00UmMiWvUUYNR3bE+d65WS5OZes6RvVEjAWlPiXbQdQ4ZwdiIe4/5MWfP/rwoUWzy2/qDXNyNA2AqQnylV27JjyVNL/5iP/8x09cqRrw2qQ/XRdkc3a1Dvv+Nwwwk/cEXuYLZe57Q39V8c0br823XpsiGjCIh0zm8l5in5QCXWgIXBDw48mspwdQaRdqOxLH61qKIUAiqlnoM/n65MAnTk4wm/MWVCFTxwXCAW+hMpMrEw54uQ/T2RI7esL8znuOtJR17ubrmxcutjttZgjn8lZV4xzg3KSXLGY5coF40iNHh7gym+etBwJ1n/9H3n6gOs71MJDtPmYn8mqaee2mnbJq1NImWslCf1II8SLwZrzCh1+TUk6v+8gUHaN2xRzQBUe1Ao9+6ZW6ZDCfZjdN/0t7YSpLb9i7xF66ksLQBNGg544sVTp1+bHk/YOx6s2p8aY1nAzw6afPY7uSvqhJ2XL4zDP5qut+JZKWvgyq39db4HXOGksVGU+ViIcNLk/n6uYYNAyODsc5O5mtSqgOJ0Ocmcg2eW59o76pTJFTo2ksR/LYk2c5NZpiMB7g2Qtp0kWLRMhk/2CEaMjkn/+1m3n8xBVPEEQTvHFHghevzhPQwHZhMO7tVONBnYl0sXqu65L+bJeS43JTMoShWRza7u2GXSl5amSCkFn/lT89mkFKLzs/XCnz8lqPSuyijRBej3PHkYRCXvmYBCq2HNPQkBJ0AaZecftXXN/+Of/sdy9Rtt3qYidoaNXe3omQ16RlNlumPxZsuazpkaNDvPDs+UVLLWtpdk3VdiMDPE+PJqr6/FCv0e+/PmDqHDvQPPdjPQzkSo65GbsUNvPa6Xmhst/bRKsKHSFgrvL8I0IIpJTPrN+wFJ2idsXsS0zu3eVgmjCcCNYlgy1207yuiiXrSojOT+WImDpCE9y9p5eZvFczjZB1N+7GmviP/tkrIKAvalKyvdaLh7bFqjeBlUhafva7l+iPBkhpXtcvV0oKlZKpWNBrWvLS1YVz7I0GeHMyXK3LPTuZxXUlz1+e4/JsvtK0QydoeI1IUgWLomVXy5N8ic7zk1lGRl0G4sFqK80fXJjj2P4+fuq2nXVu8SdOTnByLIPjep3CokHv65opOcSCBo89eZYnT09QshzyhiAU8Ha4YUPj9fki9nbJk6cniIUMtseD3LGrx9MsoH4nGjY9ERMhBLGATqpgIwFHQqKyKLCFxHK93beuCWKVOXo9WyQSQTJiEtQ1EFC2HSIV4ZMrs3lc6fU6D5k6YVNnuqI2GDR0tseD7OwJLzDeSxmkw8NJLscCTGG2tDut9db4C9Ors3noixAPGQR0jWzR5tad119fuzjd7FUhm7XOu5nXThOqUUu7aKWM7N8A7wdOAb4otgSUAd+C1K6YT4+liYUMdM1LAPJjiH4y2GI3Tf9L69dcu1IynS15N3vhaVNfnPG6PS1X313bOKQ2K308XSRQ+X8zF6kvadloEK7M5hlNFSiUXWJBHSnBrnTe6osGmMiUCJs6F6dzdXP87Hcv4WLz0pVUtY/1XK7MbK5MPGRUM4WjQYO/fedOzk7k+MHFGWIhg6M7E1VXvqlrzOUtBmrmWJss5uMbjENDUT7xtTPYrsR1XTIlh7lciT390YrynEsybDCeLlG2XSxHki6U8ZVlowGNdMFibL7AP/9rNy9InLrvYD+nRtOUbBfHld4XXIAmvZ13yZE4UlblgouVZK6C5mDqAld6u/NYSOf2m5JomlaXCNgXMyhbDpens5i6TiSgMZUpYWgCQ9fQBJydzNa5o8Erq/NDHf3RAJZd73UBr9Tqow8cohUaDVymaHuZ9rbDWMrm6I4Eo6liRWq2PXoO7WS53fVmrfNulojnSqmy39tEKzvwv4lX960S124AalfMvsKXQFRFT2qTwRbD/9L6NddzOc94Rysxxr6oQVAXnBq97jpfajx+4xDfeAcNjdmcxbED13dHzWKFz5yd5EOPP0+qYBE1dQZiAQ5ui5POW5ia4+0ggXBAJxY0vZ7bhlfnPV+w+dB9e6siHVdm89WmHoPxIEIIUkWLaNCgJxKoLm5SBYuzEzk++vCh6rmsLQPSNOiNml5v76Lt7Y4TQV66Ol8tRTs0FK22tQzqgj19YV6byDKWcrmpN8Kde3rZlghXbtgBipZDb8RlMlPypE01jVhAIKXLaKqIxHNxP37iCr/zniN89OHrRm9kLMUnnzjDlZk8mZKFLio9wzUvcc12XZCQK3m7cl0TBAzPfe5K2NUb9l6jC/YOxHjk6NACY+KHGM5PezXZmibY1xfhtpuS1YTA2g5zI2MpPv20167V/+zPTGS5eSi2aoPkj6lsOxw/M8tYqojtSCZ7w3zqfbdVP+dOJHktRyu7681W592seZHv0Rp0ZVt6NyhaM+AXABOVeX5DULti9nfQElknMbnc6rk2EW1PX5iL0zlcKekJm9y2M47lQqpQRhPaoi6+6g1gNI3jup7ARyRQUVCzMTRRdxNodHH+p2//mH/39DkCuk4iqDOVLTOVLTOdLbMtEULXDXRNQ9dKbE+ECQf06gJBCEFv5HrjCD/D+7WxFI6A0fkCA7EgJVuyMxms63rWTJa1dvfhudn1am/vqUyx2n1sOBni0nSWL7/4Onfs6iEa0jlxYRYBPHR4GyHTc1tnS1Y1ntwfNfn++bSniIYn3KJVVL0MLU3RcrmpN0xAF02lXA8PJ/mNR27mcycu89UfjWHqGn1Rk7CpM1+wKJYdpPBqkgVg6p6+u65pbIsHCAeMOkEYgM9+99ICY7JnIErA1Dk8nFiwqGk0NE+cnKjmO9R6Xc5NZvnxZG5V2cyefKkXEprLezKyAUNwZSbHJ584U/WabKb4sU8ru+vNVOddu+C4ZXuCiKlzZiJL3nK4dUeSAQKb5tx2O60Y8DzwshDiaWqMuJTyV9dtVIqOUeuO9iUmHRf2D0aWrMutpbaEbGQ8SzJskAyZRIIG0zmr4jpPNHWdN2av7+kLcWYih+16ru5sycbUNT7yjgNL3gQeP3GFgK4TDeqUbQdXeq7q2XyZ4Yr61YHBCJpIc22+QDSosy0eRNc0SrbLG3d5yXG1N88L0zmmMiUcx2UuX8bU4fX5IrGQwXS2yEAstKws60AsWK0Jj4cMTo2mkcDRnQk0IRhPl4gGDcYzJUTmekOLC9P5qtG/Nl8gU7Qp2w4jYxnP9S1BE5C3XAzh9UR3pKxIfy6Ucr0wla3Ktg4lQnzw2O7qbrhsO3z91ASmpqEFvAYljpQYwsuE39UbqWailx13wSJsOWOynKG5Nl+o5jv4xtt2XS7P5DmwLbaqbOadPWGOn5kkX/Yalxi6wHYk4YDOtbkCn/7med68v39TxY99Wtldb6Y678YFx96BGL1RT5fhow8f4vjx0Q0f01alFQP+F5UfxQ1ArTval5iMaQUsiwUSk4vh755fujpPb8RkZ0+cC9N5gCVd51999Rqffvo8E5kiAV2QLwsuzrjcsj3GtfkiZcfl3bftWHR3VOsCnUqXSIS8m3/RctGFF2d2pRc7LZQdfnBxjnuOCPb2RxhPl7g6V2B3X6QuNl9787x9V5IXLs/jOJ67ujdiMmuVCRoaz1+a45btcTRNW1KW1VdS8x/LFC0ips6LV+ZJhEymMkX6owGyRRuJJF5JXMtWgtrxkEEi5O3EL0xlyZVtAoZOqNrVSmA5kpmcl6AXqoQEaqVcf3Bhmi+/+HpVttWvLf+FN+/iyqyF47hYjkPZlkg8N3nQ1DEElF3JOw57ng/fEK9ExAdY1tDs7AlTthzOTma9a8bQmEiVMAyNW3ckqmVeK8lmfuToEF956Rol2yUS0LEdie1KtieCTGRKmzJ+7NPK7rqTdd6NbDZ3/lamlTKyx4UQAcAPnJ2RUlpLvUbR3TS6o48fn+V/e6A1mdha9xnSi6NemM6zfyDCTM5a1HXu12cjvJ2k5Hp99kzO4siOOK9cTVd3xf44m73vcDKEaQjSRa+dpCM9+cuSJdErsd35fBm7YvRu39WDU8muD5s6AeO63OeJCzO8fGWevliAg4Nem9EnT094GdshE0MTzBdtHFcS0DU+WYmnNp7LZp2qAE6+nsKWXvOKouVUM8AH4yEEnt43UC1v8rtkPXJ0iF//4qsULYdY0KQ3EmYu72WFaxW3s8BLFHOl143rxIUZciXPYMaDnsHSNI1k2GtM8vRr03zw2G4+/c3z6JpGIuS1CdU0we7eMGcnssRCBpOZAqdHM8zlLe472L9gF7ycMVnO0HgLgDyHtsWqdddlx+UtB/rqathXks18eDjJ2w728+TIBAXLJRzQGYwH0TWB43jNT2rZTAan1d31ZsmU30zu/K1OK1noDwCPA5fwvJC7hBAfVGVkimbUus/iYbPS+Uswk/OUsBbbtdVmm6eLOo4jMTRJruRgu0WmMiXilThxMxdno9vuTbt7OHF+lnTRwdSg7ILQYHsiRKZkY7mSvf1hDL3AYDTkSaROZpnIlDgWNquKaX6jjXTB4sXL8xwaihEyDd60O8almQLRkElfLFhNfGvGYklIEVPj0FCMs5NZSrZL0NBIhAwm016Wtu24zOW9ntz3vaG/LoRxeDjJw0eGqm5hX/ilaHlCMvu3xdjR43CoFOfidA5deLKqrpQVERav9t0vT4sHdV6fzfP4iStYjsvOnjC5kk04oIOUjKaK9EZN5vMWf/HyGLGgwRt3ec01aj+LxkQwPxGwluUMTbO66+lMkWzJ5tkLM9X6+XcPOisyCr9wbA/j6VK1w5fA82wkw8aCHWOrBmcjaq830+66FTaTO3+r04oL/feBd0opzwAIIQ4BfwrcuZ4DU3Qnte4zPws9qAtShfKSMfRr8wXChsblmTxF26Fsu4RNzRMUcV3iQbPOfQr1Ls5Gt92de/qRUvLi1TS266Jrgtt2JDh2YJBM0TME+wdigLfLGoyHCBg6xypxuseePIvruoynS1iOS9F20QSMZ0rV0qugoTVNfGu2OGnmov3hxVnecXib11t8Kke2aBMN6gQLgkjAoGy7DCW8jmzT2TK37qi/cT9ydIgT56c5O1FASomsyKDGQjofPLab6OyPqx2trlX6gQ/Gg1yczlK0XRIhndl8mWjQ223OZMtMZ0voQni14QGdmGtguS5l22XfQAzLkRwYjCKE4PJsgf6alp9A22qRG418bbe2eNDzkOTLDoeGonWvW652/NF3HeLzJy7z0tUUEsm9+/t468H+ai/3lRicjay9bufuer0XHd224OhmWjHgpm+8AaSUZ4UQ5lIvUNy41LrPBuPezvbUaBpNeDKgi32Rg7pgrmBRtl1ChoYuIFtyCBgaPZEgd+/rrXOfNro4g5X2pGXHJREyObgtyhuGkty1d4CPPnxowU3rI28/wFMjU5UEsIV1v6fHUlyZyRMy9UrGuUvRckiGDT5wbA9//3Mv0hs2kBWBktrEt0YWiwlKvPf19dXB0+je1Ret67zmn8/a8i/wbpQDsQCGJrAcr0QtXNE+/4/HL/Bzuwq8dHWe229K1onqDCWCXJ0tYrsutgMTqQIT6RKmLtCFwEWSs7ya8v5okDfvH+D0WJpdfRFGUwXiQQO/n9G5qRz37utbkPAHS8eSV2pEzk7kuGNXD+OZEtmiTSJsEgnoC8rPljOoh4eTfPy9ty04fmN9fCsGZ7PWXi/FRi06Nos7f6vTigF/XgjxWeBzld9/Hq+9qEKxgEeODvHJJ84wmyt7TTkMjb5ogN945OYlv9BeCZRONGCQKVlIRxAJGhzb18eh7YmqghjAdLbIyWtpyo7LY0+e5dBQlNFUsVq3XmjSfrPZDWX/YKyuj/vde3uqcepTo2mCleYfQLXHdqpgc3g4yX0H+3nh8hzXUp6k6fZEiELZqXY+q2WxmOBiymhvOdDHVKZY3ZXHgjqJymsbDd9r4xn2DUQJB7yvcr5sc2Umz7X5AuZejYCu8dzFOcIBvZrVHQ8F2N0nyJYcz9vhSoKmJ07jSsiWbFzpUrDh7ESGXMnr050ueE1fXFfSUynpyxbtqru51eSl1RiRa/MF9gxE62RrA4X0gvKz1RrU1RicbkzW6sZFh2JxWjHg/wD4h8Cv4sXAnwH+w3oOStHd+DW+sqIvVlvzuxhlR3L3vl4uTOcxdI1YyGD/QATbrY+plWybH1yYQwB37/PkST/99Hlu3h7j3v19VaMXDxkMJYLLxlovhwx2hsKcGk3x9VPj3DwUY3d/FA2YyZYxdY2eiFfSRCXZDGBPf5i/fHUMISAa0MkWLV68Ms87b10oULFYTPDD9+8DrmekB3VBPGTw3R9PU7BcBiplX+miTbpo89VXry3oZDadLaNromrAZ3NlDF0ghEAIydGdCU6cn6VkO+jCK5GTUnJ0p6ea9uH79/HZ717iW69NeF3IDI2go5Epul5pmu6ViuVtl6jjEgsYjFUWLSFDw9QFl2dy7EiGOD2a5pWrcwQNHVdSlXCtNbr+fFdiRHwhnZeuzDEQC9Z1sGssP9tIg9qNyVrduOhQLE4rWeglIcQfAk/jSamekVKW131kiq7E7/V9tEZTOlWwll3h+zfDYzUtH2sT3h46PMjjJ65wZjxNyNC5c08PQwnvRmm7XivSYwcGqq5ovz3pUoyMpZjOlknpFunKTvhMJdN6Z28EV+YYSxWYyBSJBQ3eMBjl1h1eotZXXhqjPxqgaDsULJeSLTm6I17n0vVZLiboJ3995pmLDERNLkxlsR2XXMlmKBEkHDC4eah5q9OhRJCpbJlIwFORy5cdpISdvSEgz0AsxL37e3nlapodPSHSlaQtXzXN15IfiHrlVAKvvMrQBSBIhgwGYkGiZYeZXJmbesMMxYPM5cuETINbh+OUHYlp6OzuC/HtM1M4LoSDOoamcXk6y7saVLeWMiKNHoZDQ1GeGpmqtkpNFaxqyd5gqF7Ra6MNajcma3XjokOxOK1kof8U8EfAebwd+D4hxN+XUn5tvQen6D5Wu8Jf6mY4MpbiqZEpjgwnyBQsTF1wYTpPbzTAQCxUKTWrX1O2clN64uQE2zUvKW46W6JgORQtl2+dmWL/QIRM0WvLeWhbjEzJ4fx0nvfdvauaMe9LqoJX7lWw3EXnuZyL9omTEziOJ1biuJ7vouy4jKUKPHR4iN39UV5+PbWg1elde3v55mtT1TkHdA1TE9x+UxJPg8nrpvbwkaEFEqp+uCCoe41IcmWbsu0lrAEMxAIEK41OgoaG7boETZ2y7dIfC/IHP3s7T5ycqBqE5y4VMXUNhKRkOYTCGqau871zM9UmLbC4EQnqYoFr/dPfPM+hbTH2DcaqyX6z2TJj6RIDA/WKXs2uoauzecqJYFWmdi0JW83i9t2WrNWNiw7F4rSahf6glPIcgBDiAPBVQBnwLmU9s1BXu8Jfapda20O7tjTN7/e9PeHtLFeaRXxtvsCOoGAqUyRVsBFA2NQolB1GxrKeCpoQ5MouibDXqcyX21xOn32lXJsvcGp0nrF0sdrmNFBpFnJltsC2RJihhKf0Vi/NavDQLdsYiIeqbni/KYeUNM38b9bYIx4yuW1ngEuzBVygLxLg7n29nJvMMZfzeom7Egbi8IaKQfW7u/kLtol0kVBAJ6ZplB2XvQMximW72t3NZzEjEjY1HMfl9FiabKWV6bW5PNPZEuOZEgcHoxzb31/1roTMenXnxmsoqAtcKQkYOv0xY00JW0s1V2lMLtzMqAzxrUUrBnzSN94VLgCT6zQexTrTzizUZguBtazwF9ulLleapusaH3n7gTot61ZuSjt7wrh5ybmpHH1R0xMMsSVhUyNbcgCNd926rc4t7x/fsh3OTFxXCmumz74SgrrgymwRIbyWnbYLti0JaPD6XIE3DFl88NhunhrxdtuNsfRGUZsnTk5gZV2SsYWZ/7Ux6OlskXOTOaazJfpjQX7/Z70M7c88cxFT1+mLGJy6lkLTBDf1hKqdzXy3+IIFmwTHlQQNzf8VgVhwrTx0eHDB5/Xbf36Kc1Oebr4mJGXbi79LCSXL4cUr81WVPG9BOFc338YF6WNPnsWsKOrByhO2ahtyvHh5nkTIYDAebEtzlU7QSo2+ortoxYCfEkL8FfBFvO/j+4DnhBDvBZBSfnkdx6doM+3KQvVlT/2mE2Xr+o5krSv8xhtNQBfVnedSpWmNsefleOToEC88e57ZbJneiHceZnNWZVctcFyXc1M5Xr6aqiZkxUIGU5kiL11NETa0FemzL4Ws/DiVcjBdeP24rUoDX99It1Lu5C+Ejh8f5c6bhxYowPkLoulskRcuzxM0NPoiZl2zE/8z/OHFIjt7w5i6RrZkYTkSTQgeP3GF/YOxugXbUCLI1ZkCmiYYjIcoWg7ZksMt22MLFo1PjUzVLTxGxlKcm8rhuJJIQCdV8JTyTF3Dcb1kyEYZ3okzo0suSNeSsOV3aZvNlbk2XyBbsrFsh2jQqPZlH0sVMQ19VZ/3RrNZ+4Ur1kYrBjwETAA/Wfl9CugDfhrvnqMMeBfRjizUWtlTv+nE2cksh7Z5xuWjDx9aU5yx8UYzkS7hSsme/ijxkEHA0Bd0wFoNh4eTXI4F6IsFmMmWGIgFuWdfHwOxEK9cnePFK/OkCxbxoE66YHF5JseevghHdiR5y4E+RsY8OdG3HeznF47tWZMH49RoirApyJU8QRZNE5jCU2Pb1RtuqnK2VOhjZCzF6HyBf/W5F+mNmAwngxw/k+ELz12hbLleDXhFSrYvGmC8aOFIuDCV5fMnLvPx93qSsP71MpMt8eKVeZJhjYAumMmWFhj74WSYXMkhqGvYridbu28gSk8kUNWWh+aLxidOThAJ6ORKdqU2X3gJdY6kPxYgZOoLZHgnziy9IF1LwtbnTlzmykyeWMir9dc1KNouE+ki+wdjBA2NmVyZYwcGlj1WK5//endAU+VjW5NWstD/7kYMpBvYyC/cerFUAtFjT55taW61sqe17R7H00UC5tp2JJ8/cZkLU9k6QZZdfREs26k2F2ln3C5k6vzOe45UFw3xSqOQ8XSJN+1KUrAl6aJ3vqTribb4N8KhxPVzuRrjXbtQ+fFEBtuRRAIaLuC6nis9FNDJlmx++fHneH2uUC1zW2oH5R/7qObQE/Y01r9/fpZk2CBfsimUnUrynSTjuKQKFpGAzu6+CFJKvnNupqpv7l8v56ZyVeW5ouUwUKPAVrtgq3U7pwpeHPvlqylu35XA60rs0bho9BcKXrtUG5BomsDQBDf1RhaV4V1qQfqh+/auOpzz0tV5YkGvxWzI1JGuJOc6pIsWUkrSRc/rstqQyUbviFX52NZEW+4JQoh9Qog/EEJ8WQjxF/7PRgxuM+F/4VIFq+4LNzKW6vTQVsQjR4eqiU1+W8urs3lGU8WW51abxOXjJ3GtpRxlZCzFd87NIKXXhatoObxweZ6SbVNyJB99+BCfet/tLe3wR8ZSPPbkWR790is89uTZJT8nP7EnGTYZSxVJhk1u6g3zE7t6efP+ft55ZDtv3t+P0KhmaPus9iZYuyPShODozgRB00DTNHrCXp/tgKERMTUSYbOuzG02d717li9h2uzYfue1bMnLTp/PW15bUF0jHqyECaTXoa3suMwXLAqWU9cL3b9eZrNlArqodjY7uC3adO6Hh71GK7GgyZHhBIeHE5i64AcX5pjOXi/ra9wJ7+wJsz0RQgjBQCzI/oEohqahCa2ulW2jwdzZEyZTrNeg949d+7m+Np7m9FiaTNEraVzue+vt/z36ogGEphHUvV7rszkLJHzk7asPmTR+/kt9nu1gqfOk6F6WNeDA/8RrZPJpvIx0/+eGYqO/cOtFM2M1lAgSDxqcHkvz9Mgkp8fSOI676Nx29oQZToaq8qL+jmQtSVzgnePeiFkRIfF29kFD4/RoZkU3mtUstg4PJ+sWCLfuSC644QUNnYBx/SszlSnyzNkpTo+mFywSlltAXJsvVHt9AwzEQhzb34sQEA7ovGFbjO3xEPFQgFt3JMiWHBIhr9b7laspnr0ww7MXpnny9EJj5B9b10RV5tXUvf8Xyy6W41J2JaYuKt3LvPrQkuUykS6xPRGsGmb/eumLBZjNe/kBd+7pWdD7vJbG78qtOxII4OS1dHXR2GiMHzk6hK5rHNrmuadLtqQvYvKWA31YjufybbY7bbYgrT12swVFK9fDHbuSnlKd5RA2dQZiATRNY3syxE/dNsxjP3d7XWncSmn8/GF9d8TLnSdFd9JKDLwopfz36z6STc5WckE1Znv/8uPP8fpsgaCpEQt6zTPOTmTJV1pZNuK3e7x5KMZYqshMrrzmJC7wzvHh4TgvX/VurEFDAymZK9grutG0I97XLJt+IBas3vyKls1zF+eQwL37e+tcoLB8U49moYyhZIR3/0SwWhJ2qpTi7j09DMZDxEI5SpaD7bpcnS2wbyBKUNdAiEWPPWx4ymu6EGSKFmWnoowHFMoOjvQMt8RrxhIwPdnb8XSJW2rO0+HhZNMww1KNaRq/KyFT4/xUtmq4agV7/Pdo7EDWSoiqlbKo1VwPfuey6WyJdNFfuPTy6LtWn99Ry0YLqqjysa1JKwb83wkhfgv4BlAtvJRSvrhuo9qEbGUFo3TRBkE1lu3rfqeLzdtj1t4MTEPn2IGBtuQD+Of4Tbt7qpKopi6472D/io7djsWWP8fPnbjMUyMzCAR37EryloP9nJ3I8YOLM8RCBkd3JhiIXX8v32uxnMFYSl7Vf85jT56taqX75XPTmRKhiheg5EjetNtr6dns2CIkuGN3ku+fm2Ey46IBopLdXoeEbfEgQwkvc3wuv3BnthIDUPtdmcoUefHKPCXLIaBrDMSCIL02ns0ajazmGlrudau5HvzOZeuV89IJQRXVYGTr0YoB/wngA8Db8aRUwVu0v321byqEeB/w28Bh4B4p5fOrPdZGsZUVjJJhg1S+TNFyKu5LTy87GV788liPm4F/jpNhk3v39VXP8QeO7VnRcdq52CpYLvfu669+5n75k28UanXea43CcDK0aEMSaM0g1l5z/bEgh7bFGJ0v0B8LEDR1bt2RYDAeqtanNx77R89PMJ0qY7mSgCEwdY2i5eCnGboSdM07X5YryVRi5W9bZMHUymc+MpZiKlPku+dm6I2YOK53y0gXbQbjXuJb0XIYz5Q4MpxYUxZ00XJaSrxci7jQehk8tSNWtINWDPjPAPvbrH9+Engv8J/aeMx1ZSt/4Y4MJ4mYOuNpz12YCJns7Y807ay1nix3jlutAmjXYuvzJy5z8to8c3lvFzycDLGvP9pSidLFqSxnJ7MEDS8skS7aTKaLfOzLr1JyZJ3wjT8nf/deWzKWKVpcmy+QDBscGU4SDxlVcZKpTJETF2aYzZbpiwWqmeP+Mc4aGldnCmQKNqbmtQrVhSAe9krxMkWLgKHTEzYpOd5CJVWw+IUmC6aRsVRdH+07dvXwgYbSudrMar/M7uJ0nn39EZJho3qu/C5mawlB1erY+2GKTz5xhh3JUN359WPgm3HxrXbEirXSigF/BeihjeprUsoRoKoj3S1s1S+cH9M+PJyou8H5btTGnc6hoWiditZKXIvLGeHFzvFKym78hcDnT1zm6ZHZqsFZCSNjKZ56bZKS5RIwBEi4OlsgU7TJWw6//s5DTY3C3Xt7+P65Gb59dgpD19ieCFIoO4ynvf7bT45M8I7D26oGRxOCXX2Rujk9dHjQ61XuuKQLFrM5i3Te4m++cUfVGM1mS5ydyILwdtHDiWDd+RgZSzGZKYEIEg/p5EpQsBxAki87gCBg6Nyzt5fRVHHJfu0jYyk+9fWzXJzOeT3FEfzgwixjqWJdm9jPVUoALUcSCxnsSIYYTxW5lioSDuikChY9Ea96IVY5Z8vtghe7Xmp17AHKtsOVmTyzuTL3HxpccH20uvjeCqWiihuHVrLQh4DXhBBfv5HLyLYyzTLTaw3BdLZczei+NJ3lE187w8Wp7IrL6dZSireaKoC85XLPvj4eOjxEwNBXVPb3xMkJkGBonuvZrDT0yFZaezY7Z77hNQ2dvqiJrsGl2Tzj6SKmLogFDSxH8tKVFJbjMJsrM531SsJmcyVGxtK8+vo8H/+r15jNFjk7maVku/RFTRB44jl4qmzjmRKW69Wk37W3l70Dsbrz8cTJCaT02p/2x4IYukbY1BFCULS92u9kyMuq3z8Y4/d/9rZFy/OeODnBdLZEPOT1C5/OlZnKlvjhxRn+wedf4LEnz/LVV6/x3XMzICWxoM58rsz3zs8Q0MFxXYK6xuh8gclMkZLlsj0eXDYLeqnr5dp8oS58cW7KW1yUHbfp9dFYZbDUIrHbS0UVNw5CysaMloYnCPGTzR6XUn57mdc9BWxv8qePSSn/vPKc48CjS8XAhRAfBj4MMDQ0dOcXvvCFJcfbKbLZLLHYxrqcN4LJTAnDLWFrQQCyFaUsXfMMElD9fVs8uOyx/Of6tPra1+cKmLpGrdNGSrAcl5t6F+7iWn2vxT631+cK5MveXGs9Ra4riQYN9vRHlnzPbMlGSolVyRiTlYxvBJiVft2+RGg0oJMrO5VyLq/WWghPxETXRfV9y45LQPfqwgtlh5CpL3o+Xp8rEKbMvO0ZbdeVWI6L43o7dlPXcKWX1LYtHqzquZdtl4ChkQgZ1aTG1+cKFCwHAZX5SFzp65xDQNewXBcBGJqGrgvKtqdhLoS3CNI0gWW7SCBs6oQDet17NGOpzxCoXpeW45Iu2Eg8mddEyMDQtSWvj0aKlsNoqojteK1Ug4ZWlXFd6vosWs6i520tbNX7Cai5rZQHH3zwBSnlXc3+1ooS27eFEEPA3ZWHfiilXNadLqV8aGXDXPQ4nwE+A3DXXXfJBx54oB2HbTvHjx9ns45tLTz6pVe4OzjOaMgrj/rGhXFiAZ1c2eXhI97uye8O9amfvn3ZYzUmfrX6Wj8jOxm6HnNOFSySMZNfeGBhm8z/+do1huJBDm7zvkznpnJkChYFy+atB/qqcdLDwatNP7fHnjzL8TOTFMoOs/kSuZLXZzsS0Hnk6HY++MBtS85v2i7yvR9PM5YuAgJDAw3BnoEokYBeTRgDqupm/r/T2RKZok0saLCrL0KuZPP6fIGALuiLBnjz/gGevTLDzdtj7B2ILWhI8jtvPsJLJycYyJzjS9di1XagF6Zy2K7kr//E9mqDllTB4mLBa4WaDJvEo5VwwOz1jHj/XIynivgueNv1JF8lkp5wgLLjIqTElrA9EWI2V0YISdmRvPu2YQZioeuf9d9a+rNu5Xr50H17eeHZ7/PD3CBnJ7JMZUs4rstQRQzmzj09mLq+4Ppohr/zfvX1efoiJmXHU9zbPxBhOltmIlOqhi8Wi/k3O29rYaveT0DNrZ200g/8Z4HfA47jLbg/LYT4p1LK/77OY1O0SLvjdrXHuzKb5/bB6/XgiZBJqmDVZVS3muG9luzwVhKRam+oQ/Eg6aLN989NQ8Wlmi2Wmcxa/PkrY+ztD1O2HAbD5brkr9r3e/X1eX6cyWA7nuH2pE51JtKlpq+pnZ+3+xQIAdKVBHSdsiMp2y6a8HatfdEAmhBcmsnRFzGrKmf7+iM8f3meKcur2iyWbUDQGwmQDAdIhk0ODcWq3dBeG88ghMDUNLbHg9U4+lxW1NXql12XY5V+4icuzFSz4yczRR64eWjRsrdHjg5x8lqK81M5wqYnCOPplUNA91TcwqZOwXIYTgTJlLyyRCkFu/vCSMmiyXarvV58HfvxSS+UMJwMkSvZnuyplJy8dr3pyXL44ZmBWLBuIfWDi3MMxoMMxYNcnMry0S+Ms6s/zJHKOVH64opO00oM/GPA3VLKD0opfxG4B/iXa3lTIcTPCCFeB44BXxVCfH0tx7uRaXfcrvF42+NB8mWHS9NZXCnZngiSK9lsjwdXrOi0FjWopeL0PrU31DcMeTvvTMkhX7bJl20mMmXCpidPOpUp8aNrKQplh3/yxYVqaYeHk/zGIzfTEw0ghGeMd/dF+MmbB4kFdX7rL04vUFmrnd+5ySxBU2c4EWa4J8zu/ig7kl7cd75gc3RHgt945GYefdch+mNB5vI2QVNnX3+E6ZzlxdCFF7LIlR0SIc8VfnBbFIA9A1GSIYPnLs8xOl8kXbTYNxBh36AXCz87kWMgFiAWNMiWHAKGxmA0wOmxNF/90TiXprMIPAW9ubzNeCrPsxdm+MbpcZ69MEPJtuvU2B591yH29UcoWm7VrRwwvFBA0XIwdYEmBKGKG/mtB/oJB3T6IiYvXvaawtQm27VyfS53vYRMT7/9r//EMO84PMSxA/0ETR3L8cINre6EfXGZg9uiVXXBdMHCrpTA9UcDnJ30EgZTeav6HTs1mtpQNTWFopFWstC1Bpf5DK0Z/kWRUn4F+MpajqHwaPcuoPF4+wZjRDI6Y1MlTENn70CMd946tOLe27D2UryVCHYMxELsH4hwecZzGxfKnpGJBLxLPl+2KdmeNjiSplnth4eT3DKc4MFbtlXduNPZIq+NZ3BcuHdf36LZzhOZEkPxIG/Y0wPAuckcJduhP2bw+z97W908alXOTo+mKFkOJVvSHwt6bUalpGRL3naopyoac3k6R6poEzZ1diZDpAo2P7w0x2vjGQbjQRJhk8M7rifylWyb75ydZiJdIhLQEOiMzhdJRkx6wgbPXphjT3+kqkH/gwtzHNvfV3fuP/m+2/jon71CyXJIVXTTXSkJ6F7/9Hv39XJt3sto96+Tx09cwXJdBmJBDm6LMhDzFpmtXJ+tXC+1u/TBeIjBeGjFDWb8YwzEQty5p4dzkzmyJZt4yKyKCvlhiEzJrn43rs0Xqm1u/Zr/Zl4GldmuWC9aMeBPVHbIf1r5/f3A19ZvSIqV0IrK1EpuIM2OFzC8nc6n3nc9drnS3ts+vhH2x1Tbp7pdSm7+DfXCdJ5owMBxXQqWS9l2cdyy5w53JRFT8xqn1CxYGg1Loxv33GQOIQR9sevZ8LWvq11k1L7ON1zNDEutobo6V6BkOVXRk5LtYmqCsiMxdR1XSjJFm7MTWW7eHmM8XWIuV2YmV/b0zCsKeumizWyyXF2MPXshXdnpW+TLDkXbxdA0DE0QNHSvQUcNvsSqj/95zefLaEIQDujomiBfdgiZGpGAzrZEmKBp1C2CvnVmmnv39S0qeLMcS5UVTmZKnJpN1XVpW02Nd214pi8a5PCwV/J2aFuMwXjI6wcf9NQJE5UcjHjI8ASQCtaSJX2wvKyuQrFalt1JSyn/KZ7gym3A7cBnpJS/sd4DU7TGcl2GVupib3Y8V8q2SsauV7lOowu7ZDkUyjbpko3tuujCy0r25+dKL4/64KDnlm5mWBrduNPZEkiqr2l8nd/E5NSo13DEDz0sFy7wy5x29UXYlgjRE7neqjUcMOiPBurCB7v6Pbf8wW1RZnKexpKpi0qtN9w8FCNfdqou3nTRIlssU7IdbNfLQg8aGpOZEqmixe7+MEFTZypTYipbwkXy0tV5RsZSdZ/X7r4IibBJPGTy8JEh/sYbd3j5BSWHH16cJWLW31LWowuWPx7HlRweTnBoW4wz41leG083Da0s11imWXjmI28/gK5rpApWVYjH78Lmz+FI5XVLlfRtlSZIis3JojtwIcRBYEhK+T0p5ZeBL1cev18IcUBKeX6jBqlYnOWSu1bqYm92vEFXVg1PO9yBKxnTSt6v2U42FNAJSr3iMpfoAk/YpOxg6BrRoF7NyG5mWBrduP2xINvjwepral9Xm0R3eDhBxNQ5M+41hTkynGwpXJAIGaTzVp2sLRK2J0N89OHr2dR+Vv5ALETI9AxNpugQMHT2D0TY3R9FzlN18RpCcGHeS4rTNG8hky856JX/HxiIsrM3QrqSoCilVz73mWcuEjG1utyCFy7PA3BuMsv2RIii5fLWA/3VHXDtDnM9VND860cXXtx932CMvkp/8tpzBK0LADXb6e8fjPG5E5eZypaYzZXZngjhuNcXY/7nubsvwoHBCBem8rx4ZZ5EyGT/YIRr897CZas0QVJsPpZyof9b4F80eTxf+dtPr8N4FCtkuTjhShs5NDveAIGq27sd7kB/TH75U7poEQ8aJCNm3fNW8n6Nhr4vGsBxvR1z0BCEA0EyRRtXSoYSYYq2w97+KKY+W3VLL2ZYam/utbvRRoPULH9gKcPSbGFy646KrG2m5GWJhwz29HnJabX4hnEuV6JQdjA0jWDIE5C5MJ3H1DXuDGo8e2EGy3FJ5cs4ridyEtQFOcsFAYbuJZ29Np7hR9fSSAkBQyMc0HnrwX5MXeeHF2d5x+FtANU48Y8nskxkSiAEb9rdU5XdLdsOF6ay/PoXX+XhI55UbLtV0KrXdOn6Y4td02vNESlYLj95aBsl2+b0aIbvn5/lvoP9dddgUBecuDBLPGTU5RDcsj3GdM7ipStzdTkAjR4yFR9XrJalDPheKeWrjQ9KKZ8XQuxdvyEpVspSyV2rKd1qPN7x46NA+xLmdvaEuTSd5cyEpxUeDxpe3LZg1yX/tPp+zQz9TNbr2qULge1KBJ7bOBE2uHd/P5btIPGyvJ++NMkdu5JLLkRqb7RhU8OyHcZSdp1B+ux3L7W0WFpqYeLL2h5ZRNa29jN66PAgH/+r1yhWms8MxoL0RgKkizavXJ3nTQfg0LYY4+kik+mSlzmua9iuJGzq1VyAwXiI+bxFquAluEFFdKYyfoms7uTBM+KmrnMsbNYtEP3OY0Fd4Eq3bl6NC5hm57fVxZp/TXM9rN70mh4ZS/GN0+MgIR42OTgYZTAeajlHpP76M9l28/XvUu2YfEGbWsq2y+nRNLfd1EO6krn+/KU5btkeR9M03n/3TUvOWaFohaVi4KEl/tb9PTRvENZSutWIX25Ty2rcgY8cHarWMFfdxMChoVhdbLDV92sWZxxKhAgYOgPxICVbIpH0RUwSIZOrs3lGU0UChk4iZHLPvj7ylstiNMbsA4ZO3nL50H1762Q5W433LhUXXa5czo/n/r3/+hyffvo8GnDrcJybesPkyp4ITCJkIDRBwNCIhw2EEARNjZCho2kaIdNTQZMSNCGQQH/UJBYy2NUXZe9AlETY5NxkjiszOTQh+OZrkxw/M8lkplB3DdXO2c/W9uruA4vGe5vFpFcSK/avaa8Wvf6a9o/9y48/x0f/7BVcR2LqgpLl8OKVeaYyxZZzRFq9/sqO5O59vQRNnWzJIWjqJMMGhq6zbzDGm/b0kAibOC6MpUvVz1PFxxVrZakd+HNCiL8npfx/ax8UQnwIeGF9h6VoF+3sotauNp2Hh5Pc1BsmXbDIlhxiIYNbdyTojwXrbo6tvl+zMMGRHXG+d26Wtxwc4Cd22oyMZZjLW9yzI4HEy6xPhk1Ekaq622KehFY9Aa3Ge5cLa7TS0CWV93agBctlvmDRFw0SCRgETZ0jwwl+eHEWVxZ54fI8QcNrqnJ1tkDZ8s533vLc7vfs7eXybAEpPAU1fzEV0AWXprOMjKVJhLxOYkXL4XvnZnlbgwvZn3OmYGHqXsb80Z2JBfNqnEOtwcwULQ4PJ5YNq/jnx2+XOpYqVq9pfyzJsEm60ke95LiUHZdE2CSoC06N1gu8LPXZBnXBM2envNeHTA5ui2Lq+gL39+nRNKYuqu1dAf7y1VH6owGAanmbryLXSnjrjsEF01YoFrCUAf/HwFeEED/PdYN9FxDAazGq6BLa1UWtnQlJt+5ILjDOqYJVZ5xbfb9mhj5oGLztYH8lQ9rmgZu3VV2jj37pFfpjrXsSWs0jaHWxtNqFUK2xyZTsyi5aMp0tEwkYBHTBbKXxzB27kuRLKeZyZRwpCRgaPRGTouUSCeoEdZ1DQzH2DEQZTXm70mMH+pDS20mPzRdIF22G4kG2VQy7I13u2JVkIB6qq5X350xF8ObOPclqvXrjvBYzmNfmC1yZyS0bVqk91xPxYJ0E72NPnq0eO1tySIQMgoanZx4ydVKFMprQ6hYfi322p8dS2I6sqtUVyjYnzs+ybyDK+991qG4hcvuuBD+4MMcPLsxy975eQqaBqWtsT9Qft/FcLH0dzC15LSgUsIQBl1JOAG8RQjwIHK08/FUp5Tc3ZGSKTUc7d/OtGOdW32+xYy0W016pAV3J81tZLK12IVRrbBIhb0ecDJtYjkvI1Kta6B++fx8XprJcOnkJx3UxdY2y5ZJ1XI7t7yMeDvCh+/ZWz+utOxJMpEuYuk48ZJAt2pyfzCKEt5vOlx2ilcY1Y6kiplHfrMOfsz+v2nr1xnldmy9gaHB6LF1N0ts/4PUL98MqtuMynipStB2G4iE+d+Iy//q9C7Xnlzo/sZBBqZLJnynZvHl/f9P49WKfbapgc2Q4wXAyxLmpXLV/+VAiyOHhJB/78qtcmMpWd+e3bI9xbb7IK6+neOeR7Xzk7Qd4amSqabJjK9fBxJnRZeerULTSzORbwLc2YCyKLqBdu/lWjXMr77fShUXtjXMH1JUFLff8onXdHf+2g/0t63qvZbw+tcbm4LYoL1yep2R7KmeHhxN1i5YnTk4Q0zV0TcNyJUFTozdqMp4ucUuD4AxcdwefGvWEUXRNoiGZyZWZy5fZkQzRHwsykytz7MDAqucV0L1e4rGQ4YmjWA7PXZzj3v3e7n9svsDr80WChuCmnjCGJvjuuZmWznPd+RmM8uKV+Yr4irHoZ9xoRC9P5zg7kSVVtEBKDm6LcWx/P3C9kcrIWIrvnJuhJ3w96/zCtMUdu5NYDtWEvf2DsSXPhX++1tK3XnFj04oSm0KxLrRrMbDSY9UaGivrkoyZSxpQ//mfO3GZ75+fpTdi8taDfZiVHuOrUdVazdwbFcNurjQ0SVTcxo3lg3cEdXqjgaoMaNFymMsvncA4Ol/EdSVFS6JrGrbr4EoYTRUplB1KjuTUqJco1qzkabl5NSq8wfUs7lt3JJnKlNjbH6m25CxaDr0Rc9H8hMXOj1evH+BHo5lKxrrgg8d216kA+ob1ocODnJ3IVRcvN2/3GsCkizYvXpnnTbt7GIxfL/964uQEvZXYvC+2A3B6NMMDN29r+Vz4+HK3/i78M89c5F19zrKvUyiUAVesiXbVsa5XPexix/V/jh8fband5BMnJ3j5aoreiFmXrAQb132qcYe7dyDGrzxwYNEwgZYX7B+I8PLrKXIlm6Chcdee3qY19H48t1C2mMiUql3TwqaB40oKlsN0rsxb9vdVd/urWbyUHMm9+3u5MJUnXbRIhEyO7IhTciSHhqL88fdyCAGRgEEsaKBpgjt2J1uqdKg9P6fHUoyny7z1QD97BjyBmadGpgB4amSqLonuqZGpaunWTb0RkmEvI79WsCZg6NUd/Ge/e4nDw3FevuopugUNDaRkrmCvuLpjsZyAdEM1g0LRDGXAb3DWYjjbJezSruOsx3Frj+FKF6RWtyvbaFWtVnd1jxwd4offP8dr41kGogEGowGyJa/v91dfvVbXjGY6U6zE0h3SRQfH9RqUuNJrFWrqgpCp0RcNcvtur8HJatXzfDf3mytuafBCGAKHp0am2J4MMZcvU6i0Vr13Xy9Bw2BbfGE2+lLn57Enz7KzJ7LAMD5+4gpHhhNNs84bm+HUCtYcq/Fw+HPwG51kizamLrjvYP+Kr9fFkujKucXLGhUKH2XAb2DWauDaJeyyXn2V23Hc2mMkw4GKxKng3FSuzq26kbSy6Do8nORFXRAPGViO9Er1dibJlWw+/fR53nygv/qZP/3aJAPRAOOZElJ6Dm5XSoTwmpQAJMLamkRqfDf16bEUV2cK1Qx4P3HLl2u9c0+vJwZjeI1mrs0XCZrGiisdFjOME+ki9+7rW/C4fy4bG9D4gjW1QjS+qz4ZNrl3X191Dh84tmdFY4TFk+h2GWtq+Ki4QVBXyQ3MWoQkRsZSPHl6gmcvTPPshRmms0VgdcIu7RKIWY/j1h7D7xctpSRTSYqqFcVZrmlGO1hJIxgp4f5Dgzx8ZIhj+/sZjIcYSxWxXVn9zMu2Q9FyGU97xtvUBaYmcKXXxjSgC8IBjVjQXLYsCppfU47j8ulvnidVsLhle4Kbt8c4O+nVmPtCNSVHEg8ZDMZDvGl3z6r6eteymKjOUCK0qNhOq6JHy4ntrITF3jMRUnsrxfKoq+QGZqU66T6+ETF1AdJLjnrh8jx37umpE7polbUIxLTirl2L8EztMXy36slraRCyLnFsvcIAjaxEXrbsuHztR+P0xQJVGdGZXLkqMAJezfdgzGQiWyZsaEgJIdNA1yU39YYpWS59sQAfPLZ72bIoaH5NjaeLWI5bHevegRhSwnimxLX5Ak+cnCCgi6pc62r7eteyWImWP4/Gx/3PsdXqgPWuxpg4M7/mYyu2PsqA38CsVVDk1h2JirtTENAFJ6/Vq1y1ymrropczmu0Qnmk8hqnr7B+MLTDM6xUGaKTV/u+feeYibzIFugbpgsWLl+c5NBRbIDCSLdqEAwa7ew2EgCuzeYKGRm/Q4MBAlLMTWZJhg7MTuaobfKUiNbM5q27RMJUpcnYii+W63Luvj1TBYiJdwpWSPf3RtnQtW8oYL1Xe1c7KiJWMtfE9J85s6BAUXYoy4DcwaxUU0YRZTeTJFCwQctUlVaupi17OaLZDeKbVY6zWm7FSWll0+eclKHTu2tvLuckc09kS45nSAoERs7LzPXagj4GY1yHu5LU06aLF2cksN2+Psbs/WpetvdT5a3ZNGZqoOzfnpnIgYCAWrLrZ6Ytg2Z4wzVpFgnz8a8D30nz2u5eqXprlmqsoFN2AMuA3MO0QFGmHu9MfS7syeGuNZjt2VK0co1068cvRyqKrtt3mQCzEQOy6DvdP3bazbgdaq8LmSln1MIRNraoXD617FJpdUx95R/2iYTZbRte8nAKfeMhgLGW33bCuR2hDtQBVbBaUAb/BWaugSDvcnc1o5Sa5UUazFTbinEBri67l2m027kynsiVOV5qW3LojWa11XolefOMYGz+r2kVDXyzAcCJY1UtvHF87aXdoY6NyHRSKVlAGXLFi2uGaXopWb5IbZTRbYb3PSeN7teLGdkJyUU3y2nN8y/ZEXd/x2lrnlSyOllp01Y65NpN+vT+3doc2NirXQaFoBWXAG1DusdZYz2SfVm+SG2E0V3I9rPWctOva889LY7vNlSTdrXRxtJKd6UYudtrtpdmoXAeFohWUAa9Bucc2Byu5Sa7nQqLV66Edhrfd197h4YXtNmtppSf5SozsSnemG5Xt3W4vzWYK2ygUyoDXoNxjm4NO3yR9g/yN0+MEdI2jOxNowmx6PbTL8G70tdfKOV6Jkd2sO9N27/Y3U9hGoVAGvIbNehO60ejkTbLWICMhX7IrxtVTCds/GOHa/HUlr+UMb6u7842+9m6knWk7d/sb6f5XKJZDGfAaNvNNaCV0exy/kzfJWoOsa4KxVBldExQtl6Ll8IMLcxzbf11LeynD6y8GXNdlLFXkpStzfP3UOB95+wF+6radda/Z6GtP7UxXTyfEXhSKZigDXsNWuAltlTh+p26StQZZAgjQNSjZXn/mxn7WSxneJ05O4LouZyayBA2N/miAdNFrJrJ/MLahGfVLtVVdzesaUTtThWLjUQa8hs1yE1rLDrob4vjN5gesas7t9jbUGmQpYWcyxHS2jBAQNHUOD8cpO9dN+FKG97PfvcRYqkjQ0AiZla5eIYPZnLWhGfWrXdSt9HVbYWfa7d4rxY2FMuANdPomtNYd9GaP4zeb36e+fraqg72eBqYVag1yLKiTLtr0RgPcuaeHgViobrft3+yzJYtr84U6IRS/lvqlK3N1OuAl26Uvaq57Rv3IWIrJTIlHv/QKV2bzDCeCK17UdcNi0GczVgIoFOuNMuCbjLXeNDd7HL/Z/F7KlgC47aae6mP+czfawNTuhBNhk3TR5uahGH3RYLXV4/vvvmlZIRTwFgNfPzVOumiTCBmUbJeS7bKnL7Kmz2M5Y+WP7Z6QZDgZ4uUr86TyZWIho6p+1sqibrMvBn1WYniXOnfdsmBRXgKFj+oHvslYaw/rVnsad4pm8yvZDmXbrXusVQOzHn3EDw8n+ejDh/jPH7ybx95/O3sHYgv6PrfSS/3wcJKPvP0ASK8jV9DQOLQthq5rq/48WukH7o9N1wSaEPTFAgghODeZqz6nlUXdYj21N8ti0KfVvvbNzt2nvn6Wf/HlV3n0S6/wjdPjFK36+W62BctK+sErtj5qB77JWOsOerPE8Rej2fyChr7gea0amPX2Nizm1m51d9rYPGStO6ZWdom1zUwADg5GefHyPNPZ0qLSqs3olqTOVj+LxnNnOQ4Xp71ObfcfGuTHExmeuzjHvfsFg3HveJttwdItXgLFxqAM+CajHTfNTsfxl6LZ/AZiwaq3YCVz7qSBWcnioZ2fRyvGyh+b7bo8e2GGdNHCkS4RU19UWrUZm30xWLQcHnvyLKdGU/x4IsPRnYlqiKDZZ9F47s5N5ogFdSxHognB0Z0JTpyf5dRomvsPBTflgqVbwhqKjUEZ8E3GZr9prpWm83uX10JypXPu5Lnq1OKhlYXDI0eH+NTXz7Ij7FAo2wQ0QdmCgXiQD923d0XnZ7HFR6fjsCNjKaazZVK6xe03JXnu4hwnzs9y7/5egobR9LNoPHfpokVAE8QqYZiBWIh79/fyytX0ihY6G8lmz3FRbCzKgG9CNvMOuh0sNr/Van534lx1avHQysLh8HCSoUQQ3QHbhVjI4NadSQKG3hZXa23SmKnD8TOTfOWla9x3sJ8PHNuzYYI727Xr8e579wtOjaZ55Wqah48MNf0sGs9dQNfIFm1u3Xn9eUHD4OEjQ23vS94uuiWsodgYOmLAhRC/B/w0UAbOA39XSjnfibG0Qqd3G4rNSScWD60uHMqOJBY0efjI9WQ5V8q2uFr9OKzlOLx0JUXQ0OgNG5waTW9Y2dW1+QI7gtcbng/GQ9x/KMhYqrio8W08d0d3JBhNFQkY+pJtVzfTd3+re+gUK6NTO/Angd+UUtpCiH8D/Cbwzzo0liVRtaGKzUYrC4edPWHcvKx7rF2uVj8O+8OL6apIjZSSbMmpZn+v93djtfNrPHeNBrrWGG7W7/5W99ApWqcjBlxK+Y2aX58F/nYnxtEKKutT0U42akf3yNEhXnj2/IoTA1vBj8OmixbxoHcLKdkusZCxYQlV7ZrfUsZQffcVmx0hpVz+Wes5ACH+F/BnUsrPL/L3DwMfBhgaGrrzC1/4wkYOj9fnCpi6hrjurUNKsByXm3qvr/az2SyxWGxDx7ZR3AhzK1oO6aJN2XYJGBqJkFGVP20XRcthOluu1me7UuK4koFYoO3vBZDOZCgSaPuc/HmUbAcpQQhwJUQCOpoQ6JpgWzzYhhkszXrNz6fV7/56cCN857Yi6zG3Bx988AUp5V3N/rZuBlwI8RSwvcmfPial/PPKcz4G3AW8V7YwkLvuuks+//zz7R3oMjz25NkFWZ/+77WxtuPHj/PAAw9s6Ng2iq02t9pd8D2hcaJ7jvLUyBTJsFm3m2u3q7TVa6ldrOfnNjKW4vMnLvOdczP0RkwOD8cJmca6nLfFWO/rcqM/r1q22neuFjW3lSGEWNSAr5sSm5TyISnl0SY/vvH+IPBu4OdbMd6dYrMrmylWRqOSleNKPv3N8ziOu6yS11pZL+W4TnB4OMnH33sb/+kDb+KBm7dhu9Qp1W0F1HdfsdnpVBb6I3hJaz8ppcx3YgytorI+txaNcU1dE1iOy3i6yL7B666v9TCsK6nh3WzZz4uxlROq1HdfsdnpVBb6HwJB4EnhBZielVL+SofGsixb+SZ1o9FMyao/GmA2Z9U9th7iGK3W8K4l+7k+PFBiZCzVFdfuZl2wqO++YjPTqSz0g514X4Wi2S54OBkiXbDXJWO7llZ3dItlP3/uxGUG46Flu5D5ht8pyKrh94+72QwkbN5yrVbYrAsPxY2BUmJT3FA07oIdV6JpGh95xwHOTuTW3VXayo6umZegaNl8//wsb79l26JGrll4IBk2+dyJyxQsd1MZyFrDt9p+5Z2mmxceiq2BMuCKG4rGXfCukKjecH+q04Or0MxLMDKWoTfSYheyGuIhg6dGZrh3X/+mMZCNhm+1/co7jaoTV3QaZcAVNxy1u+Djx+c23c22Wax8Lm/x1oN9dc9brAtZY5KcQFSz36ezRc5N5kgVymhC64jLt9Hw9cUCpAsW5yZzS3YT22yozmCKTrNuZWQKhWJ1+F6CZNhkLFUkGTZ528F+gkb9ertZF7LasifH9Uqf7tiVJFO0mc4WeeHyPEXLIahrmLrgM89cZGQstaHzayynOzgYBUm1X3m3lGvt7AmTKdp1j3XDwkOxdVA7cIViE9JMs7uVLmTNwgMAn3nmIhemsgR0T1as5EjetLt9HcpWQqOnYDAe4tBQjPFMad3beLYz6Ux1BlN0GmXAFYouoNUM9sXCAx++fx//5IuvgIR42ODWHQkG46G2dShbCc0Mn65r/M57jqzrQqLdSWeqTlzRaZQBVyi6hLXWJMdDJjPZEgKq+t6dcPl2yvCtR9KZqhNXdBJlwBWKLY6/89weD5LOe/Hl5y/Nccv2OJqmdcTl2wnDp5LOFFsNlcSmUGxx/J3nvsEYb9rTQyJs4rgwli7dUDXLKulMsdVQO3CFYotTu/McjIeqse+xVLHrjPdapGJV0pliq6F24ArFFmer7DybdZJbSRlcs/K8G8kDodh6qB24QrHF2So7z8WkYleShKaSzhRbCWXAuxzVTEGxHFul3EkloSkU9SgD3sWoZgqKVun0zrMdC82V9FNXKG4EVAy8i6l1KWpCVP//xMmJTg9NoajSGLv2F5orlXBdTCp2s0uuKhTrhTLgXUyjpjQol6Ji89GuhWZjEpquCeVtUtzQKBd6F6NciluPrZjT0M7Y9WbvJKdQbCTKgHcxWyW7WOGxVXMaFltoBnXBY0+e5dRoinTRJhk2ODKcXNOiZSsugBSKxVAu9C5G1bVuLbZqTkNj7DpVsLg6m2c0VeTiVJbXZwukCxZXZvJcms6uusVpu2LtCkW3oHbgXU6ns4sV7WOrlkk1K2MrJ4IEDJ3TY2mCpkbI1ClaDuPpEoeHE6tqMNJqsxK1S1dsFZQBVyg2CVs5p6Fxofnol16hP2aQLdrEgjoAQUMjXbRWvWhpZQG0VcMUihsT5UJXKDYJzVzNW7VMypd3jYUMSrYLQMl2SYTMVS9aWpGM3aphCsWNiTLgCsUm4UbKafAXK9vjQUqWS6pgUbQctieCq160tLIAUqWXiq2EcqErFJuIGyWn4fBwkocOD/L4iSvMF8pommB3X4S9A7FVx6RbkYzdymEKxY2HMuAKhWLDGRlL8dTIFEeGE9y7r69aArnWhLLlFkCq9FKxlVAudIVCseF0KhZ9I4UpFFsftQNXKJqgSo3Wl06WzN0oYQrF1kcZ8C5HGZr2o0qN1h8Vi1Yo1o5yoXcxSnlqfVClRuvPjVQyp1CsF2oH3sW0qjylWBlbVRFtM3lrWskYVygUS6MMeBezVQ1Np9mK7t3NGBZQsWiFYm0oA97FbEVDsxnYiqVGW8Fb0+hBOIzT6SEpFB1FxcC7GBVHXB+2YqlRtyuQNcv3mM6WVb6HYtMwMpZiMlPi0S+9wmNPnt2Qa1PtwLsYFUdcP7aae7fbvTXNPAh6XnSVB0GxdfEXmPeE5IaGqJQB73K2mqFRrA/dHhZolu+hCdE1HgTF1sZfYOpCVCtX/MfX8/7cERe6EOJ3hRCvCiFeFkJ8QwixoxPjUChuFLo9LNCs05grZdd4EBRbm06FqDq1A/89KeW/BBBC/Crwr4Bf6dBYFIobgm721jTzIAy6UuV7KDYFfogKcf2xjQhRdWQHLqVM1/waBWQnxqFQKLqDZh6EgVigaxckiq2Fn1DsuHJDE4qFlJ2xnUKIjwO/CKSAB6WUU4s878PAhwGGhobu/MIXvrBxg1wB2WyWWCzW6WGsC2pu3clWnhts7fmpuXUfRcuhkM+Rc00ChkYiZBAy9TUf98EHH3xBSnlXs7+tmwEXQjwFbG/yp49JKf+85nm/CYSklL+13DHvuusu+fzzz7dxlO3j+PHjPPDAA50exrqg5tadbOW5wdaen5pbd7IecxNCLGrA1y0GLqV8qMWn/jfgq8CyBlyhUCgUCoVHp7LQ31Dz63uA1zoxDoVCoVAoupVOZaF/QghxM+ACl1EZ6AqFQqFQrIiOGHAp5d/qxPsqFAqFQrFVUFroCoVCoVB0IcqAKxQKhULRhSgDrlAoFApFF6IMuEKhUCgUXYgy4AqFQqFQdCGqnahCoeg6RsZSTGZKPPqlV9jZE+aRo0NKF11xw6F24AqFoqsYGUvxmWcu4riS4WSIVMHiM89cZGQs1emhKRQbijLgCoWiq3ji5ATJsImuCTQhSIZNkmGTJ05OdHpoCsWGogy4QqHoKq7NF4iH6qN/8ZDBtflCh0akUHQGZcAVCkVXsbMnTKZo1z2WKdrs7Al3aEQKRWdQBlyhUHQVjxwdIlWwcFyJKyWpgkWqYPHI0aFOD02h2FCUAVcoFF3F4eEkH75/H7omGEsVSYZNPnz/PpWFrrjhUGVkCoWi6zg8nGQiHuRTP317p4eiUHQMtQNXKBQKhaILUQZcoVAoFIouRBlwhUKhUCi6EGXAFQqFQqHoQpQBVygUCoWiC1EGXKFQKBSKLkQZcIVCoVAouhBlwBUKhUKh6EKUAVcoFAqFogtRBlyhUCgUii5ESCk7PYaWEUJMAZc7PY5FGACmOz2IdULNrTvZynODrT0/NbfuZD3mtkdKOdjsD11lwDczQojnpZR3dXoc64GaW3eylecGW3t+am7dyUbPTbnQFQqFQqHoQpQBVygUCoWiC1EGvH18ptMDWEfU3LqTrTw32NrzU3PrTjZ0bioGrlAoFApFF6J24AqFQqFQdCHKgLcJIcTvCiFeFUK8LIT4hhBiR6fH1E6EEL8nhHitMsevCCF6Oj2mdiGEeJ8Q4pQQwhVCbInsWCHEI0KIM0KIc0KIf97p8bQLIcR/EUJMCiFOdnos7UYIsUsI8S0hxEjlevy1To+pnQghQkKIHwohXqnM73c6PaZ2IoTQhRAvCSH+cqPeUxnw9vF7UsrbpJRvBP4S+FcdHk+7eRI4KqW8DTgL/GaHx9NOTgLvBZ7p9EDagRBCB/4f4K8BR4C/I4Q40tlRtY0/Bh7p9CDWCRv4dSnlYeDNwD/cQp8bQAl4u5TyduCNwCNCiDd3dkht5deAkY18Q2XA24SUMl3zaxTYUskFUspvSCntyq/PAjd1cjztREo5IqU80+lxtJF7gHNSygtSyjLwBeBvdHhMbUFK+Qww2+lxrAdSyjEp5YuV/2fwjMHOzo6qfUiPbOVXs/KzJe6TQoibgJ8C/vNGvq8y4G1ECPFxIcRV4OfZejvwWv534GudHoRiUXYCV2t+f50tZAhuBIQQe4E7gB90eChtpeJmfhmYBJ6UUm6V+f1b4DcAdyPfVBnwFSCEeEoIcbLJz98AkFJ+TEq5C/gT4B91drQrZ7n5VZ7zMTxX3590bqQrp5W5bSFEk8e2xE7nRkAIEQP+B/CPGzx7XY+U0qmEGW8C7hFCHO3wkNaMEOLdwKSU8oWNfm9jo9+wm5FSPtTiU/8b8FXgt9ZxOG1nufkJIT4IvBt4h+yy+sMVfHZbgdeBXTW/3wSMdmgsihUghDDxjPefSCm/3OnxrBdSynkhxHG8fIZuT0h8K/AeIcRfB0JAQgjxeSnlL6z3G6sdeJsQQryh5tf3AK91aizrgRDiEeCfAe+RUuY7PR7FkjwHvEEIsU8IEQB+DviLDo9JsQxCCAF8FhiRUv5Bp8fTboQQg371ihAiDDzEFrhPSil/U0p5k5RyL9537ZsbYbxBGfB28omKS/ZV4J14GYlbiT8E4sCTlVK5P+r0gNqFEOJnhBCvA8eArwohvt7pMa2FSrLhPwK+jpcI9UUp5anOjqo9CCH+FDgB3CyEeF0I8aFOj6mNvBX4APD2ynfs5cqubqswDHyrco98Di8GvmElV1sRpcSmUCgUCkUXonbgCoVCoVB0IcqAKxQKhULRhSgDrlAoFApFF6IMuEKhUCgUXYgy4AqFQqFQdCHKgCsUa0AIsV0I8QUhxHkhxGkhxF8JIQ51elxrQQjxgBDiLat4TarSjWlECNFUxEgIcZcQ4t+vcly/IoT4xVW+9oFmXaKEEP2VDmBZIcQfrubYCkWnUEpsCsUqqQhvfAV4XEr5c5XH3ggM4XVs61YeALLA91f4uu9IKd8thIgCLwsh/rJWXlIIYUgpnweeX82gpJTroT1QBP4lcLTyo1B0DWoHrlCsngcBq9awSClfllJ+R3j8XkXc50dCiPdDdSf4bSHEF4UQZ4UQnxBC/HylT/KPhBAHKs/7YyHEHwkhvlN53rsrj4eEEP9f5bkvCSEerDz+S0KILwshnhBC/FgI8Ul/TEKIdwohTgghXhRCfKmitY0Q4pIQ4ncqj/9ICHGL8Jpo/Arw0YqQyNsqClr/QwjxXOXnrUudFCllDngBOCCE+G0hxGeEEN8A/mvtTrjyt/8ihDguhLgghPjVmjH/ovB6z78ihPhczfMfrfz/uBDi3wohvl85x/dUHr+n8thLlX9vXm6sUsrv4hlyhaKrUDtwhWL1HMUzVM14L17P49uBAeA5IYTfb/x24DBeW8wLwH+WUt4jhPg14CPAP648by/wk8ABPAWrg8A/BJBS/oQQ4hbgGzUu+zfidbAqAWeEEJ8GCsD/CTwkpcwJIf4Z8E+A/6vymmkp5ZuEEP8H8KiU8peFp7KXlVJ+CkAI8d+Ax6SU3xVC7MZTeDu82EkRQvTj9bP+Xbx+5HcC90kpC0KIBxqefgveQiheGfN/BA4BHwPeKqWcFkL0LfJWUSnlW4QQ9wP/Be/zeA24X0ppCyEeAv418LcWG6tC0c0oA65QrA/3AX8qpXSACSHEt4G7gTTwnJRyDEAIcR74RuU1P8IzZj5flFK6wI+FEBfwjN19wKcBpJSvCSEu4xk8gKellKnKcU8De4AePCP6Pc/jTwBPitTHb5jxAt6ioxkPAUcqrwevWUO80rO6lrcJIV7Ca6n4CSnlKSHE+4C/kFIWFjn2V6WUJaAkhJjECz+8HfjvUsrpyjwX6//9p5W/PyOESAhPZzsOPC683gQSr+e0QrElUQZcoVg9p4C/vcjfmrX09CnV/N+t+d2l/jvZqHMsV3Bcp3Isgac5/XeWeY3//GZowLEljLDPd6SU727yeG6J1yw25lY0npudn98FviWl/JlKOOB4C8dRKLoSFQNXKFbPN4GgEOLv+Q8IIe4WQvwk8AzwfiGELoQYBO4HfrjC479PCKFV4uL7gTOV4/585b0OAbsrjy/Gs8BbK+53hBARsXyWfAZvJ+vzDWr62wsvUW89eRr42YorniVc6H5ewX1AquJ9SALXKn//pXUep0LRUZQBVyhWSaUn+s8ADwuvjOwU8Nt4vbe/ArwKvIJn6H9DSjm+wrc4A3wb+BrwK1LKIvAfAF0I8SPgz4BfqrigFxvjFJ4h+1PhdYF6Fs8VvxT/C/gZP4kN+FXgrkpS2Wm8JLd1o9I57ePAt4UQrwCLtdacE0J8H/gjwO9K9kng/xZCfA/QW3k/IcSlynv8kvA6nB1Zy/gVio1CdSNTKDYhQog/Bv5SSvnfOz2WzYgQ4jhe0t2qStIUiq2A2oErFAqFQtGFqB24QqFQKBRdiNqBKxQKhULRhSgDrlAoFApFF6IMuEKhUCgUXYgy4AqFQqFQdCHKgCsUCoVC0YUoA65QKBQKRRfy/wMiRVx6YjXjygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Passo 2: Gere um conjunto de dados de exemplo com vetores de alta dimensão (d = 10)\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "d = 10\n",
    "X = np.random.randn(n_samples, d)  # Gera 1000 vetores de dimensão 10 com distribuição normal\n",
    "\n",
    "# Passo 3: Aplique o PCA para reduzir a dimensionalidade dos vetores para 2 dimensões\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "# Passo 4: Plote os vetores reduzidos em um gráfico de dispersão para visualização\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_2d[:, 0], X_2d[:, 1], alpha=0.5)\n",
    "plt.title('Visualização dos Vetores Reduzidos com PCA em 2 Dimensões')\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8795cdad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T14:14:30.442192Z",
     "start_time": "2024-04-13T14:14:30.436599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49671415, -0.1382643 ,  0.64768854, ...,  0.76743473,\n",
       "        -0.46947439,  0.54256004],\n",
       "       [-0.46341769, -0.46572975,  0.24196227, ...,  0.31424733,\n",
       "        -0.90802408, -1.4123037 ],\n",
       "       [ 1.46564877, -0.2257763 ,  0.0675282 , ...,  0.37569802,\n",
       "        -0.60063869, -0.29169375],\n",
       "       ...,\n",
       "       [-0.9125882 ,  0.70138989,  0.8452733 , ..., -0.90092112,\n",
       "        -1.01268556, -1.75995888],\n",
       "       [-0.44579531, -0.50372234,  0.52593728, ..., -1.77598225,\n",
       "        -0.98094673, -0.77081363],\n",
       "       [ 1.43362502,  0.19145072,  0.66216875, ..., -0.70531672,\n",
       "         0.49576557,  0.64438845]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b67c8408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T14:14:37.856033Z",
     "start_time": "2024-04-13T14:14:37.850268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70732576,  0.4316153 ],\n",
       "       [-0.32794935, -0.78891239],\n",
       "       [ 0.24338461, -0.62239709],\n",
       "       ...,\n",
       "       [-1.77556364,  0.31805985],\n",
       "       [-1.10572299, -1.44331578],\n",
       "       [ 1.38589389, -1.07308135]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30a96db",
   "metadata": {},
   "source": [
    "PCA é comumente usada para reduzir a dimensão dos seus dados. Intuitivamente, o modelo colapsa os dados através dos componentes principais. Você pode pensar no primeiro componente principal (em um conjunto de dados 2D) como a linha onde há a maior quantidade de variância. Você pode então colapsar os pontos de dados nessa linha. Portanto, você passou de 2D para 1D. Você pode generalizar essa intuição para várias dimensões.\n",
    "\n",
    "<img src=\"./imgs/pca3.png\">\n",
    "\n",
    "**Eigenvectors**: os vetores resultantes, também conhecidos como características não correlacionadas dos seus dados.\n",
    "\n",
    "**Eigenvalues**: a quantidade de informação retida por cada nova característica. Você pode pensar nisso como a variância no vetor próprio.\n",
    "\n",
    "Além disso, cada autovalor tem um vetor próprio correspondente. O autovalor informa quanto variância há no vetor próprio. Aqui estão os passos necessários para calcular a PCA:\n",
    "\n",
    "Passos para Calcular a PCA:\n",
    "\n",
    "<img src=\"./imgs/pca4.png\">\n",
    "\n",
    "1. Normalizar a média dos seus dados.\n",
    "2. Calcular a matriz de covariância.\n",
    "3. Calcular SVD na sua matriz de covariância. Isso retorna $[USV] = \\text{svd}(\\Sigma)$. As três matrizes U, S, V são desenhadas acima. U é rotulado com os vetores próprios, e S é rotulado com os autovalores.\n",
    "\n",
    "Você pode então usar as primeiras n colunas do vetor $U$, para obter seus novos dados multiplicando $XU[:,0:n]$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f4b90",
   "metadata": {},
   "source": [
    "# Week 4 - Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44766d5f",
   "metadata": {},
   "source": [
    "## Objetivos de aprendizagem\n",
    "- Gradient descent\n",
    "- Approximate nearest neighbors\n",
    "- Locality sensitive hashing\n",
    "- Hash functions\n",
    "- Hash tables\n",
    "- K nearest neighbors\n",
    "- Document search\n",
    "- Machine translation\n",
    "- Frobenius norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d50b9b",
   "metadata": {},
   "source": [
    "## Transforming word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e89c124",
   "metadata": {},
   "source": [
    "Para transformar vetores de palavras de uma língua para outra na task de machine translation, precisamos obter uma matriz $R$, que para identificar os valores similares do idioma que queremos traduzir.\n",
    "\n",
    "No exemplo abaixo, a palavra \"chat\" representa \"cat\" em francês, e podemos identificar o seu vetor após multiplicar a matriz do subconjunto de palavras em inglês pela matriz $R$ e identificar a matriz mais próxima utilizando similaridade de cosseno entre o resultado e todos os vetores em francês.\n",
    "\n",
    "Observe que $X$ corresponde à matriz de vetores de palavras em inglês e $Y$ corresponde à matriz de vetores de palavras em francês. $R$ é a matriz de mapeamento.\n",
    "\n",
    "Passos necessários para aprender $R$:\n",
    "\n",
    "1. Inicialize $R$.\n",
    "2. Para cada iteração:\n",
    "\n",
    "    a. Calcule a perda $Loss = \\lVert XR - Y \\rVert_F$.\n",
    "    \n",
    "    b. Calcule o gradiente $g = \\frac{dLoss}{dR}$.\n",
    "    \n",
    "    c. Atualize $R$: $R = R - \\alpha * g$.\n",
    "\n",
    "Aqui está um exemplo para mostrar como a norma de Frobenius funciona:\n",
    "\n",
    "$\\lVert A \\rVert_F = \\sqrt{(\\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2)}$\n",
    "\n",
    "Em resumo, você está fazendo uso do seguinte:\n",
    "\n",
    "- $XR \\approx Y$\n",
    "- Minimizar $\\lVert XR - Y \\rVert_F^2$\n",
    "\n",
    "<img src=\"./imgs/transforming_vectors1.png\">\n",
    "\n",
    "<img src=\"./imgs/transforming_vectors2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d4009c",
   "metadata": {},
   "source": [
    "## K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5cae7",
   "metadata": {},
   "source": [
    "Após computar o output de $XR$, teremos um novo vetor transformado. Precisamos encontrar quais os vetores do idioma francês estão mais próximos do novo vetor transformado. E podemos fazer isso utilizando K-nearest neighbors\n",
    "\n",
    "<img src=\"./imgs/knn.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464daa9d",
   "metadata": {},
   "source": [
    "O K-nearest neighbors (K-NN) é um algoritmo de aprendizado supervisionado utilizado para classificação e regressão. No contexto da identificação de vetores mais próximos, o K-NN é empregado como um algoritmo de **recuperação de informação (information retrieval)**, onde os \"vizinhos mais próximos\" referem-se aos vetores mais semelhantes aos vetores de consulta com base em alguma medida de distância. \n",
    "\n",
    "**Funcionamento do K-NN:**\n",
    "1. **Treinamento**:\n",
    "   - No treinamento do K-NN, os vetores de treinamento são armazenados sem nenhum processamento adicional. Cada vetor de treinamento é associado a uma classe ou rótulo no caso da classificação, ou a um valor no caso da regressão.\n",
    "\n",
    "2. **Cálculo da Distância**:\n",
    "   - Durante a fase de teste ou consulta, o K-NN calcula a distância entre o vetor de consulta e todos os vetores de treinamento. As distâncias mais comuns usadas incluem a distância euclidiana, a distância de Manhattan, a distância de Minkowski, entre outras.\n",
    "\n",
    "3. **Seleção dos K-vizinhos mais próximos**:\n",
    "   - Após o cálculo das distâncias, o K-NN seleciona os K vetores de treinamento mais próximos do vetor de consulta com base na medida de distância escolhida.\n",
    "\n",
    "4. **Classificação ou Regressão**:\n",
    "   - Para a classificação, o K-NN atribui a classe mais frequente entre os K vizinhos mais próximos ao vetor de consulta. No caso de regressão, o K-NN pode calcular a média ou a mediana dos valores associados aos K vizinhos mais próximos e atribuí-lo ao vetor de consulta.\n",
    "\n",
    "**Utilização para Identificação de Vetores Mais Próximos:**\n",
    "- **Recuperação de Informação**:\n",
    "  - No contexto de identificar vetores mais próximos, o K-NN pode ser utilizado como um algoritmo de recuperação de informação. Dados um vetor de consulta e uma coleção de vetores, o K-NN pode encontrar os vetores mais semelhantes ao vetor de consulta com base na distância escolhida.\n",
    "\n",
    "- **Sistemas de Recomendação**:\n",
    "  - Em sistemas de recomendação, o K-NN pode ser usado para encontrar os itens mais semelhantes a um item de interesse com base em avaliações de usuários ou características dos itens.\n",
    "\n",
    "- **Agrupamento de Dados**:\n",
    "  - No agrupamento de dados, o K-NN pode ser utilizado para encontrar grupos de vetores que são próximos uns dos outros em termos de suas características.\n",
    "\n",
    "**Exemplo:**\n",
    "Suponha que temos um conjunto de dados de vetores de features que representam produtos em um site de compras online. Cada vetor contém informações como preço, categoria, avaliação dos usuários, etc. Para identificar os produtos mais similares a um produto de consulta, podemos usar o K-NN da seguinte forma:\n",
    "\n",
    "- Calculamos a distância entre o vetor de consulta (representando o produto de interesse) e todos os vetores de produtos no conjunto de dados.\n",
    "- Selecionamos os K produtos mais próximos com base na distância.\n",
    "- Podemos então recomendar esses produtos ao usuário como itens similares ao produto de consulta.\n",
    "\n",
    "O K-NN é uma técnica simples e poderosa que pode ser utilizada em diversas aplicações para identificar vetores mais próximos com base em alguma medida de distância, facilitando a recuperação de informação e a análise de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab51c50",
   "metadata": {},
   "source": [
    "## Hash tables and hash functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c6d21",
   "metadata": {},
   "source": [
    "Hash tables e hash functions são conceitos fundamentais em ciência da computação, utilizados para armazenar e acessar dados de forma eficiente.\n",
    "\n",
    "**Hash Functions:**\n",
    "\n",
    "Uma função de hash é uma função que mapeia dados de tamanho arbitrário para valores de tamanho fixo, geralmente uma sequência de números ou letras. O objetivo principal de uma função de hash é espalhar os dados de forma uniforme ao longo de um espaço de hash, minimizando colisões, ou seja, o mapeamento de diferentes dados para o mesmo valor de hash. Boas funções de hash têm as seguintes propriedades:\n",
    "\n",
    "1. **Determinismo**: Para o mesmo dado de entrada, a função de hash sempre produz o mesmo valor de hash.\n",
    "2. **Eficiência**: A função de hash deve ser rápida de calcular.\n",
    "3. **Espalhamento Uniforme**: A função de hash deve distribuir os dados uniformemente pelo espaço de hash, minimizando colisões.\n",
    "\n",
    "**Exemplo de Função de Hash:**\n",
    "\n",
    "```python\n",
    "def hash_function(data, size):\n",
    "    # Exemplo simples de função de hash: soma dos códigos ASCII dos caracteres\n",
    "    hash_value = sum(ord(char) for char in data) % size\n",
    "    return hash_value\n",
    "```\n",
    "\n",
    "Neste exemplo, a função de hash calcula a soma dos códigos ASCII dos caracteres de uma string e, em seguida, aplica o módulo do tamanho da tabela hash para obter um valor de hash dentro do intervalo desejado.\n",
    "\n",
    "**Hash Tables:**\n",
    "\n",
    "Uma tabela hash (ou hash table) é uma estrutura de dados que utiliza funções de hash para armazenar e recuperar dados de forma eficiente. Consiste em um array (ou vetor) de \"buckets\" (ou \"slots\"), onde cada bucket é indexado por um valor de hash único. Os dados são armazenados na tabela hash usando seus valores de hash como chaves de acesso.\n",
    "\n",
    "**Exemplo de Hash Table:**\n",
    "\n",
    "```python\n",
    "class HashTable:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.table = [None] * size\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        index = hash_function(key, self.size)\n",
    "        self.table[index] = value\n",
    "    \n",
    "    def get(self, key):\n",
    "        index = hash_function(key, self.size)\n",
    "        return self.table[index]\n",
    "\n",
    "# Exemplo de uso\n",
    "hash_table = HashTable(10)\n",
    "hash_table.put(\"apple\", 5)\n",
    "hash_table.put(\"banana\", 8)\n",
    "\n",
    "print(hash_table.get(\"apple\"))  # Saída: 5\n",
    "print(hash_table.get(\"banana\"))  # Saída: 8\n",
    "```\n",
    "\n",
    "Neste exemplo, implementamos uma tabela hash simples em Python usando uma lista como o array de buckets. A função `put` insere um par chave-valor na tabela hash, calculando o índice usando a função de hash. A função `get` recupera o valor associado a uma chave específica, novamente usando a função de hash para encontrar o índice correto na tabela.\n",
    "\n",
    "As tabelas hash são amplamente utilizadas em diversas aplicações, incluindo bancos de dados, caches, tabelas de espalhamento e muito mais, devido à sua eficiência na busca e inserção de dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b51187",
   "metadata": {},
   "source": [
    "No contexto de tradução automática (machine translation), as hash tables e hash functions podem ser utilizadas para diversos fins, como armazenamento de vocabulário, mapeamento de palavras ou frases para índices, gerenciamento de alinhamentos de palavras, entre outros. \n",
    "\n",
    "**Armazenamento de Vocabulário:**\n",
    "\n",
    "**Hash Table**: Uma hash table pode ser usada para armazenar o vocabulário de um idioma. Cada palavra é mapeada para um índice único na hash table, que pode ser usado para referenciar a palavra em outras estruturas de dados.\n",
    "\n",
    "**Hash Function**: A função de hash pode ser usada para mapear as palavras do vocabulário para índices na hash table. Idealmente, a função de hash distribuirá as palavras de forma uniforme na hash table.\n",
    "\n",
    "**Exemplo**:\n",
    "```python\n",
    "# Criação de uma hash table para armazenar o vocabulário\n",
    "vocab_hash = {}\n",
    "\n",
    "# Adiciona palavras ao vocabulário com um índice único usando uma função de hash\n",
    "def add_word_to_vocab(word):\n",
    "    index = hash(word)  # Exemplo simples de função de hash\n",
    "    vocab_hash[index] = word\n",
    "\n",
    "# Adiciona algumas palavras ao vocabulário\n",
    "add_word_to_vocab(\"hello\")\n",
    "add_word_to_vocab(\"world\")\n",
    "\n",
    "print(vocab_hash)\n",
    "# Saída: {126423: 'hello', 168706908251269437: 'world'}\n",
    "```\n",
    "\n",
    "**Mapeamento de Palavras ou Frases para Índices:**\n",
    "\n",
    "**Hash Table**: Uma hash table pode ser usada para mapear palavras ou frases para índices em um modelo de tradução automática. Isso pode ser útil para armazenar embeddings de palavras, probabilidades de tradução, ou outros dados relacionados à tradução.\n",
    "\n",
    "**Hash Function**: A função de hash pode ser usada para mapear palavras ou frases para índices únicos na hash table.\n",
    "\n",
    "**Exemplo**:\n",
    "```python\n",
    "# Criação de uma hash table para mapear palavras para índices\n",
    "word_index_hash = {}\n",
    "\n",
    "# Adiciona palavras ao mapeamento usando uma função de hash\n",
    "def map_word_to_index(word):\n",
    "    index = hash(word)  # Exemplo simples de função de hash\n",
    "    word_index_hash[word] = index\n",
    "\n",
    "# Mapeia algumas palavras para índices\n",
    "map_word_to_index(\"hello\")\n",
    "map_word_to_index(\"world\")\n",
    "\n",
    "print(word_index_hash)\n",
    "# Saída: {'hello': 126423, 'world': 168706908251269437}\n",
    "```\n",
    "\n",
    "**Gerenciamento de Alinhamentos de Palavras:**\n",
    "\n",
    "**Hash Table**: Uma hash table pode ser usada para armazenar alinhamentos de palavras entre dois idiomas em um modelo de tradução automática. Cada palavra do idioma de origem é mapeada para uma lista de palavras alinhadas no idioma de destino.\n",
    "\n",
    "**Hash Function**: A função de hash pode ser usada para mapear palavras de um idioma para listas de palavras alinhadas em outro idioma.\n",
    "\n",
    "**Exemplo**:\n",
    "```python\n",
    "# Criação de uma hash table para armazenar alinhamentos de palavras\n",
    "alignment_hash = {}\n",
    "\n",
    "# Adiciona alinhamentos de palavras usando uma função de hash\n",
    "def add_word_alignment(source_word, target_word):\n",
    "    source_index = hash(source_word)  # Exemplo simples de função de hash\n",
    "    if source_index not in alignment_hash:\n",
    "        alignment_hash[source_index] = [target_word]\n",
    "    else:\n",
    "        alignment_hash[source_index].append(target_word)\n",
    "\n",
    "# Adiciona alguns alinhamentos de palavras\n",
    "add_word_alignment(\"hello\", \"bonjour\")\n",
    "add_word_alignment(\"hello\", \"salut\")\n",
    "\n",
    "print(alignment_hash)\n",
    "# Saída: {126423: ['bonjour', 'salut']}\n",
    "```\n",
    "\n",
    "Esses são apenas alguns exemplos de como hash tables e hash functions podem ser utilizadas no contexto de tradução automática. Elas podem ser aplicadas de muitas outras maneiras, dependendo das necessidades específicas do sistema de tradução automática."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1ec7f9",
   "metadata": {},
   "source": [
    "## Locality sensitive hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dd269d",
   "metadata": {},
   "source": [
    "Local Sensitive Hashing (LSH) é uma técnica de hashing probabilística utilizada para encontrar itens semelhantes em grandes conjuntos de dados, especialmente em dados de alta dimensionalidade, como vetores de características. Ao contrário de hash functions tradicionais, que tentam minimizar colisões para preservar a integridade dos dados, LSH tenta maximizar colisões entre itens semelhantes, a fim de agrupá-los juntos.\n",
    "\n",
    "**Funcionamento do LSH:**\n",
    "\n",
    "1. **Divisão do Espaço de Hashing**:\n",
    "   - O espaço de hash é dividido em várias regiões ou \"buckets\". Idealmente, itens semelhantes devem ser mapeados para os mesmos buckets com alta probabilidade.\n",
    "\n",
    "2. **Funções de Hashing Locais**:\n",
    "   - São utilizadas funções de hash específicas que são sensíveis à localidade, ou seja, que maximizam a probabilidade de colisão entre itens semelhantes. Essas funções podem ser projetadas de várias maneiras, como funções de projeção aleatórias ou funções sensíveis à distância.\n",
    "\n",
    "3. **Agrupamento de Itens Semelhantes**:\n",
    "   - Os itens do conjunto de dados são então mapeados para os buckets usando as funções de hash locais. Itens mapeados para os mesmos buckets são considerados candidatos a serem semelhantes.\n",
    "\n",
    "4. **Comparação de Candidatos**:\n",
    "   - Para cada item, apenas uma pequena fração dos itens no mesmo bucket é comparada para determinar os verdadeiros vizinhos mais próximos. Isso ajuda a reduzir o tempo de busca em grandes conjuntos de dados.\n",
    "\n",
    "**Exemplo de Local Sensitive Hashing:**\n",
    "\n",
    "Suponha que temos um conjunto de vetores de características que representam imagens em um banco de dados. Queremos encontrar imagens semelhantes a uma imagem de consulta específica usando LSH.\n",
    "\n",
    "- **Divisão do Espaço de Hashing**: Dividimos o espaço de hash em regiões ou buckets.\n",
    "- **Funções de Hashing Locais**: Definimos funções de hash que maximizam a probabilidade de colisão entre vetores de características semelhantes.\n",
    "- **Agrupamento de Itens Semelhantes**: Mapeamos os vetores de características das imagens para os buckets usando as funções de hash locais.\n",
    "- **Comparação de Candidatos**: Para cada imagem de consulta, comparamos apenas as imagens nos mesmos buckets para determinar os verdadeiros vizinhos mais próximos.\n",
    "\n",
    "**Uso do LSH:**\n",
    "\n",
    "- **Recuperação de Informação**: LSH é usado em sistemas de recomendação, pesquisa de similaridade em grandes conjuntos de dados, detecção de duplicatas e muito mais.\n",
    "- **Mineração de Dados**: É utilizado em mineração de texto, mineração de imagens, mineração de grafos e outras aplicações de mineração de dados.\n",
    "- **Análise de Big Data**: LSH é particularmente útil em análises de big data, onde a eficiência de busca é crucial devido ao grande volume de dados.\n",
    "\n",
    "Em resumo, Local Sensitive Hashing é uma técnica eficaz para encontrar itens semelhantes em grandes conjuntos de dados, especialmente em dados de alta dimensionalidade, como vetores de características, ajudando a reduzir o tempo de busca e os requisitos computacionais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e582ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T20:25:57.502721Z",
     "start_time": "2024-04-13T20:25:57.485161Z"
    }
   },
   "source": [
    "Local Sensitive Hashing (LSH) pode ser aplicado no contexto de tradução automática de várias maneiras para facilitar a busca eficiente de traduções ou alinhamentos de frases semelhantes em grandes conjuntos de dados.\n",
    "\n",
    "**1. Busca Eficiente de Traduções Alternativas:**\n",
    "\n",
    "- **Aplicação**: Em tradução automática, é comum ter várias traduções possíveis para uma determinada frase de origem. LSH pode ser usado para indexar e recuperar traduções alternativas de forma eficiente.\n",
    "\n",
    "- **Implementação**: As frases no idioma de origem e suas traduções correspondentes podem ser mapeadas para um espaço vetorial. Em seguida, LSH pode ser aplicado para agrupar traduções semelhantes em buckets. Isso permite uma busca rápida por traduções alternativas semelhantes à frase de origem.\n",
    "\n",
    "**2. Alinhamento de Frases Semelhantes:**\n",
    "\n",
    "- **Aplicação**: Em alinhamento de frases, é necessário encontrar frases semelhantes em dois idiomas diferentes. LSH pode ser usado para encontrar alinhamentos aproximados entre frases semelhantes.\n",
    "\n",
    "- **Implementação**: As frases nos idiomas de origem e destino podem ser mapeadas para espaços vetoriais. Em seguida, LSH pode ser aplicado para agrupar frases semelhantes em cada idioma em buckets. Isso facilita a busca por frases aproximadamente alinhadas entre os dois idiomas.\n",
    "\n",
    "**3. Identificação de Frases Parcialmente Correspondentes:**\n",
    "\n",
    "- **Aplicação**: Em tradução automática, pode ser útil identificar partes de frases que correspondem parcialmente a outras frases em outro idioma. LSH pode ser usado para identificar frases parcialmente correspondentes de forma eficiente.\n",
    "\n",
    "- **Implementação**: As partes das frases podem ser mapeadas para espaços vetoriais. Em seguida, LSH pode ser aplicado para agrupar partes de frases semelhantes em buckets. Isso permite a identificação eficiente de partes de frases que correspondem parcialmente a outras frases em outro idioma.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Suponha que temos um conjunto de frases em inglês e suas traduções em francês. Queremos encontrar traduções alternativas para uma frase de origem específica usando LSH.\n",
    "\n",
    "- **Passo 1**: Mapear frases para vetores de características.\n",
    "- **Passo 2**: Aplicar LSH para agrupar frases semelhantes em buckets.\n",
    "- **Passo 3**: Para uma frase de origem, recuperar traduções alternativas semelhantes das frases no mesmo bucket.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Exemplo de frases em inglês\n",
    "english_sentences = [\"hello\", \"goodbye\", \"thank you\", \"how are you\", \"what is your name\"]\n",
    "\n",
    "# Exemplo de traduções em francês\n",
    "french_translations = [\"bonjour\", \"au revoir\", \"merci\", \"comment vas-tu\", \"quel est ton nom\"]\n",
    "\n",
    "# Mapear frases para vetores de características usando TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(english_sentences)\n",
    "\n",
    "# Aplicar LSH para agrupar frases semelhantes\n",
    "lshf = NearestNeighbors(n_neighbors=10)\n",
    "lshf.fit(X)\n",
    "\n",
    "# Recuperar traduções alternativas para uma frase de origem específica\n",
    "query = vectorizer.transform([\"hello\"])\n",
    "neighbors = lshf.kneighbors(query, n_neighbors=3)\n",
    "\n",
    "# Exibir traduções alternativas\n",
    "for neighbor_index in neighbors[1][0]:\n",
    "    print(french_translations[neighbor_index])\n",
    "```\n",
    "\n",
    "Este é um exemplo simples de como usar LSH no contexto de tradução automática para encontrar traduções alternativas para uma frase de origem específica. O LSH é aplicado para agrupar frases semelhantes em buckets, o que facilita a busca eficiente por traduções alternativas semelhantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604861d2",
   "metadata": {},
   "source": [
    "## Multiple Planes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9443637b",
   "metadata": {},
   "source": [
    "No contexto de Local Sensitive Hashing (LSH), a técnica de múltiplos planos é uma abordagem utilizada para aumentar a sensibilidade local do hashing, permitindo uma melhor discriminação entre itens semelhantes e não semelhantes. Essa técnica é comumente aplicada em tarefas de busca aproximada, onde o objetivo é agrupar itens semelhantes em buckets para facilitar a recuperação eficiente de vizinhos próximos.\n",
    "\n",
    "**Funcionamento dos Múltiplos Planos:**\n",
    "\n",
    "1. **Definição dos Planos**:\n",
    "   - Os múltiplos planos são definidos como hiperplanos em um espaço de hash de alta dimensão. Cada plano é representado por um vetor de pesos que define a orientação e a localização do plano no espaço.\n",
    "\n",
    "2. **Projeção dos Itens**:\n",
    "   - Os itens a serem hashados são projetados nos múltiplos planos. Isso é feito calculando o produto interno entre o vetor de características do item e o vetor de pesos de cada plano.\n",
    "\n",
    "3. **Geração dos Hash Values**:\n",
    "   - Com base nas projeções, são gerados hash values para cada item em relação a cada plano. Dependendo do sinal do resultado da projeção, um bit é atribuído como 0 ou 1.\n",
    "\n",
    "4. **Combinação dos Hash Values**:\n",
    "   - Os hash values de cada plano são combinados para formar um hash final para cada item. Isso pode ser feito concatenando os hash values ou usando operações de bits, como XOR ou OR.\n",
    "\n",
    "5. **Armazenamento nos Buckets**:\n",
    "   - Com base no hash final, os itens são atribuídos a buckets específicos na hash table.\n",
    "\n",
    "**Exemplo de Múltiplos Planos em LSH:**\n",
    "\n",
    "Considere um problema de busca aproximada em que queremos agrupar documentos semelhantes em buckets usando LSH com múltiplos planos.\n",
    "\n",
    "- **Definição dos Planos**: Os múltiplos planos são representados por vetores de pesos aleatórios.\n",
    "- **Projeção dos Documentos**: Os vetores de características dos documentos são projetados nos múltiplos planos.\n",
    "- **Geração dos Hash Values**: Com base nas projeções, são gerados hash values atribuindo 0 ou 1 dependendo do sinal das projeções.\n",
    "- **Combinação dos Hash Values**: Os hash values de cada plano são combinados para formar o hash final de cada documento.\n",
    "- **Armazenamento nos Buckets**: Com base no hash final, os documentos são atribuídos a buckets específicos na hash table.\n",
    "\n",
    "**Exemplo de Implementação em Python:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Função para gerar vetores de pesos aleatórios para os planos\n",
    "def generate_random_planes(num_planes, num_features):\n",
    "    return np.random.randn(num_planes, num_features)\n",
    "\n",
    "# Função para projetar um item nos planos e gerar hash values\n",
    "def project_and_hash(item, planes):\n",
    "    projections = np.dot(planes, item)\n",
    "    hash_values = (projections >= 0).astype(int)  # Atribui 1 para projeções positivas e 0 para negativas\n",
    "    return hash_values\n",
    "\n",
    "# Exemplo de uso\n",
    "num_planes = 5\n",
    "num_features = 10\n",
    "planes = generate_random_planes(num_planes, num_features)\n",
    "item = np.random.randn(num_features)  # Exemplo de vetor de características de um item\n",
    "hash_values = project_and_hash(item, planes)\n",
    "print(\"Hash Values:\", hash_values)\n",
    "```\n",
    "\n",
    "Neste exemplo, estamos gerando múltiplos planos aleatórios e projetando um vetor de características de um item nesses planos para gerar hash values. Os hash values são então combinados para formar um hash final para o item. Este hash final pode ser usado para atribuir o item a um bucket específico na hash table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb57cee",
   "metadata": {},
   "source": [
    "## Approximate nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916686d3",
   "metadata": {},
   "source": [
    "Approximate Nearest Neighbors (ANN) é uma técnica usada para encontrar pontos que são aproximadamente os vizinhos mais próximos de um ponto de consulta em um conjunto de dados, em vez de encontrar os vizinhos mais próximos exatos. Isso é útil em cenários onde a exatidão total não é necessária, mas a eficiência computacional é crucial.\n",
    "\n",
    "**Funcionamento do Approximate Nearest Neighbors (ANN):**\n",
    "\n",
    "1. **Indexação do Conjunto de Dados**:\n",
    "   - Antes de encontrar os vizinhos mais próximos, o conjunto de dados é indexado de uma maneira que facilite a busca eficiente. Isso pode ser feito usando estruturas de dados como árvores kd, hashing ou técnicas de redução de dimensionalidade, como PCA ou LSH.\n",
    "\n",
    "2. **Busca Aproximada**:\n",
    "   - Quando um ponto de consulta é recebido, o algoritmo de ANN busca no conjunto de dados para encontrar os pontos que são aproximadamente os vizinhos mais próximos do ponto de consulta. A busca é feita de uma maneira que prioriza a eficiência computacional sobre a precisão absoluta.\n",
    "\n",
    "3. **Retorno dos Resultados**:\n",
    "   - Os pontos que são aproximadamente os vizinhos mais próximos do ponto de consulta são retornados como resultados. Dependendo do método utilizado, a precisão dos resultados pode variar, mas geralmente é suficiente para muitos aplicativos.\n",
    "\n",
    "**Exemplos de Aplicativos:**\n",
    "\n",
    "1. **Sistemas de Recomendação**:\n",
    "   - Em sistemas de recomendação, é comum usar ANN para encontrar itens semelhantes aos itens que um usuário já gostou. Isso permite recomendações rápidas, mesmo em grandes conjuntos de dados.\n",
    "\n",
    "2. **Busca em Grandes Conjuntos de Dados**:\n",
    "   - Em sistemas de busca, ANN pode ser usado para encontrar documentos ou imagens semelhantes a uma consulta do usuário em grandes conjuntos de dados.\n",
    "\n",
    "3. **Processamento de Linguagem Natural (NLP)**:\n",
    "   - Em NLP, ANN pode ser usado para encontrar frases ou documentos semelhantes a uma consulta do usuário, facilitando a recuperação de informações relevantes.\n",
    "\n",
    "**Exemplo de Uso de ANN:**\n",
    "\n",
    "Suponha que temos um conjunto de dados de vetores de características que representam imagens. Queremos encontrar imagens que são aproximadamente as mais semelhantes a uma imagem de consulta usando ANN.\n",
    "\n",
    "- **Passo 1**: Indexar o conjunto de dados de imagens para facilitar a busca eficiente.\n",
    "- **Passo 2**: Quando uma imagem de consulta é recebida, usar ANN para encontrar as imagens que são aproximadamente as mais semelhantes à imagem de consulta.\n",
    "- **Passo 3**: Retornar as imagens encontradas como resultados.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Exemplo de conjunto de dados de imagens\n",
    "X = ...  # Vetores de características das imagens\n",
    "\n",
    "# Indexar o conjunto de dados de imagens\n",
    "ann_model = NearestNeighbors(n_neighbors=5, algorithm='auto')\n",
    "ann_model.fit(X)\n",
    "\n",
    "# Quando uma imagem de consulta é recebida\n",
    "query_image = ...  # Vetor de características da imagem de consulta\n",
    "distances, indices = ann_model.kneighbors([query_image])\n",
    "\n",
    "# Retornar as imagens encontradas como resultados\n",
    "similar_images = X[indices[0]]\n",
    "print(\"Imagens aproximadamente mais semelhantes:\", similar_images)\n",
    "```\n",
    "\n",
    "Neste exemplo, estamos usando a biblioteca scikit-learn para encontrar as imagens que são aproximadamente as mais semelhantes a uma imagem de consulta usando ANN. A busca é rápida e eficiente, mesmo em grandes conjuntos de dados de imagens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c34dc7",
   "metadata": {},
   "source": [
    "No contexto de machine translation (tradução automática), o Approximate Nearest Neighbors (ANN) refere-se à técnica de encontrar traduções aproximadamente mais próximas de uma frase de origem específica em um conjunto de dados de pares de frases. Em vez de buscar exatamente a tradução mais próxima, o objetivo é encontrar traduções que sejam aproximadamente semelhantes à frase de origem de uma maneira eficiente em termos computacionais.\n",
    "\n",
    "**Funcionamento do Approximate Nearest Neighbors (ANN) em Machine Translation:**\n",
    "\n",
    "1. **Indexação do Conjunto de Dados**:\n",
    "   - Antes de encontrar as traduções aproximadamente mais próximas, o conjunto de dados de pares de frases é indexado de uma forma que facilite a busca eficiente. Isso pode ser feito usando técnicas como hashing, LSH (Local Sensitive Hashing), ou outros métodos de indexação.\n",
    "\n",
    "2. **Busca Aproximada**:\n",
    "   - Quando uma frase de origem é recebida, o algoritmo de ANN busca no conjunto de dados para encontrar traduções que sejam aproximadamente as mais próximas da frase de origem. Isso é feito de uma maneira que prioriza a eficiência computacional sobre a precisão absoluta.\n",
    "\n",
    "3. **Retorno dos Resultados**:\n",
    "   - As traduções aproximadamente mais próximas da frase de origem são retornadas como resultados. Embora essas traduções possam não ser exatamente as mais próximas em termos de similaridade linguística, elas são consideradas aceitáveis para o contexto da aplicação.\n",
    "\n",
    "**Exemplo de Aplicação de ANN em Machine Translation:**\n",
    "\n",
    "Suponha que temos um conjunto de dados de pares de frases em inglês e francês. Queremos encontrar traduções aproximadamente mais próximas de uma frase de origem em inglês usando ANN.\n",
    "\n",
    "- **Passo 1**: Indexar o conjunto de dados de pares de frases para facilitar a busca eficiente.\n",
    "- **Passo 2**: Quando uma frase de origem em inglês é recebida, usar ANN para encontrar traduções aproximadamente mais próximas da frase de origem.\n",
    "- **Passo 3**: Retornar as traduções aproximadamente mais próximas encontradas como resultados.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Exemplo de conjunto de dados de pares de frases em inglês e francês\n",
    "english_sentences = [\"hello\", \"goodbye\", \"thank you\", \"how are you\", \"what is your name\"]\n",
    "french_translations = [\"bonjour\", \"au revoir\", \"merci\", \"comment vas-tu\", \"quel est ton nom\"]\n",
    "\n",
    "# Indexar o conjunto de dados de pares de frases\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(english_sentences)\n",
    "\n",
    "# Usar ANN para encontrar traduções aproximadamente mais próximas\n",
    "ann_model = NearestNeighbors(n_neighbors=1, algorithm='auto')\n",
    "ann_model.fit(X)\n",
    "\n",
    "# Quando uma frase de origem em inglês é recebida\n",
    "query_sentence = \"hello\"\n",
    "query_vector = vectorizer.transform([query_sentence])\n",
    "\n",
    "# Encontrar a tradução aproximadamente mais próxima da frase de origem\n",
    "distance, index = ann_model.kneighbors(query_vector)\n",
    "\n",
    "# Retornar a tradução aproximadamente mais próxima encontrada\n",
    "nearest_translation = french_translations[index[0][0]]\n",
    "print(\"Tradução aproximadamente mais próxima:\", nearest_translation)\n",
    "```\n",
    "\n",
    "Neste exemplo, estamos usando a biblioteca scikit-learn para encontrar a tradução aproximadamente mais próxima de uma frase de origem em inglês usando ANN. O processo é rápido e eficiente, mesmo em grandes conjuntos de dados de pares de frases.\n",
    "\n",
    "**Explicação**:\n",
    "\n",
    "**1. Indexação das Frases em Inglês**: Usamos a classe TfidfVectorizer do scikit-learn para converter as frases em inglês em vetores de características. Esses vetores representam a \"assinatura\" de cada frase em inglês com base na frequência dos termos.\n",
    "\n",
    "**2. Aplicação de ANN aos Vetores de Características**: A classe NearestNeighbors é usada para aplicar a técnica de ANN aos vetores de características das frases em inglês. Isso cria uma estrutura de dados que facilita a busca eficiente por traduções aproximadamente mais próximas.\n",
    "\n",
    "**3. Consulta com uma Frase em Inglês**: Quando uma nova frase em inglês é recebida como consulta, ela também é transformada em um vetor de características usando o mesmo TfidfVectorizer.\n",
    "\n",
    "**4. Busca pela Tradução Aproximadamente Mais Próxima**: Usando a classe NearestNeighbors, buscamos a tradução aproximadamente mais próxima da frase de origem em inglês. A busca é feita no conjunto de dados indexado de vetores de características das frases em inglês, não diretamente nas traduções em francês.\n",
    "\n",
    "**5. Recuperação da Tradução Correspondente**: Depois de encontrar o índice da tradução aproximadamente mais próxima, podemos usar esse índice para acessar a lista french_translations e recuperar a tradução correspondente em francês.\n",
    "\n",
    "As traduções em francês não são explicitamente indexadas para a aplicação de ANN. Em vez disso, a busca é realizada nos vetores de características das frases em inglês, e a tradução correspondente é recuperada usando o índice encontrado. Isso permite uma busca eficiente por traduções aproximadamente mais próximas sem a necessidade de indexar separadamente as traduções em francês."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5cfff5",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Classification and Vector Spaces, disponível em https://www.coursera.org/learn/classification-vector-spaces-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15521585",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a818f630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-13T21:17:12.002066Z",
     "start_time": "2024-04-13T21:17:11.997072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A norma de Frobenius de A é: 7.14142842854285\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Definir a matriz A\n",
    "A = np.array([[1, 3],\n",
    "              [4, 5]])\n",
    "\n",
    "# Calcular a norma de Frobenius\n",
    "norma_frobenius = np.linalg.norm(A, 'fro')\n",
    "\n",
    "print(\"A norma de Frobenius de A é:\", norma_frobenius)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.465px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
