# Stanford CS224N: NLP with Deep Learning
O processamento de linguagem natural (NLP) ou linguística computacional é uma das tecnologias mais importantes da era da informação. As aplicações da NLP estão por toda parte porque as pessoas comunicam quase tudo por meio de uma linguagem: pesquisa na web, publicidade, e-mails, atendimento ao cliente, tradução de idiomas, agentes virtuais, relatórios médicos, política, etc. Na última década, as abordagens de aprendizagem profunda (ou redes neurais) obtiveram desempenho muito alto em muitas tarefas diferentes de NLP, usando modelos neurais únicos de ponta a ponta que não exigem engenharia tradicional de recursos específicos para tarefas. Neste curso, os alunos obterão uma introdução completa às pesquisas de ponta em Deep Learning para NLP. Através de palestras, trabalhos e um projeto final, os alunos aprenderão as habilidades necessárias para projetar, implementar e compreender seus próprios modelos de redes neurais, utilizando o framework [Pytorch](https://pytorch.org/).

Acesse o cronograma, materiais, recuros e programa do curso [aqui](https://web.stanford.edu/class/cs224n/), e [aqui](https://web.stanford.edu/class/cs224u/background.html)

Playlist com as aulas no [youtube](https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ)

# Summary
1. [Introdução](#introduction-and-word-vectors)
2. [Neural Classifiers](#neural-classifiers)
3. [Backpropagation and Neural Networks](#backpropagation-and-neural-networks)
4. [Syntactic Structure and Dependency Parsing](#syntactic-structure-and-dependency-parsing)
5. [Recurrent Neural Networks (RNN)](#recurrent-neural-networks-rnn)
6. [Simple and LSTM RNNs](#simple-and-lstm-rnns)
7. [Translation, Seq2Seq, Attention](#translation-seq2seq-attention)
8. [Self-Attention and Transformers](#self-attention-and-transformers)
9. [Transformers and Pretraining](#transformers-and-pretraining)
10. [Question Answering](#question-answering)
11. [Natural Language Generation](#natural-language-generation)
12. [Coreference Resolution](#coreference-resolution)
13. [T5 and Large Language Models](#t5-and-large-language-models)
14. [Add Knowledge to Language Models](#add-knowledge-to-language-models)
15. [Social and Ethical Considerations](#social-and-ethical-considerations)
16. [Model Analysis and Explanation](#model-analysis-and-explanation)
17. [Future of NLP + Deep Learning](#future-of-nlp--deep-learning)
18. [Low Resource Machine Translation](#low-resource-machine-translation)
19. [BERT and Other Pre-trained Language Models](#bert-and-other-pre-trained-language-models)
20. [Socially Intelligent NLP Systems](#socially-intelligent-nlp-systems)
21. [Building Knowledge Representation](#building-knowledge-representation)
22. [Scaling Language Models](#scaling-language-models)
23. [Conclusão](#conclusão)


## Introduction and Word Vectors
Conteúdo...

## Neural Classifiers
Conteúdo...

## Backpropagation and Neural Networks
Conteúdo...

## Syntactic Structure and Dependency Parsing
Conteúdo...

## Recurrent Neural Networks (RNN)
Conteúdo...

## Simple and LSTM RNNs
Conteúdo...

## Translation, Seq2Seq, Attention
Conteúdo...

## Self-Attention and Transformers
Conteúdo...

## Transformers and Pretraining
Conteúdo...

## Question Answering
Conteúdo...

## Natural Language Generation
Conteúdo...

## Coreference Resolution
Conteúdo...

## T5 and Large Language Models
Conteúdo...

## Add Knowledge to Language Models
Conteúdo...

## Social and Ethical Considerations
Conteúdo...

## Model Analysis and Explanation
Conteúdo...

## Future of NLP + Deep Learning
Conteúdo...

## Low Resource Machine Translation
Conteúdo...

## BERT and Other Pre-trained Language Models
Conteúdo...

## Socially Intelligent NLP Systems
Conteúdo...

## Building Knowledge Representation
Conteúdo...

## Scaling Language Models
Conteúdo...

