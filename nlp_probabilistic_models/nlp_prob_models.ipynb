{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10b0d1bf",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Probabilistic Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Classification and Vector Spaces da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1427afca",
   "metadata": {},
   "source": [
    "# Week 1 - Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd5fd3f",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Word probabilities\n",
    "- Dynamic programming\n",
    "- Minimum edit distance\n",
    "- Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b934a3",
   "metadata": {},
   "source": [
    "## Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca7592",
   "metadata": {},
   "source": [
    "A autocorreção em NLP é uma técnica que corrige automaticamente erros ortográficos ou de digitação em texto. Ela é amplamente utilizada em aplicativos de mensagens, processadores de texto e mecanismos de busca para melhorar a experiência do usuário. Ela é geralmente implementada usando modelos de **linguagem probabilísticos**, como os modelos de N-gramas. Esses modelos calculam a probabilidade de uma sequência de palavras e sugerem a sequência mais provável, dada uma palavra mal digitada. Eles são treinados em grandes corpora de texto para aprender padrões comuns na linguagem.\n",
    "\n",
    "<img src=\"./imgs/autocorrect_phone.png\">\n",
    "\n",
    "\n",
    "Para implementar um sistema de autocorreção, precisamos seguir as seguintes etapas:\n",
    "\n",
    "1. **Identificação do Erro:**\n",
    "   - O primeiro passo é identificar que uma palavra pode estar incorreta. Isso é feito comparando a palavra digitada com um dicionário ou usando técnicas mais avançadas, como distância de edição/edit distance (Levenshtein), que mede a diferença entre duas palavras com base nas operações necessárias para transformar uma na outra (inserção, exclusão, substituição).\n",
    "\n",
    "2. **Sugestão de Correção:**\n",
    "   - Uma vez identificado o erro, o sistema de autocorreção sugere uma ou mais correções com base na probabilidade das palavras no contexto. Isso pode ser feito usando modelos de N-gramas, onde a probabilidade de uma palavra é calculada com base nas palavras anteriores.\n",
    "\n",
    "3. **Escolha da Melhor Correção:**\n",
    "   - A sugestão de correção é escolhida com base em critérios como a probabilidade da palavra correta no contexto, a distância de edição mínima da palavra digitada e outras heurísticas.\n",
    "\n",
    "4. **Aplicação da Correção:**\n",
    "   - A correção é então aplicada ao texto, substituindo a palavra mal digitada pela palavra correta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643dda9c",
   "metadata": {},
   "source": [
    "**Exemplos**\n",
    "\n",
    "1. **Erro de Digitação Simples:**\n",
    "   - **Texto Digitado:** \"Eu gosto de mangaa.\"\n",
    "   - **Correção Proposta:** \"Eu gosto de manga.\"\n",
    "   - **Explicação:** O sistema identifica que \"mangaa\" pode ser um erro de digitação para \"manga\" com base na proximidade no teclado e sugere a correção.\n",
    "\n",
    "2. **Inversão de Letras:**\n",
    "   - **Texto Digitado:** \"Estarmos esperndo por você.\"\n",
    "   - **Correção Proposta:** \"Estaremos esperando por você.\"\n",
    "   - **Explicação:** O sistema percebe que \"esperndo\" pode ser um erro de inversão de letras em \"esperando\" e sugere a correção.\n",
    "\n",
    "3. **Erros de Espaçamento:**\n",
    "   - **Texto Digitado:** \"Nãoseinada\"\n",
    "   - **Correção Proposta:** \"Não sei nada\"\n",
    "   - **Explicação:** O sistema identifica que \"Nãoseinada\" pode ser uma combinação de palavras e sugere a separação correta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a3de58",
   "metadata": {},
   "source": [
    "**Limitações**\n",
    "- **Palavras Fora do Vocabulário:** Se uma palavra está muito fora do vocabulário do modelo, a autocorreção pode falhar em sugerir a correção correta.\n",
    "- **Ambiguidade:** Em casos de ambiguidade, onde várias correções são possíveis, a autocorreção pode sugerir uma correção incorreta.\n",
    "- **Contexto Limitado:** Modelos de N-gramas têm um contexto limitado, então a autocorreção pode não capturar nuances mais complexas da linguagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c59f820",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f0bc8a",
   "metadata": {},
   "source": [
    "### Métodos de Identificação de Palavras Erradas\n",
    "\n",
    "Identificar palavras erradas é o primeiro passo crucial no processo de autocorreção em NLP. Isso envolve a detecção de erros ortográficos, de digitação, e até erros gramaticais. Vamos explorar os métodos mais comuns para identificar palavras erradas, com explicações e exemplos.\n",
    "\n",
    "\n",
    "1. **Comparação com um Dicionário:**\n",
    "   - **Descrição:** Um método básico é comparar cada palavra do texto com um dicionário de palavras válidas. Se uma palavra não estiver no dicionário, é marcada como errada.\n",
    "   - **Exemplo:** Se o texto contiver \"applle\", a palavra \"applle\" não será encontrada no dicionário e será marcada como incorreta.\n",
    "\n",
    "\n",
    "2. **Distância de Edição (Levenshtein):**\n",
    "   - **Descrição:** A distância de edição mede quantas operações (inserção, deleção, substituição) são necessárias para transformar uma palavra em outra. Palavras com uma pequena distância de edição em relação a palavras do dicionário são consideradas erradas.\n",
    "   - **Exemplo:** A palavra \"bok\" tem uma distância de edição de 1 em relação a \"book\" (substituição de 'k' por 'o'), indicando um possível erro de digitação.\n",
    "\n",
    "\n",
    "3. **Modelos de N-gramas:**\n",
    "   - **Descrição:** Modelos de N-gramas podem identificar palavras erradas com base na improbabilidade de uma sequência de palavras. Se uma palavra resulta em uma sequência de N-gramas que raramente ocorre no corpus de treinamento, pode ser considerada errada.\n",
    "   - **Exemplo:** Em \"I have a blak cat\", a sequência \"a blak cat\" pode ser menos provável que \"a black cat\", indicando que \"blak\" pode estar errado.\n",
    "\n",
    "\n",
    "4. **Redes Neurais e Modelos de Linguagem:**\n",
    "   - **Descrição:** Modelos de linguagem avançados, como BERT ou GPT, podem identificar palavras erradas ao avaliar a coerência do contexto. Eles são treinados em grandes quantidades de texto e podem detectar palavras que não fazem sentido no contexto dado.\n",
    "   - **Exemplo:** Em \"She drived the car\", um modelo de linguagem pode identificar que \"drived\" é incorreto no contexto e sugerir \"drove\".\n",
    "\n",
    "#### Exemplos Práticos de Identificação de Palavras Erradas\n",
    "\n",
    "1. **Exemplo com Dicionário:**\n",
    "   - **Texto:** \"I am lerning NLP.\"\n",
    "   - **Identificação:** A palavra \"lerning\" não está no dicionário.\n",
    "   - **Correção Proposta:** \"I am learning NLP.\"\n",
    "\n",
    "2. **Exemplo com Distância de Edição:**\n",
    "   - **Texto:** \"He went to the shcool.\"\n",
    "   - **Identificação:** A palavra \"shcool\" tem uma distância de edição de 1 em relação a \"school\".\n",
    "   - **Correção Proposta:** \"He went to the school.\"\n",
    "\n",
    "3. **Exemplo com Modelos de N-gramas:**\n",
    "   - **Texto:** \"The quick brown fox jmps over the lazy dog.\"\n",
    "   - **Identificação:** A sequência \"fox jmps\" é muito improvável.\n",
    "   - **Correção Proposta:** \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "4. **Exemplo com Modelos de Linguagem:**\n",
    "   - **Texto:** \"He gived her a gift.\"\n",
    "   - **Identificação:** O modelo de linguagem identifica que \"gived\" não faz sentido no contexto e sugere \"gave\".\n",
    "   - **Correção Proposta:** \"He gave her a gift.\"\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Palavras Novas ou Jargões:** Palavras que são novas, jargões ou nomes próprios podem não estar em um dicionário, levando a falsos positivos.\n",
    "- **Erros Contextuais:** Alguns erros só podem ser identificados corretamente no contexto adequado, o que modelos de linguagem mais avançados fazem melhor.\n",
    "- **Desempenho:** Métodos mais avançados, como modelos de linguagem, geralmente têm melhor desempenho, mas requerem mais poder computacional e dados para treinamento.\n",
    "\n",
    "\n",
    "Identificar palavras erradas é um processo que pode ser feito de várias maneiras, desde métodos simples como comparação com um dicionário até técnicas avançadas envolvendo modelos de linguagem. Cada método tem seus prós e contras, e a escolha do método adequado depende do contexto e dos requisitos específicos da aplicação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc39203e",
   "metadata": {},
   "source": [
    "### Cálculo da Distância de Edição\n",
    "\n",
    "A distância de edição, ou distância de Levenshtein, é uma métrica que mede o quão diferentes duas strings são, calculando o número mínimo de operações necessárias para transformar uma string em outra. As operações permitidas são inserção, deleção e substituição de caracteres.\n",
    "\n",
    "O algoritmo de Levenshtein usa uma abordagem de programação dinâmica para calcular a distância de edição entre duas strings. Aqui está uma descrição do algoritmo:\n",
    "\n",
    "1. **Inicialização:**\n",
    "   - Crie uma matriz $d$ de tamanho $(m+1) \\times (n+1)$, onde $m$ é o comprimento da primeira string $s$ e $n$ é o comprimento da segunda string $t$.\n",
    "   - Inicialize $d[i][0] = i$ para $i = 0, 1, ..., m$.\n",
    "   - Inicialize $d[0][j] = j$ para $j = 0, 1, ..., n$.\n",
    "\n",
    "2. **Recorrência:**\n",
    "   - Para cada $i = 1, ..., m$ e $j = 1, ..., n$:\n",
    "     - Se $s[i-1] = t[j-1]$, então $custo = 0$; caso contrário, $custo = 1$.\n",
    "     - $d[i][j] = \\min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + custo)$.\n",
    "\n",
    "3. **Resultado:**\n",
    "   - A distância de edição é encontrada em $d[m][n]$.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Aqui está uma implementação do cálculo da distância de edição em Python:\n",
    "\n",
    "```python\n",
    "def distancia_de_edicao(s, t):\n",
    "    m = len(s)\n",
    "    n = len(t)\n",
    "    \n",
    "    # Criação da matriz\n",
    "    d = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    \n",
    "    # Inicialização da matriz\n",
    "    for i in range(m + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        d[0][j] = j\n",
    "    \n",
    "    # Preenchimento da matriz com os valores de distância\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                custo = 0\n",
    "            else:\n",
    "                custo = 1\n",
    "            d[i][j] = min(d[i - 1][j] + 1,  # Deleção\n",
    "                          d[i][j - 1] + 1,  # Inserção\n",
    "                          d[i - 1][j - 1] + custo)  # Substituição\n",
    "    \n",
    "    # Distância de edição é encontrada na última célula da matriz\n",
    "    return d[m][n]\n",
    "\n",
    "# Exemplos\n",
    "s1 = \"kitten\"\n",
    "s2 = \"sitting\"\n",
    "print(f\"Distância de edição entre '{s1}' e '{s2}': {distancia_de_edicao(s1, s2)}\")\n",
    "\n",
    "s3 = \"flaw\"\n",
    "s4 = \"lawn\"\n",
    "print(f\"Distância de edição entre '{s3}' e '{s4}': {distancia_de_edicao(s3, s4)}\")\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Strings:** \"kitten\" e \"sitting\"\n",
    "   - **Cálculo:** Transformar \"kitten\" em \"sitting\" envolve as operações:\n",
    "     - Substituir 'k' por 's': \"sitten\"\n",
    "     - Substituir 'e' por 'i': \"sittin\"\n",
    "     - Inserir 'g' no final: \"sitting\"\n",
    "   - **Distância de Edição:** 3\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Strings:** \"flaw\" e \"lawn\"\n",
    "   - **Cálculo:** Transformar \"flaw\" em \"lawn\" envolve as operações:\n",
    "     - Substituir 'f' por 'l': \"law\"\n",
    "     - Inserir 'n' no final: \"lawn\"\n",
    "   - **Distância de Edição:** 2\n",
    "\n",
    "#### Visualização da Matriz\n",
    "\n",
    "Para ilustrar como a matriz $d$ é preenchida, vejamos o exemplo das strings \"kitten\" e \"sitting\":\n",
    "\n",
    "```\n",
    "     '' s i t t i n g\n",
    "  '' 0  1 2 3 4 5 6 7\n",
    "  k  1  1 2 3 4 5 6 7\n",
    "  i  2  2 1 2 3 4 5 6\n",
    "  t  3  3 2 1 2 3 4 5\n",
    "  t  4  4 3 2 1 2 3 4\n",
    "  e  5  5 4 3 2 2 3 4\n",
    "  n  6  6 5 4 3 3 2 3\n",
    "```\n",
    "\n",
    "A distância de edição é uma métrica poderosa para comparar a similaridade entre duas strings, sendo amplamente utilizada em tarefas de NLP, como correção ortográfica e reconhecimento de padrões. O algoritmo de Levenshtein é eficiente e fácil de implementar, fornecendo uma base sólida para muitas aplicações de processamento de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df260c",
   "metadata": {},
   "source": [
    "[Vídeo](https://www.youtube.com/watch?v=kyeyjPlKzJM) com explicação intuitiva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70977fbb",
   "metadata": {},
   "source": [
    "### Métodos para Filtrar Palavras Candidatas\n",
    "\n",
    "Filtrar palavras candidatas é um passo crucial após identificar palavras erradas para fornecer sugestões de correção. Este processo envolve gerar uma lista de possíveis correções para uma palavra mal escrita e, em seguida, classificar ou selecionar as melhores opções com base em vários critérios. Aqui estão os principais métodos para filtrar palavras candidatas, com explicações e exemplos.\n",
    "\n",
    "\n",
    "1. **Distância de Edição:**\n",
    "   - **Descrição:** Calcular a distância de edição entre a palavra errada e cada palavra do dicionário. As palavras com a menor distância de edição são consideradas as melhores candidatas.\n",
    "   - **Exemplo:** Para a palavra \"speling\", palavras como \"spelling\" (distância 1) e \"spieling\" (distância 2) seriam consideradas, com \"spelling\" sendo a melhor candidata devido à menor distância de edição.\n",
    "\n",
    "2. **N-gramas e Modelos de Linguagem:**\n",
    "   - **Descrição:** Utilizar modelos de N-gramas ou modelos de linguagem para avaliar a probabilidade das palavras candidatas no contexto da frase.\n",
    "   - **Exemplo:** Em \"I have a blak cat\", o modelo pode sugerir \"black\" como correção para \"blak\" porque \"a black cat\" é uma sequência mais comum e provável do que \"a blak cat\".\n",
    "\n",
    "3. **Frequência no Corpus:**\n",
    "   - **Descrição:** As palavras candidatas são classificadas com base na sua frequência em um corpus de texto. Palavras mais comuns são mais prováveis de serem correções corretas.\n",
    "   - **Exemplo:** Se \"hte\" foi digitado incorretamente e \"the\" é muito mais frequente no corpus do que outras combinações possíveis, \"the\" será a principal sugestão.\n",
    "\n",
    "4. **Contexto Semântico:**\n",
    "   - **Descrição:** Analisar o contexto semântico utilizando embeddings de palavras ou modelos contextuais (como BERT) para selecionar palavras que façam sentido no contexto da frase.\n",
    "   - **Exemplo:** Em \"I visited the captial city\", o modelo pode sugerir \"capital\" em vez de \"captial\" porque \"visited the capital city\" faz sentido semanticamente.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Vamos ver como esses métodos podem ser combinados para filtrar palavras candidatas. Para simplificação, usaremos apenas a distância de edição e a frequência no corpus neste exemplo.\n",
    "\n",
    "1. Calcular a distância de edição para cada palavra do dicionário.\n",
    "2. Classificar palavras por distância de edição.\n",
    "3. Utilizar frequência no corpus para ordenar palavras com a mesma distância.\n",
    "\n",
    "```python\n",
    "def distancia_de_edicao(s, t):\n",
    "    m = len(s)  # Comprimento da primeira string\n",
    "    n = len(t)  # Comprimento da segunda string\n",
    "\n",
    "    # Inicializa uma matriz (m+1) x (n+1) com zeros\n",
    "    d = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "\n",
    "    # Inicializa a primeira coluna da matriz\n",
    "    for i in range(m + 1):\n",
    "        d[i][0] = i\n",
    "\n",
    "    # Inicializa a primeira linha da matriz\n",
    "    for j in range(n + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    # Preenche a matriz com os valores de distância de edição\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                custo = 0  # Nenhum custo se os caracteres são iguais\n",
    "            else:\n",
    "                custo = 1  # Custo de substituição se os caracteres são diferentes\n",
    "            d[i][j] = min(d[i - 1][j] + 1,    # Custo de deleção\n",
    "                          d[i][j - 1] + 1,    # Custo de inserção\n",
    "                          d[i - 1][j - 1] + custo)  # Custo de substituição\n",
    "\n",
    "    return d[m][n]  # Retorna a distância de edição entre as duas strings\n",
    "\n",
    "def filtrar_candidatos(palavra_errada, dicionario, frequencia_corpus):\n",
    "    candidatos = []\n",
    "    # Calcula a distância de edição para cada palavra do dicionário\n",
    "    for palavra in dicionario:\n",
    "        dist = distancia_de_edicao(palavra_errada, palavra)\n",
    "        candidatos.append((palavra, dist))  # Adiciona a palavra e sua distância à lista de candidatos\n",
    "\n",
    "    # Ordena os candidatos primeiro pela distância de edição e depois pela frequência no corpus (em ordem decrescente)\n",
    "    candidatos.sort(key=lambda x: (x[1], -frequencia_corpus.get(x[0], 0)))\n",
    "\n",
    "    # Retorna apenas as palavras candidatas, sem as distâncias\n",
    "    return [candidato[0] for candidato in candidatos]\n",
    "\n",
    "# Exemplo de dicionário e frequências\n",
    "dicionario = [\"spelling\", \"spieling\", \"selling\", \"smiling\"]\n",
    "frequencia_corpus = {\"spelling\": 500, \"spieling\": 5, \"selling\": 300, \"smiling\": 150}\n",
    "\n",
    "# Palavra errada\n",
    "palavra_errada = \"speling\"\n",
    "\n",
    "# Filtrar palavras candidatas\n",
    "candidatos = filtrar_candidatos(palavra_errada, dicionario, frequencia_corpus)\n",
    "print(f\"Candidatos para '{palavra_errada}': {candidatos}\")\n",
    "\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Palavra Errada:** \"speling\"\n",
    "   - **Dicionário:** [\"spelling\", \"spieling\", \"selling\", \"smiling\"]\n",
    "   - **Frequência no Corpus:** {\"spelling\": 500, \"spieling\": 5, \"selling\": 300, \"smiling\": 150}\n",
    "   - **Candidatos:** [\"spelling\", \"selling\", \"smiling\", \"spieling\"]\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Palavra Errada:** \"recieve\"\n",
    "   - **Dicionário:** [\"receive\", \"recipe\", \"recite\"]\n",
    "   - **Frequência no Corpus:** {\"receive\": 1000, \"recipe\": 400, \"recite\": 50}\n",
    "   - **Candidatos:** [\"receive\", \"recipe\", \"recite\"]\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Balanceamento entre Precisão e Performance:** Calcular a distância de edição para um grande dicionário pode ser computacionalmente caro. Técnicas como truncagem de dicionário ou uso de índices podem melhorar a performance.\n",
    "- **Combinação de Critérios:** Usar múltiplos critérios (distância de edição, frequência no corpus, contexto semântico) pode aumentar a precisão das sugestões.\n",
    "- **Personalização:** Ajustar a frequência do corpus com base no domínio específico (por exemplo, termos médicos para um dicionário médico) pode melhorar os resultados.\n",
    "\n",
    "\n",
    "Filtrar palavras candidatas é um processo multifacetado que combina técnicas de comparação de strings, análise de frequência e compreensão contextual. Implementar uma abordagem robusta pode significativamente melhorar a precisão das sugestões de correção em sistemas de autocorreção e outras aplicações de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea00311e",
   "metadata": {},
   "source": [
    "### Métodos para Calcular as Probabilidades das Palavras\n",
    "\n",
    "Calcular as probabilidades das palavras é um passo importante para melhorar a precisão de sugestões em tarefas como correção ortográfica. Esse processo envolve a utilização de modelos de linguagem que atribuem uma probabilidade a cada palavra candidata com base em diferentes critérios, como frequência no corpus, contexto e modelos de n-gramas.\n",
    "\n",
    "\n",
    "1. **Frequência no Corpus:**\n",
    "   - **Descrição:** As palavras mais frequentes em um grande corpus de texto são consideradas mais prováveis.\n",
    "   - **Exemplo:** Se a palavra \"the\" aparece 5000 vezes em um corpus, sua probabilidade é maior do que a de uma palavra que aparece apenas 5 vezes.\n",
    "\n",
    "<img src=\"./imgs/calculating_word_prob.png\">\n",
    "\n",
    "2. **Modelos de N-gramas:**\n",
    "   - **Descrição:** Usar a frequência de sequências de palavras (n-gramas) para calcular a probabilidade de uma palavra em um dado contexto.\n",
    "   - **Exemplo:** Em um modelo de bigrama, a probabilidade de \"cat\" seguir \"the\" pode ser calculada como $(P(cat|the) = \\frac{C(the\\ cat)}{C(the)} $, onde $C$ denota contagens no corpus.\n",
    "\n",
    "\n",
    "3. **Modelos de Linguagem Baseados em Redes Neurais:**\n",
    "   - **Descrição:** Modelos como Word2Vec, GloVe, BERT, ou GPT podem gerar probabilidades para palavras com base em embeddings de palavras e contexto.\n",
    "   - **Exemplo:** Dado o contexto \"I have a bl__ cat\", um modelo de linguagem pode calcular a probabilidade de várias palavras preencherem o espaço em branco, como \"black\", \"blue\", etc.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Vamos ver como podemos implementar o cálculo de probabilidades de palavras usando frequências no corpus e um simples modelo de bigrama. Para isso precisamos seguir os seguintes passos:\n",
    "\n",
    "1. **Frequência no Corpus:** Calcular a probabilidade baseada na frequência relativa da palavra no corpus.\n",
    "2. **Modelo de Bigramas:** Calcular a probabilidade de uma palavra dado seu contexto anterior usando bigramas.\n",
    "\n",
    "\n",
    "```python\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Corpus de exemplo\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat chased the dog\",\n",
    "    \"the dog barked at the cat\"\n",
    "]\n",
    "\n",
    "# Função para calcular frequências unigrama e bigrama\n",
    "def calcular_frequencias(corpus):\n",
    "    unigramas = Counter()\n",
    "    bigramas = defaultdict(Counter)\n",
    "    \n",
    "    for frase in corpus:\n",
    "        palavras = frase.split()\n",
    "        for i in range(len(palavras)):\n",
    "            unigramas[palavras[i]] += 1\n",
    "            if i > 0:\n",
    "                bigramas[palavras[i-1]][palavras[i]] += 1\n",
    "    \n",
    "    total_palavras = sum(unigramas.values())\n",
    "    return unigramas, bigramas, total_palavras\n",
    "\n",
    "# Calcular frequências no corpus\n",
    "unigramas, bigramas, total_palavras = calcular_frequencias(corpus)\n",
    "\n",
    "# Função para calcular probabilidade unigrama\n",
    "def probabilidade_unigrama(palavra):\n",
    "    return unigramas[palavra] / total_palavras\n",
    "\n",
    "# Função para calcular probabilidade bigrama\n",
    "def probabilidade_bigrama(palavra_anterior, palavra):\n",
    "    if palavra_anterior in bigramas and palavra in bigramas[palavra_anterior]:\n",
    "        return bigramas[palavra_anterior][palavra] / unigramas[palavra_anterior]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Exemplo de cálculo de probabilidades\n",
    "palavra = \"cat\"\n",
    "palavra_anterior = \"the\"\n",
    "\n",
    "print(f\"Probabilidade unigrama de '{palavra}': {probabilidade_unigrama(palavra)}\")\n",
    "print(f\"Probabilidade bigrama de '{palavra}' dado '{palavra_anterior}': {probabilidade_bigrama(palavra_anterior, palavra)}\")\n",
    "\n",
    "output:\n",
    "Probabilidade unigrama de 'cat': 0.13043478260869565\n",
    "Probabilidade bigrama de 'cat' dado 'the': 0.375\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Palavra:** \"cat\"\n",
    "   - **Probabilidade Unigrama:** Calculada como a frequência da palavra \"cat\" dividida pelo total de palavras no corpus.\n",
    "   - **Probabilidade Bigrama:** Calculada como a frequência da sequência \"the cat\" dividida pela frequência de \"the\".\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Palavra:** \"dog\"\n",
    "   - **Probabilidade Unigrama:** Calculada como a frequência da palavra \"dog\" dividida pelo total de palavras no corpus.\n",
    "   - **Probabilidade Bigrama:** Calculada como a frequência da sequência \"the dog\" dividida pela frequência de \"the\".\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Balanceamento de Dados:** Um corpus pequeno ou desequilibrado pode levar a estimativas de probabilidade imprecisas. É importante utilizar um corpus representativo e suficientemente grande.\n",
    "- **Suavização:** Técnicas de suavização, como Laplace ou Good-Turing, são usadas para lidar com bigramas ou n-gramas que não aparecem no corpus.\n",
    "- **Modelos Avançados:** Modelos de linguagem baseados em redes neurais (como BERT ou GPT) podem fornecer probabilidades mais precisas ao considerar o contexto completo de uma frase.\n",
    "\n",
    "Calcular as probabilidades das palavras é um componente essencial em muitas aplicações de NLP, incluindo correção ortográfica e autocompletar. Usando frequências no corpus e modelos de n-gramas, podemos estimar essas probabilidades de forma eficaz. Em aplicações mais avançadas, modelos de linguagem baseados em redes neurais podem proporcionar uma compreensão mais profunda e precisa do contexto, resultando em sugestões de palavras ainda mais acertadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6db0ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Minimum edit distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bec66",
   "metadata": {},
   "source": [
    "### Operações de Distância Mínima de Edição\n",
    "\n",
    "A distância mínima de edição (ou distância de Levenshtein) é uma métrica que mede o número mínimo de operações necessárias para transformar uma string em outra. As operações permitidas são inserção, deleção e substituição de caracteres. Vamos detalhar cada operação e como elas são usadas para calcular a distância mínima de edição, seguido de exemplos práticos.\n",
    "\n",
    "\n",
    "1. **Inserção (Insert):** Adicionar um caractere à string. Exemplo: Transformar \"cat\" em \"cats\" requer uma inserção de 's' no final. A operação é `Insert('s')`.\n",
    "\n",
    "2. **Deleção (Delete):** Remover um caractere da string. Exemplo: Transformar \"cats\" em \"cat\" requer uma deleção de 's'. A operação é `Delete('s')`.\n",
    "\n",
    "3. **Substituição (Replace):** Substituir um caractere por outro. Exemplo: Transformar \"cat\" em \"cut\" requer uma substituição de 'a' por 'u'. A operação é `Replace('a', 'u')`.\n",
    "\n",
    "Podemos atribuir pesos diferentes para cada operação, para assim, buscar otimizar a menor distancia. Por exemplo, atribuir os pesos de 1, 1 e 2 para insert, delete e replace. Para buscar a menor distancia utilizamos o algoritmod e Levenshtein.\n",
    "\n",
    "<img src=\"./imgs/edit_distance_w.png\">\n",
    "\n",
    "O algoritmo de Levenshtein usa programação dinâmica para calcular a distância mínima de edição entre duas strings. Como já citado acima, segue os seguintes passos:\n",
    "\n",
    "1. **Inicialização:**\n",
    "   - Crie uma matriz `d` de tamanho `(m+1) x (n+1)`, onde `m` é o comprimento da primeira string `s` e `n` é o comprimento da segunda string `t`.\n",
    "   - Inicialize `d[i][0] = i` para `i = 0, 1, ..., m`.\n",
    "   - Inicialize `d[0][j] = j` para `j = 0, 1, ..., n`.\n",
    "\n",
    "2. **Preenchimento da Matriz:**\n",
    "   - Para cada `i = 1, ..., m` e `j = 1, ..., n`:\n",
    "     - Se `s[i-1] == t[j-1]`, então `custo = 0`; caso contrário, `custo = 1`.\n",
    "     - `d[i][j] = min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + custo)`.\n",
    "\n",
    "3. **Resultado:**\n",
    "   - A distância de edição é encontrada em `d[m][n]`.\n",
    "\n",
    "(Ver #2.3.2.1 a implementação e exemplos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e009acad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Minimum edit distance algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394ced89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T18:46:18.739139Z",
     "start_time": "2024-06-18T18:46:18.722360Z"
    }
   },
   "source": [
    "Quando estamos computando a distancia mínima de edição, começamos com a palavra fonte e temos que transformá-la na palavra alvo. \n",
    "\n",
    "<img src=\"./imgs/minimum_edit_target.png\">\n",
    "     \n",
    "Partindo de # para #, teremos um custo de 0. Partindo de p para #, teremos um custo de 1, porque faremos uma deleção. De p para s, teremos um custo de 2, porque esse é o custo mínimo que se poderia ter para ir de p a s. Podemos continuar assim preenchendo um elemento por vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be50787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T18:53:33.139135Z",
     "start_time": "2024-06-18T18:53:33.133892Z"
    }
   },
   "source": [
    "Para preencher a seguinte tabela:\n",
    "\n",
    "<img src=\"./imgs/minimum_edit1.png\">\n",
    "\n",
    "\n",
    "Existem três equações:\n",
    "\n",
    "- $D[i,j] = D[i-1, j] + \\text{del_cost}$: indica que você deseja preencher a célula atual (i,j) usando o custo na célula encontrada diretamente acima.\n",
    "\n",
    "- $D[i,j] = D[i, j-1] + \\text{ins_cost}$: indica que você deseja preencher a célula atual (i,j) usando o custo na célula localizada diretamente à sua esquerda.\n",
    "\n",
    "- $D[i,j] = D[i-1, j-1] + \\text{rep_cost}$: o custo rep pode ser 2 ou 0 dependendo se você vai realmente substituí-lo ou não.\n",
    "\n",
    "A cada passo você verifica os três caminhos possíveis de onde pode vir e seleciona o menos caro. Quando terminar, você obterá o seguinte:\n",
    "\n",
    "<img src=\"./imgs/minimum_edit2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe2aef5",
   "metadata": {},
   "source": [
    "# Week 2 - Part of Speech Tagging and Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc569f1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Part of Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cca180b",
   "metadata": {},
   "source": [
    "**Part of Speech Tagging** (ou marcação de partes do discurso) é o processo de atribuir uma etiqueta a cada palavra de um texto, indicando a sua função gramatical. As etiquetas típicas incluem categorias como substantivo, verbo, adjetivo, advérbio, pronome, preposição, conjunção, entre outros.\n",
    "\n",
    "O processo de POS Tagging envolve várias etapas:\n",
    "\n",
    "1. **Tokenização:** Dividir o texto em unidades menores, chamadas tokens (geralmente palavras).\n",
    "2. **Análise Contextual:** Avaliar o contexto em que cada palavra aparece para determinar a sua função gramatical.\n",
    "3. **Aplicação de Regras e Modelos Estatísticos:** Utilizar regras linguísticas e/ou modelos treinados em grandes corpora de textos anotados para prever a etiqueta correta.\n",
    "\n",
    "**Exemplos de POS Tagging**\n",
    "\n",
    "Considere a frase: \n",
    "- ``\"The quick brown fox jumps over the lazy dog.\"``\n",
    "\n",
    "Após a tokenização e POS Tagging, a frase pode ser anotada da seguinte forma:\n",
    "- ``The/DT quick/JJ brown/JJ fox/NN jumps/VBZ over/IN the/DT lazy/JJ dog/NN``\n",
    "\n",
    "Onde:\n",
    "- DT: Determinante\n",
    "- JJ: Adjetivo\n",
    "- NN: Substantivo\n",
    "- VBZ: Verbo, 3ª pessoa do singular presente\n",
    "- IN: Preposição\n",
    "\n",
    "POS tagging possui diversas aplicações, como:\n",
    "\n",
    "### 1. Named Entity Recognition (NER)\n",
    "\n",
    "**Named Entity Recognition** é a tarefa de identificar e classificar entidades mencionadas no texto em categorias predefinidas, como nomes de pessoas, organizações, localizações, datas, etc. O POS Tagging é uma etapa importante no NER, pois ajuda a identificar as entidades com base na função gramatical das palavras.\n",
    "\n",
    "**Exemplo:**\n",
    "- Texto: \"Barack Obama was born in Hawaii.\"\n",
    "- NER Resultado: [Barack Obama/PERSON, Hawaii/LOCATION]\n",
    "\n",
    "### 2. Coreference Resolution\n",
    "\n",
    "**Coreference Resolution** refere-se à tarefa de identificar quando diferentes expressões no texto referem-se à mesma entidade. O POS Tagging é essencial aqui para identificar pronomes e outras referências que podem apontar para entidades mencionadas anteriormente no texto.\n",
    "\n",
    "**Exemplo:**\n",
    "- Texto: \"John said he would come. He is very reliable.\"\n",
    "- Coreference Resultado: [John -> he]\n",
    "\n",
    "### 3. Speech Recognition\n",
    "\n",
    "**Speech Recognition** é o processo de converter áudio em texto. O POS Tagging pode ser usado para melhorar a precisão do reconhecimento, fornecendo pistas contextuais sobre a estrutura gramatical esperada das frases.\n",
    "\n",
    "**Exemplo:**\n",
    "- Áudio: \"I can see the sea.\"\n",
    "- Reconhecimento com POS: \"I/PRP can/MD see/VB the/DT sea/NN.\"\n",
    "\n",
    "### Como Implementar POS Tagging\n",
    "\n",
    "Existem várias bibliotecas e ferramentas para realizar POS Tagging. Um exemplo popular é a biblioteca `nltk` (Natural Language Toolkit) em Python.\n",
    "\n",
    "**Exemplo com `nltk`:**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Texto a ser analisado\n",
    "texto = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenização\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "\n",
    "# POS Tagging\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Exibição dos resultados\n",
    "print(tags)\n",
    "```\n",
    "\n",
    "**Saída esperada:**\n",
    "```\n",
    "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
    "```\n",
    "\n",
    "O POS Tagging é uma ferramenta fundamental em NLP, fornecendo informações gramaticais cruciais que são usadas em várias aplicações avançadas, como NER, Coreference Resolution e Speech Recognition. Ele permite uma análise mais profunda e precisa do texto, facilitando a construção de sistemas que compreendem e processam a linguagem natural de forma mais eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269df21e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c96d31",
   "metadata": {},
   "source": [
    "Markov Chains é um tipo de modelo estocástico que descreve a sequência de possíveis eventos. Nele, a probabilidade do evento seguinte, depende apenas do estado do evento anterior. Um estado é a representação da condição no momento presente, e, esse modelo pode ser representado como a estrutura de dados DAG/grafo. Nessa DAG/grafo, cada círculo representa o estado do modelo e as flechas representam a direção. Cada caminho de cada estado possui uma probabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3908fdb1",
   "metadata": {},
   "source": [
    "**Cadeias de Markov** são modelos probabilísticos que descrevem uma sequência de possíveis eventos, onde a probabilidade de cada evento depende apenas do estado atual e não dos eventos anteriores **(propriedade de Markov)**. Em outras palavras, uma cadeia de Markov é um sistema que transita de um estado para outro com base em probabilidades fixas. Esses modelos possuem os seguintes conceitos básicos:\n",
    "\n",
    "1. **Estados:** Conjunto de todos os possíveis estados do sistema.\n",
    "2. **Transições:** Mudanças de um estado para outro.\n",
    "3. **Probabilidades de Transição:** Probabilidades associadas a cada transição entre estados.\n",
    "\n",
    "\n",
    "Imagine um clima que pode ser \"ensolarado\", \"nublado\" ou \"chuvoso\". As probabilidades de transição entre esses estados podem ser representadas por uma matriz de transição:\n",
    "\n",
    "|        | Ensolarado | Nublado | Chuvoso |\n",
    "|--------|-------------|---------|---------|\n",
    "| **Ensolarado** | 0.8         | 0.15    | 0.05    |\n",
    "| **Nublado**    | 0.2         | 0.6     | 0.2     |\n",
    "| **Chuvoso**    | 0.25        | 0.25    | 0.5     |\n",
    "\n",
    "Se hoje está ensolarado, a probabilidade de estar ensolarado novamente amanhã é 0.8, nublado 0.15 e chuvoso 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684c38e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Markov Chains and POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292faff",
   "metadata": {},
   "source": [
    "No contexto de **Part of Speech Tagging**, as cadeias de Markov são usadas para modelar as sequências de etiquetas gramaticais (tags). A ideia é que a etiqueta de uma palavra depende da etiqueta da palavra anterior.\n",
    "\n",
    "<img src=\"./imgs/markov_chain_words.png\">\n",
    "<img src=\"./imgs/markov_chain_dag.png\">\n",
    "\n",
    "<img src=\"./imgs/markov_chain_words_dag.png\">\n",
    "<img src=\"./imgs/markov_chain_transition_matrix.png\">\n",
    "\n",
    "Na matriz de transição acima, temos 40% de probabilidade da frase iniciar com NN (noun = substantivo), 10% de começar com um verbo e 50% de iniciar com outro POS.\n",
    "\n",
    "Para encontrar a sequência mais provável de etiquetas em POS Tagging, comummente é utilizado o Algoritmo de Viterbi, usando um modelo de **Cadeia de Markov Oculta (HMM)**.\n",
    "\n",
    "**Passos do Algoritmo:**\n",
    "\n",
    "1. **Inicialização:** Definir as probabilidades iniciais para o primeiro estado (etiqueta da primeira palavra).\n",
    "2. **Recursão:** Para cada palavra subsequente, calcular a probabilidade de cada etiqueta baseada nas etiquetas anteriores e na probabilidade de transição.\n",
    "3. **Terminação:** Selecionar a sequência de etiquetas com a maior probabilidade.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Considere uma frase \"The cat sleeps\".\n",
    "\n",
    "1. **Probabilidades de Emissão:** Probabilidade de uma palavra ser uma certa etiqueta (e.g., P(\"The\"|DT) = 0.9).\n",
    "2. **Probabilidades de Transição:** Probabilidade de uma etiqueta seguir outra (e.g., P(DT -> NN) = 0.8).\n",
    "\n",
    "Aplicando o algoritmo de Viterbi, podemos calcular a sequência mais provável de etiquetas: \"The/DT cat/NN sleeps/VBZ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6fdae",
   "metadata": {},
   "source": [
    "### Código Exemplo de POS Tagging com HMM\n",
    "\n",
    "Aqui está um exemplo simplificado de como um HMM pode ser usado para POS tagging em Python:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "# Treinamento de dados de exemplo\n",
    "train_data = [\n",
    "    [('The', 'DT'), ('cat', 'NN'), ('sleeps', 'VBZ')],\n",
    "    [('A', 'DT'), ('dog', 'NN'), ('barks', 'VBZ')],\n",
    "]\n",
    "\n",
    "# Criação e treino do modelo HMM\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)\n",
    "\n",
    "# Teste com uma nova sentença\n",
    "test_sentence = ['The', 'dog', 'sleeps']\n",
    "tagged_sentence = hmm_tagger.tag(test_sentence)\n",
    "print(tagged_sentence)\n",
    "```\n",
    "\n",
    "**Saída esperada:**\n",
    "```\n",
    "[('The', 'DT'), ('dog', 'NN'), ('sleeps', 'VBZ')]\n",
    "```\n",
    "\n",
    "Este exemplo treina um HMM simples para POS tagging e usa o modelo para etiquetar uma nova sentença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ac09e45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T19:04:49.750703Z",
     "start_time": "2024-06-19T19:04:48.064581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('dog', 'NN'), ('sleeps', 'VBZ')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaoag/.local/lib/python3.10/site-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "/home/joaoag/.local/lib/python3.10/site-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "/home/joaoag/.local/lib/python3.10/site-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "# Treinamento de dados de exemplo\n",
    "train_data = [\n",
    "    [('The', 'DT'), ('cat', 'NN'), ('sleeps', 'VBZ')],\n",
    "    [('A', 'DT'), ('dog', 'NN'), ('barks', 'VBZ')],\n",
    "]\n",
    "\n",
    "# Criação e treino do modelo HMM\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)\n",
    "\n",
    "# Teste com uma nova sentença\n",
    "test_sentence = ['The', 'dog', 'sleeps']\n",
    "tagged_sentence = hmm_tagger.tag(test_sentence)\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a39f75",
   "metadata": {},
   "source": [
    "### Markov Chains in Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef4282",
   "metadata": {},
   "source": [
    "No **Reconhecimento de Fala**, as cadeias de Markov são usadas para modelar a sequência de fonemas ou palavras. Um exemplo comum é o uso de **Modelos de Markov Ocultos (HMMs)** para representar a relação entre o áudio (observações) e as sequências de palavras ou fonemas (estados ocultos).\n",
    "\n",
    "**Passos no Reconhecimento de Fala:**\n",
    "\n",
    "1. **Treinamento:** Usar dados de áudio etiquetados para treinar o modelo HMM, ajustando as probabilidades de transição entre fonemas e as probabilidades de emissão de observações (características do áudio).\n",
    "2. **Decodificação:** Dado um novo trecho de áudio, usar o algoritmo de Viterbi para encontrar a sequência mais provável de fonemas ou palavras que correspondem ao áudio.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Para a frase falada \"Hello world\":\n",
    "\n",
    "1. **Probabilidades de Emissão:** Probabilidades de um segmento de áudio ser emitido por um determinado fonema.\n",
    "2. **Probabilidades de Transição:** Probabilidades de um fonema seguir outro fonema.\n",
    "\n",
    "Usando HMMs, o sistema pode decodificar o áudio para determinar a sequência de palavras \"Hello world\".\n",
    "\n",
    "As Cadeias de Markov, particularmente os Modelos de Markov Ocultos, são ferramentas poderosas em NLP e reconhecimento de fala, permitindo modelar dependências sequenciais em dados. Em POS Tagging, elas ajudam a prever a sequência de etiquetas gramaticais baseadas no contexto, enquanto no reconhecimento de fala, elas facilitam a interpretação de sequências de áudio em texto significativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbfa99f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb18ca92",
   "metadata": {},
   "source": [
    "**Hidden Markov Model** (HMM) é uma extensão das Cadeias de Markov, onde os estados não são diretamente observáveis (ocultos). Em vez disso, observamos sinais ou evidências que são probabilisticamente dependentes dos estados ocultos. HMM é amplamente utilizado em tarefas como reconhecimento de fala, POS tagging, entre outras. Conceitos Básicos do HMM:\n",
    "\n",
    "1. **Estados Ocultos:** Conjunto de estados que não são diretamente observáveis.\n",
    "2. **Observações:** Conjunto de observações que são visíveis e dependem dos estados ocultos.\n",
    "3. **Probabilidades de Transição:** Probabilidades de transitar de um estado oculto para outro.\n",
    "4. **Probabilidades de Emissão:** Probabilidades de um estado oculto gerar uma determinada observação.\n",
    "5. **Probabilidades Iniciais:** Probabilidades de começar em cada estado oculto.\n",
    "\n",
    "Um HMM é definido por:\n",
    "- Um conjunto de estados ocultos $ S = \\{s_1, s_2, \\ldots, s_N\\} $ ou $ Q = \\{q_1, q_2, \\ldots, s_N\\} $.\n",
    "- Um conjunto de observações $ O = \\{o_1, o_2, \\ldots, o_M\\} $.\n",
    "- Uma matriz de transição $ A $, onde $ A_{ij} $ é a probabilidade de transitar de $ s_i $ para $ s_j $.\n",
    "- Uma matriz de emissão $ B $, onde $ B_{jk} $ é a probabilidade de observar $ o_k $ dado o estado $ s_j $.\n",
    "- Um vetor de probabilidades iniciais $ \\pi $, onde $ \\pi_i $ é a probabilidade inicial de estar no estado $ s_i $.\n",
    "\n",
    "As **probabilidades de transição** permitiram identificar a probabilidade de transição de um POS para outro. Já nos **modelos de Markov ocultos**, usamos **probabilidades de emissão** que dão a probabilidade de passar de um estado (tag POS) **para uma palavra específica**.\n",
    "\n",
    "<img src=\"./imgs/hmm_states.png\">\n",
    "\n",
    "\n",
    "Por exemplo, dado que você está em um estado verb, podemos passar para outras palavras com certas probabilidades. Esta matriz de emissão B será usada com sua matriz de transição A, para ajudar a identificar a classe gramatical de uma palavra em uma frase. Para preencher sua matriz B, podemos simplesmente ter um conjunto de dados rotulado e calcular as probabilidades de ir de um POS para cada palavra do seu vocabulário. \n",
    "\n",
    "<img src=\"./imgs/hmm_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46b1cef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:32:03.075918Z",
     "start_time": "2024-06-19T20:32:03.067065Z"
    }
   },
   "source": [
    "### Aplicações do HMM\n",
    "\n",
    "No **POS Tagging**, as palavras em uma sentença são as observações e as etiquetas gramaticais (partes do discurso) são os estados ocultos.\n",
    "\n",
    "Considere a frase \"The cat sleeps\".\n",
    "\n",
    "- Observações: [\"The\", \"cat\", \"sleeps\"]\n",
    "- Estados Ocultos: [\"DT\", \"NN\", \"VBZ\"]\n",
    "\n",
    "**Passos:**\n",
    "\n",
    "1. **Inicialização:** Definir probabilidades iniciais para cada etiqueta gramatical (estado oculto).\n",
    "2. **Transição:** Definir probabilidades de transição entre etiquetas gramaticais.\n",
    "3. **Emissão:** Definir probabilidades de emissão das palavras dado cada etiqueta gramatical.\n",
    "\n",
    "No **reconhecimento de fala**, os sinais de áudio (características extraídas do áudio) são as observações e as sequências de palavras ou fonemas são os estados ocultos.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Considere a sequência de áudio correspondente a \"Hello world\".\n",
    "\n",
    "- Observações: [características do áudio]\n",
    "- Estados Ocultos: [\"H\", \"e\", \"l\", \"l\", \"o\", \" \", \"w\", \"o\", \"r\", \"l\", \"d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0e932",
   "metadata": {},
   "source": [
    "### Exemplo Prático em Python\n",
    "\n",
    "Vamos implementar um exemplo simples de HMM para POS tagging usando a biblioteca `hmmlearn`.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Definir os estados (etiquetas gramaticais)\n",
    "states = [\"DT\", \"NN\", \"VBZ\"]\n",
    "n_states = len(states)\n",
    "\n",
    "# Definir as observações (palavras)\n",
    "observations = [\"The\", \"cat\", \"sleeps\"]\n",
    "n_observations = len(observations)\n",
    "\n",
    "# Definir as sequências observadas em números\n",
    "obs_map = {word: idx for idx, word in enumerate(observations)}\n",
    "obs_seq = [obs_map[word] for word in observations]\n",
    "\n",
    "# Inicializar o modelo HMM\n",
    "model = hmm.MultinomialHMM(n_components=n_states, n_iter=100)\n",
    "\n",
    "# Probabilidades iniciais\n",
    "model.startprob_ = np.array([0.6, 0.3, 0.1])\n",
    "\n",
    "# Probabilidades de transição\n",
    "model.transmat_ = np.array([\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.3, 0.4, 0.3],\n",
    "    [0.5, 0.2, 0.3]\n",
    "])\n",
    "\n",
    "# Probabilidades de emissão\n",
    "model.emissionprob_ = np.array([\n",
    "    [0.6, 0.3, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "# Ajustar o modelo com a sequência de observação\n",
    "obs_seq = np.array(obs_seq).reshape(-1, 1)\n",
    "logprob, state_seq = model.decode(obs_seq, algorithm=\"viterbi\")\n",
    "\n",
    "# Mapeando os estados de volta para as etiquetas gramaticais\n",
    "state_map = {idx: state for idx, state in enumerate(states)}\n",
    "tagged_seq = [state_map[state] for state in state_seq]\n",
    "\n",
    "print(\"Observações:\", observations)\n",
    "print(\"Estados preditos:\", tagged_seq)\n",
    "```\n",
    "\n",
    "**Saída esperada:**\n",
    "```\n",
    "Observações: ['The', 'cat', 'sleeps']\n",
    "Estados preditos: ['DT', 'NN', 'VBZ']\n",
    "```\n",
    "\n",
    "Neste exemplo, usamos um HMM simples para etiquetar uma frase. O HMM é treinado com probabilidades de transição e emissão predefinidas e, em seguida, decodificamos a sequência de observações para encontrar a sequência mais provável de estados ocultos (etiquetas gramaticais)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a9480b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Calculating Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93d6b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95b768",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e38af271",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1735fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "Explique, descreva e dê exemplos de mispelled words e additive words em autocorrects em NLP\n",
    "Explique, descreva e dê exemplos de como calcular probabilidades de palavras corretas em autocorrects em NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aeb029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Calculating the Trasition Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1f7456",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## The Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c433703",
   "metadata": {},
   "source": [
    "O Algoritmo de Viterbi é usado para encontrar a sequência mais provável de estados ocultos (etiquetas) dada uma sequência de observações (palavras ou sinais de áudio).\n",
    "\n",
    "**Passos do Algoritmo de Viterbi:**\n",
    "\n",
    "1. **Inicialização:**\n",
    "   $$\n",
    "    V_1(i) = \\pi_i \\cdot B_i(O_1)\n",
    "   $$\n",
    "   \n",
    "    onde $ V_1(i) $ é a probabilidade do estado $ s_i $ ser o primeiro estado e $ O_1 $ é a primeira observação.\n",
    "\n",
    "2. **Recursão:**\n",
    "   $$\n",
    "    V_t(j) = \\max_i (V_{t-1}(i) \\cdot A_{ij}) \\cdot B_j(O_t)\n",
    "   $$\n",
    "   \n",
    "    onde $ V_t(j) $ é a probabilidade máxima da sequência até o estado $ s_j $ no tempo $ t $.\n",
    "\n",
    "3. **Terminação:**\n",
    "   $$\n",
    "    \\max_i V_T(i)\n",
    "   $$\n",
    "   \n",
    "    onde $ T $ é o tempo final.\n",
    "\n",
    "4. **Retropropagação:** Rastrear os estados que levaram à sequência de maior probabilidade.\n",
    "\n",
    "\n",
    "O HMM é uma ferramenta poderosa em NLP e reconhecimento de fala, permitindo modelar dependências sequenciais ocultas nos dados. Ele é utilizado em diversas aplicações, desde POS tagging até reconhecimento de fala, oferecendo um framework probabilístico robusto para interpretar e processar dados sequenciais."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d06bcb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Viterbi: Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60e17b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Viterbi: Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d45c5e9",
   "metadata": {},
   "source": [
    "# Week 3 - Autocomplete and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81bd2b7",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embedding with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75df6aca",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Classification and Vector Spaces, disponível em https://www.coursera.org/learn/probabilistic-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65dcae",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.441px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
