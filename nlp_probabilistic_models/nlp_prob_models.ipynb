{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4f7391",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Probabilistic Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Classification and Vector Spaces da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a224d49",
   "metadata": {},
   "source": [
    "# Week 1 - Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c7d24",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Word probabilities\n",
    "- Dynamic programming\n",
    "- Minimum edit distance\n",
    "- Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0259d7be",
   "metadata": {},
   "source": [
    "## Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bee0f6",
   "metadata": {},
   "source": [
    "A autocorreção em NLP é uma técnica que corrige automaticamente erros ortográficos ou de digitação em texto. Ela é amplamente utilizada em aplicativos de mensagens, processadores de texto e mecanismos de busca para melhorar a experiência do usuário. Ela é geralmente implementada usando modelos de **linguagem probabilísticos**, como os modelos de N-gramas. Esses modelos calculam a probabilidade de uma sequência de palavras e sugerem a sequência mais provável, dada uma palavra mal digitada. Eles são treinados em grandes corpora de texto para aprender padrões comuns na linguagem.\n",
    "\n",
    "<img src=\"./imgs/autocorrect_phone.png\">\n",
    "\n",
    "\n",
    "Para implementar um sistema de autocorreção, precisamos seguir as seguintes etapas:\n",
    "\n",
    "1. **Identificação do Erro:**\n",
    "   - O primeiro passo é identificar que uma palavra pode estar incorreta. Isso é feito comparando a palavra digitada com um dicionário ou usando técnicas mais avançadas, como distância de edição/edit distance (Levenshtein), que mede a diferença entre duas palavras com base nas operações necessárias para transformar uma na outra (inserção, exclusão, substituição).\n",
    "\n",
    "2. **Sugestão de Correção:**\n",
    "   - Uma vez identificado o erro, o sistema de autocorreção sugere uma ou mais correções com base na probabilidade das palavras no contexto. Isso pode ser feito usando modelos de N-gramas, onde a probabilidade de uma palavra é calculada com base nas palavras anteriores.\n",
    "\n",
    "3. **Escolha da Melhor Correção:**\n",
    "   - A sugestão de correção é escolhida com base em critérios como a probabilidade da palavra correta no contexto, a distância de edição mínima da palavra digitada e outras heurísticas.\n",
    "\n",
    "4. **Aplicação da Correção:**\n",
    "   - A correção é então aplicada ao texto, substituindo a palavra mal digitada pela palavra correta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e512a3d",
   "metadata": {},
   "source": [
    "**Exemplos**\n",
    "\n",
    "1. **Erro de Digitação Simples:**\n",
    "   - **Texto Digitado:** \"Eu gosto de mangaa.\"\n",
    "   - **Correção Proposta:** \"Eu gosto de manga.\"\n",
    "   - **Explicação:** O sistema identifica que \"mangaa\" pode ser um erro de digitação para \"manga\" com base na proximidade no teclado e sugere a correção.\n",
    "\n",
    "2. **Inversão de Letras:**\n",
    "   - **Texto Digitado:** \"Estarmos esperndo por você.\"\n",
    "   - **Correção Proposta:** \"Estaremos esperando por você.\"\n",
    "   - **Explicação:** O sistema percebe que \"esperndo\" pode ser um erro de inversão de letras em \"esperando\" e sugere a correção.\n",
    "\n",
    "3. **Erros de Espaçamento:**\n",
    "   - **Texto Digitado:** \"Nãoseinada\"\n",
    "   - **Correção Proposta:** \"Não sei nada\"\n",
    "   - **Explicação:** O sistema identifica que \"Nãoseinada\" pode ser uma combinação de palavras e sugere a separação correta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d52057",
   "metadata": {},
   "source": [
    "**Limitações**\n",
    "- **Palavras Fora do Vocabulário:** Se uma palavra está muito fora do vocabulário do modelo, a autocorreção pode falhar em sugerir a correção correta.\n",
    "- **Ambiguidade:** Em casos de ambiguidade, onde várias correções são possíveis, a autocorreção pode sugerir uma correção incorreta.\n",
    "- **Contexto Limitado:** Modelos de N-gramas têm um contexto limitado, então a autocorreção pode não capturar nuances mais complexas da linguagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a5bd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b05806",
   "metadata": {},
   "source": [
    "### Métodos de Identificação de Palavras Erradas\n",
    "\n",
    "Identificar palavras erradas é o primeiro passo crucial no processo de autocorreção em NLP. Isso envolve a detecção de erros ortográficos, de digitação, e até erros gramaticais. Vamos explorar os métodos mais comuns para identificar palavras erradas, com explicações e exemplos.\n",
    "\n",
    "\n",
    "1. **Comparação com um Dicionário:**\n",
    "   - **Descrição:** Um método básico é comparar cada palavra do texto com um dicionário de palavras válidas. Se uma palavra não estiver no dicionário, é marcada como errada.\n",
    "   - **Exemplo:** Se o texto contiver \"applle\", a palavra \"applle\" não será encontrada no dicionário e será marcada como incorreta.\n",
    "\n",
    "\n",
    "2. **Distância de Edição (Levenshtein):**\n",
    "   - **Descrição:** A distância de edição mede quantas operações (inserção, deleção, substituição) são necessárias para transformar uma palavra em outra. Palavras com uma pequena distância de edição em relação a palavras do dicionário são consideradas erradas.\n",
    "   - **Exemplo:** A palavra \"bok\" tem uma distância de edição de 1 em relação a \"book\" (substituição de 'k' por 'o'), indicando um possível erro de digitação.\n",
    "\n",
    "\n",
    "3. **Modelos de N-gramas:**\n",
    "   - **Descrição:** Modelos de N-gramas podem identificar palavras erradas com base na improbabilidade de uma sequência de palavras. Se uma palavra resulta em uma sequência de N-gramas que raramente ocorre no corpus de treinamento, pode ser considerada errada.\n",
    "   - **Exemplo:** Em \"I have a blak cat\", a sequência \"a blak cat\" pode ser menos provável que \"a black cat\", indicando que \"blak\" pode estar errado.\n",
    "\n",
    "\n",
    "4. **Redes Neurais e Modelos de Linguagem:**\n",
    "   - **Descrição:** Modelos de linguagem avançados, como BERT ou GPT, podem identificar palavras erradas ao avaliar a coerência do contexto. Eles são treinados em grandes quantidades de texto e podem detectar palavras que não fazem sentido no contexto dado.\n",
    "   - **Exemplo:** Em \"She drived the car\", um modelo de linguagem pode identificar que \"drived\" é incorreto no contexto e sugerir \"drove\".\n",
    "\n",
    "#### Exemplos Práticos de Identificação de Palavras Erradas\n",
    "\n",
    "1. **Exemplo com Dicionário:**\n",
    "   - **Texto:** \"I am lerning NLP.\"\n",
    "   - **Identificação:** A palavra \"lerning\" não está no dicionário.\n",
    "   - **Correção Proposta:** \"I am learning NLP.\"\n",
    "\n",
    "2. **Exemplo com Distância de Edição:**\n",
    "   - **Texto:** \"He went to the shcool.\"\n",
    "   - **Identificação:** A palavra \"shcool\" tem uma distância de edição de 1 em relação a \"school\".\n",
    "   - **Correção Proposta:** \"He went to the school.\"\n",
    "\n",
    "3. **Exemplo com Modelos de N-gramas:**\n",
    "   - **Texto:** \"The quick brown fox jmps over the lazy dog.\"\n",
    "   - **Identificação:** A sequência \"fox jmps\" é muito improvável.\n",
    "   - **Correção Proposta:** \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "4. **Exemplo com Modelos de Linguagem:**\n",
    "   - **Texto:** \"He gived her a gift.\"\n",
    "   - **Identificação:** O modelo de linguagem identifica que \"gived\" não faz sentido no contexto e sugere \"gave\".\n",
    "   - **Correção Proposta:** \"He gave her a gift.\"\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Palavras Novas ou Jargões:** Palavras que são novas, jargões ou nomes próprios podem não estar em um dicionário, levando a falsos positivos.\n",
    "- **Erros Contextuais:** Alguns erros só podem ser identificados corretamente no contexto adequado, o que modelos de linguagem mais avançados fazem melhor.\n",
    "- **Desempenho:** Métodos mais avançados, como modelos de linguagem, geralmente têm melhor desempenho, mas requerem mais poder computacional e dados para treinamento.\n",
    "\n",
    "\n",
    "Identificar palavras erradas é um processo que pode ser feito de várias maneiras, desde métodos simples como comparação com um dicionário até técnicas avançadas envolvendo modelos de linguagem. Cada método tem seus prós e contras, e a escolha do método adequado depende do contexto e dos requisitos específicos da aplicação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59d1af2",
   "metadata": {},
   "source": [
    "### Cálculo da Distância de Edição\n",
    "\n",
    "A distância de edição, ou distância de Levenshtein, é uma métrica que mede o quão diferentes duas strings são, calculando o número mínimo de operações necessárias para transformar uma string em outra. As operações permitidas são inserção, deleção e substituição de caracteres.\n",
    "\n",
    "O algoritmo de Levenshtein usa uma abordagem de programação dinâmica para calcular a distância de edição entre duas strings. Aqui está uma descrição do algoritmo:\n",
    "\n",
    "1. **Inicialização:**\n",
    "   - Crie uma matriz $d$ de tamanho $(m+1) \\times (n+1)$, onde $m$ é o comprimento da primeira string $s$ e $n$ é o comprimento da segunda string $t$.\n",
    "   - Inicialize $d[i][0] = i$ para $i = 0, 1, ..., m$.\n",
    "   - Inicialize $d[0][j] = j$ para $j = 0, 1, ..., n$.\n",
    "\n",
    "2. **Recorrência:**\n",
    "   - Para cada $i = 1, ..., m$ e $j = 1, ..., n$:\n",
    "     - Se $s[i-1] = t[j-1]$, então $custo = 0$; caso contrário, $custo = 1$.\n",
    "     - $d[i][j] = \\min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + custo)$.\n",
    "\n",
    "3. **Resultado:**\n",
    "   - A distância de edição é encontrada em $d[m][n]$.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Aqui está uma implementação do cálculo da distância de edição em Python:\n",
    "\n",
    "```python\n",
    "def distancia_de_edicao(s, t):\n",
    "    m = len(s)\n",
    "    n = len(t)\n",
    "    \n",
    "    # Criação da matriz\n",
    "    d = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    \n",
    "    # Inicialização da matriz\n",
    "    for i in range(m + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        d[0][j] = j\n",
    "    \n",
    "    # Preenchimento da matriz com os valores de distância\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                custo = 0\n",
    "            else:\n",
    "                custo = 1\n",
    "            d[i][j] = min(d[i - 1][j] + 1,  # Deleção\n",
    "                          d[i][j - 1] + 1,  # Inserção\n",
    "                          d[i - 1][j - 1] + custo)  # Substituição\n",
    "    \n",
    "    # Distância de edição é encontrada na última célula da matriz\n",
    "    return d[m][n]\n",
    "\n",
    "# Exemplos\n",
    "s1 = \"kitten\"\n",
    "s2 = \"sitting\"\n",
    "print(f\"Distância de edição entre '{s1}' e '{s2}': {distancia_de_edicao(s1, s2)}\")\n",
    "\n",
    "s3 = \"flaw\"\n",
    "s4 = \"lawn\"\n",
    "print(f\"Distância de edição entre '{s3}' e '{s4}': {distancia_de_edicao(s3, s4)}\")\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Strings:** \"kitten\" e \"sitting\"\n",
    "   - **Cálculo:** Transformar \"kitten\" em \"sitting\" envolve as operações:\n",
    "     - Substituir 'k' por 's': \"sitten\"\n",
    "     - Substituir 'e' por 'i': \"sittin\"\n",
    "     - Inserir 'g' no final: \"sitting\"\n",
    "   - **Distância de Edição:** 3\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Strings:** \"flaw\" e \"lawn\"\n",
    "   - **Cálculo:** Transformar \"flaw\" em \"lawn\" envolve as operações:\n",
    "     - Substituir 'f' por 'l': \"law\"\n",
    "     - Inserir 'n' no final: \"lawn\"\n",
    "   - **Distância de Edição:** 2\n",
    "\n",
    "#### Visualização da Matriz\n",
    "\n",
    "Para ilustrar como a matriz $d$ é preenchida, vejamos o exemplo das strings \"kitten\" e \"sitting\":\n",
    "\n",
    "```\n",
    "     '' s i t t i n g\n",
    "  '' 0  1 2 3 4 5 6 7\n",
    "  k  1  1 2 3 4 5 6 7\n",
    "  i  2  2 1 2 3 4 5 6\n",
    "  t  3  3 2 1 2 3 4 5\n",
    "  t  4  4 3 2 1 2 3 4\n",
    "  e  5  5 4 3 2 2 3 4\n",
    "  n  6  6 5 4 3 3 2 3\n",
    "```\n",
    "\n",
    "A distância de edição é uma métrica poderosa para comparar a similaridade entre duas strings, sendo amplamente utilizada em tarefas de NLP, como correção ortográfica e reconhecimento de padrões. O algoritmo de Levenshtein é eficiente e fácil de implementar, fornecendo uma base sólida para muitas aplicações de processamento de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf68bc7",
   "metadata": {},
   "source": [
    "[Vídeo](https://www.youtube.com/watch?v=kyeyjPlKzJM) com explicação intuitiva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5eb862",
   "metadata": {},
   "source": [
    "### Métodos para Filtrar Palavras Candidatas\n",
    "\n",
    "Filtrar palavras candidatas é um passo crucial após identificar palavras erradas para fornecer sugestões de correção. Este processo envolve gerar uma lista de possíveis correções para uma palavra mal escrita e, em seguida, classificar ou selecionar as melhores opções com base em vários critérios. Aqui estão os principais métodos para filtrar palavras candidatas, com explicações e exemplos.\n",
    "\n",
    "\n",
    "1. **Distância de Edição:**\n",
    "   - **Descrição:** Calcular a distância de edição entre a palavra errada e cada palavra do dicionário. As palavras com a menor distância de edição são consideradas as melhores candidatas.\n",
    "   - **Exemplo:** Para a palavra \"speling\", palavras como \"spelling\" (distância 1) e \"spieling\" (distância 2) seriam consideradas, com \"spelling\" sendo a melhor candidata devido à menor distância de edição.\n",
    "\n",
    "2. **N-gramas e Modelos de Linguagem:**\n",
    "   - **Descrição:** Utilizar modelos de N-gramas ou modelos de linguagem para avaliar a probabilidade das palavras candidatas no contexto da frase.\n",
    "   - **Exemplo:** Em \"I have a blak cat\", o modelo pode sugerir \"black\" como correção para \"blak\" porque \"a black cat\" é uma sequência mais comum e provável do que \"a blak cat\".\n",
    "\n",
    "3. **Frequência no Corpus:**\n",
    "   - **Descrição:** As palavras candidatas são classificadas com base na sua frequência em um corpus de texto. Palavras mais comuns são mais prováveis de serem correções corretas.\n",
    "   - **Exemplo:** Se \"hte\" foi digitado incorretamente e \"the\" é muito mais frequente no corpus do que outras combinações possíveis, \"the\" será a principal sugestão.\n",
    "\n",
    "4. **Contexto Semântico:**\n",
    "   - **Descrição:** Analisar o contexto semântico utilizando embeddings de palavras ou modelos contextuais (como BERT) para selecionar palavras que façam sentido no contexto da frase.\n",
    "   - **Exemplo:** Em \"I visited the captial city\", o modelo pode sugerir \"capital\" em vez de \"captial\" porque \"visited the capital city\" faz sentido semanticamente.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Vamos ver como esses métodos podem ser combinados para filtrar palavras candidatas. Para simplificação, usaremos apenas a distância de edição e a frequência no corpus neste exemplo.\n",
    "\n",
    "1. Calcular a distância de edição para cada palavra do dicionário.\n",
    "2. Classificar palavras por distância de edição.\n",
    "3. Utilizar frequência no corpus para ordenar palavras com a mesma distância.\n",
    "\n",
    "```python\n",
    "def distancia_de_edicao(s, t):\n",
    "    m = len(s)  # Comprimento da primeira string\n",
    "    n = len(t)  # Comprimento da segunda string\n",
    "\n",
    "    # Inicializa uma matriz (m+1) x (n+1) com zeros\n",
    "    d = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "\n",
    "    # Inicializa a primeira coluna da matriz\n",
    "    for i in range(m + 1):\n",
    "        d[i][0] = i\n",
    "\n",
    "    # Inicializa a primeira linha da matriz\n",
    "    for j in range(n + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    # Preenche a matriz com os valores de distância de edição\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                custo = 0  # Nenhum custo se os caracteres são iguais\n",
    "            else:\n",
    "                custo = 1  # Custo de substituição se os caracteres são diferentes\n",
    "            d[i][j] = min(d[i - 1][j] + 1,    # Custo de deleção\n",
    "                          d[i][j - 1] + 1,    # Custo de inserção\n",
    "                          d[i - 1][j - 1] + custo)  # Custo de substituição\n",
    "\n",
    "    return d[m][n]  # Retorna a distância de edição entre as duas strings\n",
    "\n",
    "def filtrar_candidatos(palavra_errada, dicionario, frequencia_corpus):\n",
    "    candidatos = []\n",
    "    # Calcula a distância de edição para cada palavra do dicionário\n",
    "    for palavra in dicionario:\n",
    "        dist = distancia_de_edicao(palavra_errada, palavra)\n",
    "        candidatos.append((palavra, dist))  # Adiciona a palavra e sua distância à lista de candidatos\n",
    "\n",
    "    # Ordena os candidatos primeiro pela distância de edição e depois pela frequência no corpus (em ordem decrescente)\n",
    "    candidatos.sort(key=lambda x: (x[1], -frequencia_corpus.get(x[0], 0)))\n",
    "\n",
    "    # Retorna apenas as palavras candidatas, sem as distâncias\n",
    "    return [candidato[0] for candidato in candidatos]\n",
    "\n",
    "# Exemplo de dicionário e frequências\n",
    "dicionario = [\"spelling\", \"spieling\", \"selling\", \"smiling\"]\n",
    "frequencia_corpus = {\"spelling\": 500, \"spieling\": 5, \"selling\": 300, \"smiling\": 150}\n",
    "\n",
    "# Palavra errada\n",
    "palavra_errada = \"speling\"\n",
    "\n",
    "# Filtrar palavras candidatas\n",
    "candidatos = filtrar_candidatos(palavra_errada, dicionario, frequencia_corpus)\n",
    "print(f\"Candidatos para '{palavra_errada}': {candidatos}\")\n",
    "\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Palavra Errada:** \"speling\"\n",
    "   - **Dicionário:** [\"spelling\", \"spieling\", \"selling\", \"smiling\"]\n",
    "   - **Frequência no Corpus:** {\"spelling\": 500, \"spieling\": 5, \"selling\": 300, \"smiling\": 150}\n",
    "   - **Candidatos:** [\"spelling\", \"selling\", \"smiling\", \"spieling\"]\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Palavra Errada:** \"recieve\"\n",
    "   - **Dicionário:** [\"receive\", \"recipe\", \"recite\"]\n",
    "   - **Frequência no Corpus:** {\"receive\": 1000, \"recipe\": 400, \"recite\": 50}\n",
    "   - **Candidatos:** [\"receive\", \"recipe\", \"recite\"]\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Balanceamento entre Precisão e Performance:** Calcular a distância de edição para um grande dicionário pode ser computacionalmente caro. Técnicas como truncagem de dicionário ou uso de índices podem melhorar a performance.\n",
    "- **Combinação de Critérios:** Usar múltiplos critérios (distância de edição, frequência no corpus, contexto semântico) pode aumentar a precisão das sugestões.\n",
    "- **Personalização:** Ajustar a frequência do corpus com base no domínio específico (por exemplo, termos médicos para um dicionário médico) pode melhorar os resultados.\n",
    "\n",
    "\n",
    "Filtrar palavras candidatas é um processo multifacetado que combina técnicas de comparação de strings, análise de frequência e compreensão contextual. Implementar uma abordagem robusta pode significativamente melhorar a precisão das sugestões de correção em sistemas de autocorreção e outras aplicações de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f1a108",
   "metadata": {},
   "source": [
    "### Métodos para Calcular as Probabilidades das Palavras\n",
    "\n",
    "Calcular as probabilidades das palavras é um passo importante para melhorar a precisão de sugestões em tarefas como correção ortográfica. Esse processo envolve a utilização de modelos de linguagem que atribuem uma probabilidade a cada palavra candidata com base em diferentes critérios, como frequência no corpus, contexto e modelos de n-gramas.\n",
    "\n",
    "\n",
    "1. **Frequência no Corpus:**\n",
    "   - **Descrição:** As palavras mais frequentes em um grande corpus de texto são consideradas mais prováveis.\n",
    "   - **Exemplo:** Se a palavra \"the\" aparece 5000 vezes em um corpus, sua probabilidade é maior do que a de uma palavra que aparece apenas 5 vezes.\n",
    "\n",
    "<img src=\"./imgs/calculating_word_prob.png\">\n",
    "\n",
    "2. **Modelos de N-gramas:**\n",
    "   - **Descrição:** Usar a frequência de sequências de palavras (n-gramas) para calcular a probabilidade de uma palavra em um dado contexto.\n",
    "   - **Exemplo:** Em um modelo de bigrama, a probabilidade de \"cat\" seguir \"the\" pode ser calculada como $(P(cat|the) = \\frac{C(the\\ cat)}{C(the)} $, onde $C$ denota contagens no corpus.\n",
    "\n",
    "\n",
    "3. **Modelos de Linguagem Baseados em Redes Neurais:**\n",
    "   - **Descrição:** Modelos como Word2Vec, GloVe, BERT, ou GPT podem gerar probabilidades para palavras com base em embeddings de palavras e contexto.\n",
    "   - **Exemplo:** Dado o contexto \"I have a bl__ cat\", um modelo de linguagem pode calcular a probabilidade de várias palavras preencherem o espaço em branco, como \"black\", \"blue\", etc.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Vamos ver como podemos implementar o cálculo de probabilidades de palavras usando frequências no corpus e um simples modelo de bigrama. Para isso precisamos seguir os seguintes passos:\n",
    "\n",
    "1. **Frequência no Corpus:** Calcular a probabilidade baseada na frequência relativa da palavra no corpus.\n",
    "2. **Modelo de Bigramas:** Calcular a probabilidade de uma palavra dado seu contexto anterior usando bigramas.\n",
    "\n",
    "\n",
    "```python\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Corpus de exemplo\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat chased the dog\",\n",
    "    \"the dog barked at the cat\"\n",
    "]\n",
    "\n",
    "# Função para calcular frequências unigrama e bigrama\n",
    "def calcular_frequencias(corpus):\n",
    "    unigramas = Counter()\n",
    "    bigramas = defaultdict(Counter)\n",
    "    \n",
    "    for frase in corpus:\n",
    "        palavras = frase.split()\n",
    "        for i in range(len(palavras)):\n",
    "            unigramas[palavras[i]] += 1\n",
    "            if i > 0:\n",
    "                bigramas[palavras[i-1]][palavras[i]] += 1\n",
    "    \n",
    "    total_palavras = sum(unigramas.values())\n",
    "    return unigramas, bigramas, total_palavras\n",
    "\n",
    "# Calcular frequências no corpus\n",
    "unigramas, bigramas, total_palavras = calcular_frequencias(corpus)\n",
    "\n",
    "# Função para calcular probabilidade unigrama\n",
    "def probabilidade_unigrama(palavra):\n",
    "    return unigramas[palavra] / total_palavras\n",
    "\n",
    "# Função para calcular probabilidade bigrama\n",
    "def probabilidade_bigrama(palavra_anterior, palavra):\n",
    "    if palavra_anterior in bigramas and palavra in bigramas[palavra_anterior]:\n",
    "        return bigramas[palavra_anterior][palavra] / unigramas[palavra_anterior]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Exemplo de cálculo de probabilidades\n",
    "palavra = \"cat\"\n",
    "palavra_anterior = \"the\"\n",
    "\n",
    "print(f\"Probabilidade unigrama de '{palavra}': {probabilidade_unigrama(palavra)}\")\n",
    "print(f\"Probabilidade bigrama de '{palavra}' dado '{palavra_anterior}': {probabilidade_bigrama(palavra_anterior, palavra)}\")\n",
    "\n",
    "output:\n",
    "Probabilidade unigrama de 'cat': 0.13043478260869565\n",
    "Probabilidade bigrama de 'cat' dado 'the': 0.375\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Palavra:** \"cat\"\n",
    "   - **Probabilidade Unigrama:** Calculada como a frequência da palavra \"cat\" dividida pelo total de palavras no corpus.\n",
    "   - **Probabilidade Bigrama:** Calculada como a frequência da sequência \"the cat\" dividida pela frequência de \"the\".\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Palavra:** \"dog\"\n",
    "   - **Probabilidade Unigrama:** Calculada como a frequência da palavra \"dog\" dividida pelo total de palavras no corpus.\n",
    "   - **Probabilidade Bigrama:** Calculada como a frequência da sequência \"the dog\" dividida pela frequência de \"the\".\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Balanceamento de Dados:** Um corpus pequeno ou desequilibrado pode levar a estimativas de probabilidade imprecisas. É importante utilizar um corpus representativo e suficientemente grande.\n",
    "- **Suavização:** Técnicas de suavização, como Laplace ou Good-Turing, são usadas para lidar com bigramas ou n-gramas que não aparecem no corpus.\n",
    "- **Modelos Avançados:** Modelos de linguagem baseados em redes neurais (como BERT ou GPT) podem fornecer probabilidades mais precisas ao considerar o contexto completo de uma frase.\n",
    "\n",
    "Calcular as probabilidades das palavras é um componente essencial em muitas aplicações de NLP, incluindo correção ortográfica e autocompletar. Usando frequências no corpus e modelos de n-gramas, podemos estimar essas probabilidades de forma eficaz. Em aplicações mais avançadas, modelos de linguagem baseados em redes neurais podem proporcionar uma compreensão mais profunda e precisa do contexto, resultando em sugestões de palavras ainda mais acertadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c5de7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Minimum edit distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1f4a8a",
   "metadata": {},
   "source": [
    "### Operações de Distância Mínima de Edição\n",
    "\n",
    "A distância mínima de edição (ou distância de Levenshtein) é uma métrica que mede o número mínimo de operações necessárias para transformar uma string em outra. As operações permitidas são inserção, deleção e substituição de caracteres. Vamos detalhar cada operação e como elas são usadas para calcular a distância mínima de edição, seguido de exemplos práticos.\n",
    "\n",
    "\n",
    "1. **Inserção (Insert):** Adicionar um caractere à string. Exemplo: Transformar \"cat\" em \"cats\" requer uma inserção de 's' no final. A operação é `Insert('s')`.\n",
    "\n",
    "2. **Deleção (Delete):** Remover um caractere da string. Exemplo: Transformar \"cats\" em \"cat\" requer uma deleção de 's'. A operação é `Delete('s')`.\n",
    "\n",
    "3. **Substituição (Replace):** Substituir um caractere por outro. Exemplo: Transformar \"cat\" em \"cut\" requer uma substituição de 'a' por 'u'. A operação é `Replace('a', 'u')`.\n",
    "\n",
    "Podemos atribuir pesos diferentes para cada operação, para assim, buscar otimizar a menor distancia. Por exemplo, atribuir os pesos de 1, 1 e 2 para insert, delete e replace. Para buscar a menor distancia utilizamos o algoritmod e Levenshtein.\n",
    "\n",
    "<img src=\"./imgs/edit_distance_w.png\">\n",
    "\n",
    "O algoritmo de Levenshtein usa programação dinâmica para calcular a distância mínima de edição entre duas strings. Como já citado acima, segue os seguintes passos:\n",
    "\n",
    "1. **Inicialização:**\n",
    "   - Crie uma matriz `d` de tamanho `(m+1) x (n+1)`, onde `m` é o comprimento da primeira string `s` e `n` é o comprimento da segunda string `t`.\n",
    "   - Inicialize `d[i][0] = i` para `i = 0, 1, ..., m`.\n",
    "   - Inicialize `d[0][j] = j` para `j = 0, 1, ..., n`.\n",
    "\n",
    "2. **Preenchimento da Matriz:**\n",
    "   - Para cada `i = 1, ..., m` e `j = 1, ..., n`:\n",
    "     - Se `s[i-1] == t[j-1]`, então `custo = 0`; caso contrário, `custo = 1`.\n",
    "     - `d[i][j] = min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + custo)`.\n",
    "\n",
    "3. **Resultado:**\n",
    "   - A distância de edição é encontrada em `d[m][n]`.\n",
    "\n",
    "(Ver #2.3.2.1 a implementação e exemplos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc60e86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Minimum edit distance algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10936f00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T18:46:18.739139Z",
     "start_time": "2024-06-18T18:46:18.722360Z"
    }
   },
   "source": [
    "Quando estamos computando a distancia mínima de edição, começamos com a palavra fonte e temos que transformá-la na palavra alvo. \n",
    "\n",
    "<img src=\"./imgs/minimum_edit_target.png\">\n",
    "     \n",
    "Partindo de # para #, teremos um custo de 0. Partindo de p para #, teremos um custo de 1, porque faremos uma deleção. De p para s, teremos um custo de 2, porque esse é o custo mínimo que se poderia ter para ir de p a s. Podemos continuar assim preenchendo um elemento por vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ea72df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T18:53:33.139135Z",
     "start_time": "2024-06-18T18:53:33.133892Z"
    }
   },
   "source": [
    "Para preencher a seguinte tabela:\n",
    "\n",
    "<img src=\"./imgs/minimum_edit1.png\">\n",
    "\n",
    "\n",
    "Existem três equações:\n",
    "\n",
    "- $D[i,j] = D[i-1, j] + \\text{del_cost}$: indica que você deseja preencher a célula atual (i,j) usando o custo na célula encontrada diretamente acima.\n",
    "\n",
    "- $D[i,j] = D[i, j-1] + \\text{ins_cost}$: indica que você deseja preencher a célula atual (i,j) usando o custo na célula localizada diretamente à sua esquerda.\n",
    "\n",
    "- $D[i,j] = D[i-1, j-1] + \\text{rep_cost}$: o custo rep pode ser 2 ou 0 dependendo se você vai realmente substituí-lo ou não.\n",
    "\n",
    "A cada passo você verifica os três caminhos possíveis de onde pode vir e seleciona o menos caro. Quando terminar, você obterá o seguinte:\n",
    "\n",
    "<img src=\"./imgs/minimum_edit2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35453848",
   "metadata": {},
   "source": [
    "# Week 2 - Part of Speech Tagging and Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b91b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Part of Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d45dfd",
   "metadata": {},
   "source": [
    "**Part of Speech Tagging** (ou marcação de partes do discurso) é o processo de atribuir uma etiqueta a cada palavra de um texto, indicando a sua função gramatical. As etiquetas típicas incluem categorias como substantivo, verbo, adjetivo, advérbio, pronome, preposição, conjunção, entre outros.\n",
    "\n",
    "O processo de POS Tagging envolve várias etapas:\n",
    "\n",
    "1. **Tokenização:** Dividir o texto em unidades menores, chamadas tokens (geralmente palavras).\n",
    "2. **Análise Contextual:** Avaliar o contexto em que cada palavra aparece para determinar a sua função gramatical.\n",
    "3. **Aplicação de Regras e Modelos Estatísticos:** Utilizar regras linguísticas e/ou modelos treinados em grandes corpora de textos anotados para prever a etiqueta correta.\n",
    "\n",
    "**Exemplos de POS Tagging**\n",
    "\n",
    "Considere a frase: \n",
    "- ``\"The quick brown fox jumps over the lazy dog.\"``\n",
    "\n",
    "Após a tokenização e POS Tagging, a frase pode ser anotada da seguinte forma:\n",
    "- ``The/DT quick/JJ brown/JJ fox/NN jumps/VBZ over/IN the/DT lazy/JJ dog/NN``\n",
    "\n",
    "Onde:\n",
    "- DT: Determinante\n",
    "- JJ: Adjetivo\n",
    "- NN: Substantivo\n",
    "- VBZ: Verbo, 3ª pessoa do singular presente\n",
    "- IN: Preposição\n",
    "\n",
    "POS tagging possui diversas aplicações, como:\n",
    "\n",
    "### 1. Named Entity Recognition (NER)\n",
    "\n",
    "**Named Entity Recognition** é a tarefa de identificar e classificar entidades mencionadas no texto em categorias predefinidas, como nomes de pessoas, organizações, localizações, datas, etc. O POS Tagging é uma etapa importante no NER, pois ajuda a identificar as entidades com base na função gramatical das palavras.\n",
    "\n",
    "**Exemplo:**\n",
    "- Texto: \"Barack Obama was born in Hawaii.\"\n",
    "- NER Resultado: [Barack Obama/PERSON, Hawaii/LOCATION]\n",
    "\n",
    "### 2. Coreference Resolution\n",
    "\n",
    "**Coreference Resolution** refere-se à tarefa de identificar quando diferentes expressões no texto referem-se à mesma entidade. O POS Tagging é essencial aqui para identificar pronomes e outras referências que podem apontar para entidades mencionadas anteriormente no texto.\n",
    "\n",
    "**Exemplo:**\n",
    "- Texto: \"John said he would come. He is very reliable.\"\n",
    "- Coreference Resultado: [John -> he]\n",
    "\n",
    "### 3. Speech Recognition\n",
    "\n",
    "**Speech Recognition** é o processo de converter áudio em texto. O POS Tagging pode ser usado para melhorar a precisão do reconhecimento, fornecendo pistas contextuais sobre a estrutura gramatical esperada das frases.\n",
    "\n",
    "**Exemplo:**\n",
    "- Áudio: \"I can see the sea.\"\n",
    "- Reconhecimento com POS: \"I/PRP can/MD see/VB the/DT sea/NN.\"\n",
    "\n",
    "### Como Implementar POS Tagging\n",
    "\n",
    "Existem várias bibliotecas e ferramentas para realizar POS Tagging. Um exemplo popular é a biblioteca `nltk` (Natural Language Toolkit) em Python.\n",
    "\n",
    "**Exemplo com `nltk`:**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Texto a ser analisado\n",
    "texto = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenização\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "\n",
    "# POS Tagging\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Exibição dos resultados\n",
    "print(tags)\n",
    "```\n",
    "\n",
    "**Saída esperada:**\n",
    "```\n",
    "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
    "```\n",
    "\n",
    "O POS Tagging é uma ferramenta fundamental em NLP, fornecendo informações gramaticais cruciais que são usadas em várias aplicações avançadas, como NER, Coreference Resolution e Speech Recognition. Ele permite uma análise mais profunda e precisa do texto, facilitando a construção de sistemas que compreendem e processam a linguagem natural de forma mais eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f415bdf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f3cc89",
   "metadata": {},
   "source": [
    "Markov Chains é um tipo de modelo estocástico que descreve a sequência de possíveis eventos. Nele, a probabilidade do evento seguinte, depende apenas do estado do evento anterior. Um estado é a representação da condição no momento presente, e, esse modelo pode ser representado como a estrutura de dados DAG/grafo. Nessa DAG/grafo, cada círculo representa o estado do modelo e as flechas representam a direção. Cada caminho de cada estado possui uma probabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d95845",
   "metadata": {},
   "source": [
    "**Cadeias de Markov** são modelos probabilísticos que descrevem uma sequência de possíveis eventos, onde a probabilidade de cada evento depende apenas do estado atual e não dos eventos anteriores **(propriedade de Markov)**. Em outras palavras, uma cadeia de Markov é um sistema que transita de um estado para outro com base em probabilidades fixas. Esses modelos possuem os seguintes conceitos básicos:\n",
    "\n",
    "1. **Estados:** Conjunto de todos os possíveis estados do sistema.\n",
    "2. **Transições:** Mudanças de um estado para outro.\n",
    "3. **Probabilidades de Transição:** Probabilidades associadas a cada transição entre estados.\n",
    "\n",
    "\n",
    "Imagine um clima que pode ser \"ensolarado\", \"nublado\" ou \"chuvoso\". As probabilidades de transição entre esses estados podem ser representadas por uma matriz de transição:\n",
    "\n",
    "|        | Ensolarado | Nublado | Chuvoso |\n",
    "|--------|-------------|---------|---------|\n",
    "| **Ensolarado** | 0.8         | 0.15    | 0.05    |\n",
    "| **Nublado**    | 0.2         | 0.6     | 0.2     |\n",
    "| **Chuvoso**    | 0.25        | 0.25    | 0.5     |\n",
    "\n",
    "Se hoje está ensolarado, a probabilidade de estar ensolarado novamente amanhã é 0.8, nublado 0.15 e chuvoso 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a95829",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Markov Chains and POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d8bcf",
   "metadata": {},
   "source": [
    "No contexto de **Part of Speech Tagging**, as cadeias de Markov são usadas para modelar as sequências de etiquetas gramaticais (tags). A ideia é que a etiqueta de uma palavra depende da etiqueta da palavra anterior.\n",
    "\n",
    "<img src=\"./imgs/markov_chain_words.png\">\n",
    "<img src=\"./imgs/markov_chain_dag.png\">\n",
    "\n",
    "<img src=\"./imgs/markov_chain_words_dag.png\">\n",
    "<img src=\"./imgs/markov_chain_transition_matrix.png\">\n",
    "\n",
    "Na matriz de transição acima, temos 40% de probabilidade da frase iniciar com NN (noun = substantivo), 10% de começar com um verbo e 50% de iniciar com outro POS.\n",
    "\n",
    "Para encontrar a sequência mais provável de etiquetas em POS Tagging, comummente é utilizado o Algoritmo de Viterbi, usando um modelo de **Cadeia de Markov Oculta (HMM)**.\n",
    "\n",
    "**Passos do Algoritmo:**\n",
    "\n",
    "1. **Inicialização:** Definir as probabilidades iniciais para o primeiro estado (etiqueta da primeira palavra).\n",
    "2. **Recursão:** Para cada palavra subsequente, calcular a probabilidade de cada etiqueta baseada nas etiquetas anteriores e na probabilidade de transição.\n",
    "3. **Terminação:** Selecionar a sequência de etiquetas com a maior probabilidade.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Considere uma frase \"The cat sleeps\".\n",
    "\n",
    "1. **Probabilidades de Emissão:** Probabilidade de uma palavra ser uma certa etiqueta (e.g., P(\"The\"|DT) = 0.9).\n",
    "2. **Probabilidades de Transição:** Probabilidade de uma etiqueta seguir outra (e.g., P(DT -> NN) = 0.8).\n",
    "\n",
    "Aplicando o algoritmo de Viterbi, podemos calcular a sequência mais provável de etiquetas: \"The/DT cat/NN sleeps/VBZ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b86e67",
   "metadata": {},
   "source": [
    "### Código Exemplo de POS Tagging com HMM\n",
    "\n",
    "Aqui está um exemplo simplificado de como um HMM pode ser usado para POS tagging em Python:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "# Treinamento de dados de exemplo\n",
    "train_data = [\n",
    "    [('The', 'DT'), ('cat', 'NN'), ('sleeps', 'VBZ')],\n",
    "    [('A', 'DT'), ('dog', 'NN'), ('barks', 'VBZ')],\n",
    "]\n",
    "\n",
    "# Criação e treino do modelo HMM\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)\n",
    "\n",
    "# Teste com uma nova sentença\n",
    "test_sentence = ['The', 'dog', 'sleeps']\n",
    "tagged_sentence = hmm_tagger.tag(test_sentence)\n",
    "print(tagged_sentence)\n",
    "```\n",
    "\n",
    "**Saída esperada:**\n",
    "```\n",
    "[('The', 'DT'), ('dog', 'NN'), ('sleeps', 'VBZ')]\n",
    "```\n",
    "\n",
    "Este exemplo treina um HMM simples para POS tagging e usa o modelo para etiquetar uma nova sentença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e5867ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T19:04:49.750703Z",
     "start_time": "2024-06-19T19:04:48.064581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('dog', 'NN'), ('sleeps', 'VBZ')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joaoag/.local/lib/python3.10/site-packages/nltk/tag/hmm.py:334: RuntimeWarning: overflow encountered in cast\n",
      "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
      "/home/joaoag/.local/lib/python3.10/site-packages/nltk/tag/hmm.py:336: RuntimeWarning: overflow encountered in cast\n",
      "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
      "/home/joaoag/.local/lib/python3.10/site-packages/nltk/tag/hmm.py:332: RuntimeWarning: overflow encountered in cast\n",
      "  P[i] = self._priors.logprob(si)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "# Treinamento de dados de exemplo\n",
    "train_data = [\n",
    "    [('The', 'DT'), ('cat', 'NN'), ('sleeps', 'VBZ')],\n",
    "    [('A', 'DT'), ('dog', 'NN'), ('barks', 'VBZ')],\n",
    "]\n",
    "\n",
    "# Criação e treino do modelo HMM\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)\n",
    "\n",
    "# Teste com uma nova sentença\n",
    "test_sentence = ['The', 'dog', 'sleeps']\n",
    "tagged_sentence = hmm_tagger.tag(test_sentence)\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22ae204",
   "metadata": {},
   "source": [
    "### Markov Chains in Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc51630",
   "metadata": {},
   "source": [
    "No **Reconhecimento de Fala**, as cadeias de Markov são usadas para modelar a sequência de fonemas ou palavras. Um exemplo comum é o uso de **Modelos de Markov Ocultos (HMMs)** para representar a relação entre o áudio (observações) e as sequências de palavras ou fonemas (estados ocultos).\n",
    "\n",
    "**Passos no Reconhecimento de Fala:**\n",
    "\n",
    "1. **Treinamento:** Usar dados de áudio etiquetados para treinar o modelo HMM, ajustando as probabilidades de transição entre fonemas e as probabilidades de emissão de observações (características do áudio).\n",
    "2. **Decodificação:** Dado um novo trecho de áudio, usar o algoritmo de Viterbi para encontrar a sequência mais provável de fonemas ou palavras que correspondem ao áudio.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Para a frase falada \"Hello world\":\n",
    "\n",
    "1. **Probabilidades de Emissão:** Probabilidades de um segmento de áudio ser emitido por um determinado fonema.\n",
    "2. **Probabilidades de Transição:** Probabilidades de um fonema seguir outro fonema.\n",
    "\n",
    "Usando HMMs, o sistema pode decodificar o áudio para determinar a sequência de palavras \"Hello world\".\n",
    "\n",
    "As Cadeias de Markov, particularmente os Modelos de Markov Ocultos, são ferramentas poderosas em NLP e reconhecimento de fala, permitindo modelar dependências sequenciais em dados. Em POS Tagging, elas ajudam a prever a sequência de etiquetas gramaticais baseadas no contexto, enquanto no reconhecimento de fala, elas facilitam a interpretação de sequências de áudio em texto significativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c95806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26af1a88",
   "metadata": {},
   "source": [
    "**Hidden Markov Model** (HMM) é uma extensão das Cadeias de Markov, onde os estados não são diretamente observáveis (ocultos). Em vez disso, observamos sinais ou evidências que são probabilisticamente dependentes dos estados ocultos. HMM é amplamente utilizado em tarefas como reconhecimento de fala, POS tagging, entre outras. Conceitos Básicos do HMM:\n",
    "\n",
    "1. **Estados Ocultos:** Conjunto de estados que não são diretamente observáveis.\n",
    "2. **Observações:** Conjunto de observações que são visíveis e dependem dos estados ocultos.\n",
    "3. **Probabilidades de Transição:** Probabilidades de transitar de um estado oculto para outro.\n",
    "4. **Probabilidades de Emissão:** Probabilidades de um estado oculto gerar uma determinada observação.\n",
    "5. **Probabilidades Iniciais:** Probabilidades de começar em cada estado oculto.\n",
    "\n",
    "Um HMM é definido por:\n",
    "- Um conjunto de estados ocultos $ S = \\{s_1, s_2, \\ldots, s_N\\} $ ou $ Q = \\{q_1, q_2, \\ldots, s_N\\} $.\n",
    "- Um conjunto de observações $ O = \\{o_1, o_2, \\ldots, o_M\\} $.\n",
    "- Uma matriz de transição $ A $, onde $ A_{ij} $ é a probabilidade de transitar de $ s_i $ para $ s_j $.\n",
    "- Uma matriz de emissão $ B $, onde $ B_{jk} $ é a probabilidade de observar $ o_k $ dado o estado $ s_j $.\n",
    "- Um vetor de probabilidades iniciais $ \\pi $, onde $ \\pi_i $ é a probabilidade inicial de estar no estado $ s_i $.\n",
    "\n",
    "As **probabilidades de transição** permitiram identificar a probabilidade de transição de um POS para outro. Já nos **modelos de Markov ocultos**, usamos **probabilidades de emissão** que dão a probabilidade de passar de um estado (tag POS) **para uma palavra específica**.\n",
    "\n",
    "<img src=\"./imgs/hmm_states.png\">\n",
    "\n",
    "\n",
    "Por exemplo, dado que você está em um estado verb, podemos passar para outras palavras com certas probabilidades. Esta matriz de emissão B será usada com sua matriz de transição A, para ajudar a identificar a classe gramatical de uma palavra em uma frase. Para preencher sua matriz B, podemos simplesmente ter um conjunto de dados rotulado e calcular as probabilidades de ir de um POS para cada palavra do seu vocabulário. \n",
    "\n",
    "<img src=\"./imgs/hmm_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926682fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:32:03.075918Z",
     "start_time": "2024-06-19T20:32:03.067065Z"
    }
   },
   "source": [
    "### Aplicações do HMM\n",
    "\n",
    "No **POS Tagging**, as palavras em uma sentença são as observações e as etiquetas gramaticais (partes do discurso) são os estados ocultos.\n",
    "\n",
    "Considere a frase \"The cat sleeps\".\n",
    "\n",
    "- Observações: [\"The\", \"cat\", \"sleeps\"]\n",
    "- Estados Ocultos: [\"DT\", \"NN\", \"VBZ\"]\n",
    "\n",
    "**Passos:**\n",
    "\n",
    "1. **Inicialização:** Definir probabilidades iniciais para cada etiqueta gramatical (estado oculto).\n",
    "2. **Transição:** Definir probabilidades de transição entre etiquetas gramaticais.\n",
    "3. **Emissão:** Definir probabilidades de emissão das palavras dado cada etiqueta gramatical.\n",
    "\n",
    "No **reconhecimento de fala**, os sinais de áudio (características extraídas do áudio) são as observações e as sequências de palavras ou fonemas são os estados ocultos.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Considere a sequência de áudio correspondente a \"Hello world\".\n",
    "\n",
    "- Observações: [características do áudio]\n",
    "- Estados Ocultos: [\"H\", \"e\", \"l\", \"l\", \"o\", \" \", \"w\", \"o\", \"r\", \"l\", \"d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d0c67",
   "metadata": {},
   "source": [
    "### Exemplo Prático em Python\n",
    "\n",
    "Vamos implementar um exemplo simples de HMM para POS tagging usando a biblioteca `hmmlearn`.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Definir os estados (etiquetas gramaticais)\n",
    "states = [\"DT\", \"NN\", \"VBZ\"]\n",
    "n_states = len(states)\n",
    "\n",
    "# Definir as observações (palavras)\n",
    "observations = [\"The\", \"cat\", \"sleeps\"]\n",
    "n_observations = len(observations)\n",
    "\n",
    "# Definir as sequências observadas em números\n",
    "obs_map = {word: idx for idx, word in enumerate(observations)}\n",
    "obs_seq = [obs_map[word] for word in observations]\n",
    "\n",
    "# Inicializar o modelo HMM\n",
    "model = hmm.MultinomialHMM(n_components=n_states, n_iter=100)\n",
    "\n",
    "# Probabilidades iniciais\n",
    "model.startprob_ = np.array([0.6, 0.3, 0.1])\n",
    "\n",
    "# Probabilidades de transição\n",
    "model.transmat_ = np.array([\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.3, 0.4, 0.3],\n",
    "    [0.5, 0.2, 0.3]\n",
    "])\n",
    "\n",
    "# Probabilidades de emissão\n",
    "model.emissionprob_ = np.array([\n",
    "    [0.6, 0.3, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "# Ajustar o modelo com a sequência de observação\n",
    "obs_seq = np.array(obs_seq).reshape(-1, 1)\n",
    "logprob, state_seq = model.decode(obs_seq, algorithm=\"viterbi\")\n",
    "\n",
    "# Mapeando os estados de volta para as etiquetas gramaticais\n",
    "state_map = {idx: state for idx, state in enumerate(states)}\n",
    "tagged_seq = [state_map[state] for state in state_seq]\n",
    "\n",
    "print(\"Observações:\", observations)\n",
    "print(\"Estados preditos:\", tagged_seq)\n",
    "```\n",
    "\n",
    "**Saída esperada:**\n",
    "```\n",
    "Observações: ['The', 'cat', 'sleeps']\n",
    "Estados preditos: ['DT', 'NN', 'VBZ']\n",
    "```\n",
    "\n",
    "Neste exemplo, usamos um HMM simples para etiquetar uma frase. O HMM é treinado com probabilidades de transição e emissão predefinidas e, em seguida, decodificamos a sequência de observações para encontrar a sequência mais provável de estados ocultos (etiquetas gramaticais)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4606ac",
   "metadata": {},
   "source": [
    "## Markov Chain vs Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2ec089",
   "metadata": {},
   "source": [
    "Saiba mais em:\n",
    "https://stackoverflow.com/questions/10748426/what-is-the-difference-between-markov-chains-and-hidden-markov-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936efd9",
   "metadata": {},
   "source": [
    "Embora as **Cadeias de Markov** e os **Modelos de Markov Ocultos (HMMs)** compartilhem conceitos fundamentais, como estados e probabilidades de transição, eles são aplicados em diferentes contextos e têm diferenças importantes em termos de visibilidade dos estados e aplicação prática.\n",
    "\n",
    "### Cadeia de Markov - Definição\n",
    "Uma **Cadeia de Markov** é um modelo matemático que descreve um processo onde a probabilidade de transição para o próximo estado depende apenas do estado atual, e não dos estados anteriores (propriedade de Markov).\n",
    "\n",
    "1. **Estados Visíveis:** Todos os estados são diretamente observáveis.\n",
    "2. **Probabilidades de Transição:** Matriz de transição que define a probabilidade de passar de um estado para outro.\n",
    "3. **Propriedade de Markov:** A transição para o próximo estado depende apenas do estado atual.\n",
    "\n",
    "Imagine um sistema climático simplificado com três estados: ensolarado, nublado e chuvoso. A matriz de transição pode ser:\n",
    "\n",
    "|        | Ensolarado | Nublado | Chuvoso |\n",
    "|--------|------------|---------|---------|\n",
    "| **Ensolarado** | 0.8        | 0.15    | 0.05    |\n",
    "| **Nublado**    | 0.2        | 0.6     | 0.2     |\n",
    "| **Chuvoso**    | 0.25       | 0.25    | 0.5     |\n",
    "\n",
    "Se hoje está ensolarado, a probabilidade de amanhã também estar ensolarado é 0.8, nublado é 0.15, e chuvoso é 0.05.\n",
    "\n",
    "### Modelos de Markov Ocultos (Hidden Markov Models - HMMs) -  Definição\n",
    "Um **Modelo de Markov Oculto** é uma extensão das Cadeias de Markov, onde os estados não são diretamente observáveis. Em vez disso, observamos sinais ou evidências que são probabilisticamente dependentes dos estados ocultos.\n",
    "\n",
    "1. **Estados Ocultos:** Os estados não são diretamente observáveis.\n",
    "2. **Observações Visíveis:** O que observamos são sinais ou evidências que dependem dos estados ocultos.\n",
    "3. **Probabilidades de Transição:** Matriz de transição que define a probabilidade de passar de um estado oculto para outro.\n",
    "4. **Probabilidades de Emissão:** Matriz de emissão que define a probabilidade de uma observação ser gerada a partir de um estado oculto.\n",
    "\n",
    "No contexto de reconhecimento de fala, os estados ocultos podem ser fonemas, enquanto as observações são características extraídas do sinal de áudio.\n",
    "\n",
    "### Comparação Direta\n",
    "\n",
    "| Aspecto                   | Cadeia de Markov                  | Modelo de Markov Oculto (HMM)         |\n",
    "|---------------------------|-----------------------------------|---------------------------------------|\n",
    "| **Estados**               | Visíveis                          | Ocultos                               |\n",
    "| **Observações**           | Estados                           | Dependem probabilisticamente dos estados ocultos |\n",
    "| **Aplicação**             | Modelagem de processos observáveis diretamente | Modelagem de processos onde a sequência de estados não é diretamente observável |\n",
    "| **Exemplo**               | Clima (ensolarado, nublado, chuvoso) | Reconhecimento de fala (fonemas e áudio)           |\n",
    "\n",
    "### Aplicações em NLP - Cadeias de Markov\n",
    "As Cadeias de Markov podem ser usadas em tarefas onde a sequência de estados é observável e a propriedade de Markov (dependência do estado atual) se mantém. Por exemplo:\n",
    "- **Modelo de Texto:** Gerar texto baseado em bigramas ou trigrama onde cada estado é uma palavra.\n",
    "\n",
    "#### HMMs\n",
    "Os HMMs são aplicados em tarefas onde os estados são ocultos e apenas as observações estão disponíveis. Exemplos incluem:\n",
    "- **Part of Speech Tagging (POS Tagging):** Os estados ocultos são as etiquetas gramaticais, e as observações são as palavras.\n",
    "- **Reconhecimento de Fala:** Os estados ocultos são os fonemas ou palavras, e as observações são características do áudio.\n",
    "\n",
    "### Exemplos Práticos - Cadeia de Markov - Modelo de Texto\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "# Exemplo simples de Cadeia de Markov para geração de texto\n",
    "def generate_text(model, start_word, length=10):\n",
    "    current_word = start_word\n",
    "    result = [current_word]\n",
    "    \n",
    "    for _ in range(length - 1):\n",
    "        current_word = random.choices(list(model[current_word].keys()), list(model[current_word].values()))[0]\n",
    "        result.append(current_word)\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "# Modelo de transição (simplificado)\n",
    "markov_model = {\n",
    "    \"I\": {\"am\": 1.0},\n",
    "    \"am\": {\"a\": 0.5, \"happy\": 0.5},\n",
    "    \"a\": {\"student\": 1.0},\n",
    "    \"happy\": {\"student\": 1.0},\n",
    "    \"student\": {\"I\": 1.0}\n",
    "}\n",
    "\n",
    "# Gerar texto\n",
    "print(generate_text(markov_model, \"I\"))\n",
    "```\n",
    "\n",
    "#### HMM - POS Tagging com `hmmlearn`\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Definir os estados (etiquetas gramaticais)\n",
    "states = [\"DT\", \"NN\", \"VBZ\"]\n",
    "n_states = len(states)\n",
    "\n",
    "# Definir as observações (palavras)\n",
    "observations = [\"The\", \"cat\", \"sleeps\"]\n",
    "n_observations = len(observations)\n",
    "\n",
    "# Definir as sequências observadas em números\n",
    "obs_map = {word: idx for idx, word in enumerate(observations)}\n",
    "obs_seq = [obs_map[word] for word in observations]\n",
    "\n",
    "# Inicializar o modelo HMM\n",
    "model = hmm.MultinomialHMM(n_components=n_states, n_iter=100)\n",
    "\n",
    "# Probabilidades iniciais\n",
    "model.startprob_ = np.array([0.6, 0.3, 0.1])\n",
    "\n",
    "# Probabilidades de transição\n",
    "model.transmat_ = np.array([\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.3, 0.4, 0.3],\n",
    "    [0.5, 0.2, 0.3]\n",
    "])\n",
    "\n",
    "# Probabilidades de emissão\n",
    "model.emissionprob_ = np.array([\n",
    "    [0.6, 0.3, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "# Ajustar o modelo com a sequência de observação\n",
    "obs_seq = np.array(obs_seq).reshape(-1, 1)\n",
    "logprob, state_seq = model.decode(obs_seq, algorithm=\"viterbi\")\n",
    "\n",
    "# Mapeando os estados de volta para as etiquetas gramaticais\n",
    "state_map = {idx: state for idx, state in enumerate(states)}\n",
    "tagged_seq = [state_map[state] for state in state_seq]\n",
    "\n",
    "print(\"Observações:\", observations)\n",
    "print(\"Estados preditos:\", tagged_seq)\n",
    "```\n",
    "\n",
    "Embora as Cadeias de Markov e os Modelos de Markov Ocultos compartilhem uma base teórica comum, eles diferem significativamente em suas aplicações e complexidade. As Cadeias de Markov são mais simples e usadas quando os estados são diretamente observáveis. Os HMMs são mais complexos e poderosos, permitindo modelar processos onde os estados não são diretamente observáveis, como em tarefas de NLP e reconhecimento de fala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa437b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Calculating Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bc373",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/calculating_prob1.png\">\n",
    "\n",
    "O número de vezes que o quadrado azul e roxo ocorrem juntos é 2/3. Usaremos a mesma lógica para popular as matrizes de transição e emissão. Numa matriz de transição, contaremos o número de vezes que a tag $t_{(i-1)}, t_{(i)}$ ocorrem juntos e dividiremos pelo total de vezes que $t_{(i-1)}$ ocorre (que é igual ao número de vezes que aparece seguido por qualquer outra coisa).\n",
    "\n",
    "<img src=\"./imgs/calculating_prob1.png\">\n",
    "\n",
    "$C(t_{(i-1)}, t_{(i)}$ é a quantidade de vezes que a tag $(i-1)$ ocorreu antes da $\\text{tag i}$. A partir disso podemos computar a probabilidade que a tag ocorre antes de outra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcd591f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Populating the Trasition and Emission Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba91d8a",
   "metadata": {},
   "source": [
    "Para popular a **matriz de transição**, temos que trackear a quantidade de vezes que uma tag aparece antes da outra tag.\n",
    "\n",
    "<img src=\"./imgs/transition_matrix1.png\">\n",
    "\n",
    "Na tabela acima, cada cor representa uma tag, e as linhas são os estados atuais e as colunas são so estado seguintes, sendo o laranja o estado inicial. O números na matriz correspondem a quantidade de vezes que a tag ocorre antes da outra.\n",
    "\n",
    "Para ir de O para NN, ou seja, para calcular $P(NN|O)$, temos que computar o seguinte:\n",
    "\n",
    "<img src=\"./imgs/transition_matrix2.png\">\n",
    "\n",
    "Para generalizar:\n",
    "\n",
    "$$ P(t_{i}|t_{i-1}) = \\frac{C(t_{i-1}, t_{i})}{\\sum^{N}_{j=1}C(t_{i-1}, t_{i})} $$\n",
    "\n",
    "Por vezes vai correr do estado atual e o seguinte ser zero, dando uma probabilidade de 0. Para resolver isso esse problema, podemos utilizar uma **suavização (smoothing)**, como a seguir.\n",
    "\n",
    "\n",
    "<img src=\"./imgs/transition_matrix3.png\">\n",
    "\n",
    "O $\\epsilon$ permite que não tenhamos nenhuma probabilidade zero. Mas, em aplicações reais, não aplicaríamos a suavização na primeira linha, por que, ao aplicar, iriamos per da frase começar com qualquer TAG de POS, incluindo pontuações.\n",
    "\n",
    "Para popular uma **matriz de emissão**, faremos de forma semelhante. Mas iremos quantificar as palavras associadas com suas tags POS.\n",
    "\n",
    "<img src=\"./imgs/emission_matrix1.png\">\n",
    "\n",
    "Para popular a matriz, também iremos usar a suavização, sendo:\n",
    "\n",
    "$$ P(w_{i}|t_{i}) = \\frac{C(t_{i}, w_{i}) + \\epsilon}{\\sum^{V}_{j=1} C(t_{i}, w_{j} + N * \\epsilon}$$\n",
    "\n",
    "$$ = \\frac{C(t_{i}, w_{i}) + \\epsilon}{C(t_{i}) + N * \\epsilon}$$\n",
    "\n",
    "Onde $C(t_{i}, w_{i})$ é a contagem associada a quantas vezes a tag $t_{i}$ está associada a palavra $w_{i}$. O epsilon acima é o caractere de suavização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc52ab4",
   "metadata": {},
   "source": [
    "## Calculating the Trasition and Emission Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9085820e",
   "metadata": {},
   "source": [
    "Para treinar um Hidden Markov Model (HMM) para tarefas como Part of Speech (POS) tagging, precisamos calcular as probabilidades das matrizes de transição e emissão com base em um corpus de texto anotado com etiquetas POS. Aqui estão os passos e exemplos detalhados para calcular essas probabilidades.\n",
    "\n",
    "### 1. Preparação do Corpus\n",
    "\n",
    "Primeiro, precisamos de um corpus anotado com etiquetas POS. Suponha que temos o seguinte corpus simples:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    [(\"The\", \"DT\"), (\"cat\", \"NN\"), (\"sleeps\", \"VBZ\")],\n",
    "    [(\"A\", \"DT\"), (\"dog\", \"NN\"), (\"barks\", \"VBZ\")],\n",
    "    [(\"The\", \"DT\"), (\"dog\", \"NN\"), (\"runs\", \"VBZ\")]\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. Contagem de Transições e Emissões\n",
    "\n",
    "#### a. Contagem de Transições entre Tags POS\n",
    "\n",
    "Contamos como as etiquetas de POS transicionam de uma palavra para outra. Adicionamos um estado inicial fictício (`<s>`) para representar o início de uma frase.\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "\n",
    "# Contadores de transições e emissões\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "for sentence in corpus:\n",
    "    previous_tag = \"<s>\"\n",
    "    tag_counts[previous_tag] += 1\n",
    "    for word, tag in sentence:\n",
    "        # Contagem de transições\n",
    "        transition_counts[previous_tag][tag] += 1\n",
    "        \n",
    "        # Contagem de emissões\n",
    "        emission_counts[tag][word] += 1\n",
    "        \n",
    "        # Contagem de tags\n",
    "        tag_counts[tag] += 1\n",
    "        previous_tag = tag\n",
    "    transition_counts[previous_tag][\"</s>\"] += 1\n",
    "    tag_counts[\"</s>\"] += 1\n",
    "```\n",
    "\n",
    "#### b. Contagem de Emissões (palavra dada a tag)\n",
    "\n",
    "Cada vez que uma palavra ocorre com uma determinada etiqueta POS, incrementamos a contagem de emissão correspondente.\n",
    "\n",
    "### 3. Calculando as Probabilidades\n",
    "\n",
    "#### a. Probabilidades de Transição\n",
    "\n",
    "A probabilidade de transição de um estado $ s_i $ para um estado $ s_j $ é calculada como:\n",
    "\n",
    "$$ P(s_j \\mid s_i) = \\frac{\\text{contagem}(s_i \\rightarrow s_j)}{\\text{contagem}(s_i)} $$\n",
    "\n",
    "```python\n",
    "# Probabilidades de transição\n",
    "transition_prob = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for previous_tag in transition_counts:\n",
    "    total_transitions = sum(transition_counts[previous_tag].values())\n",
    "    for tag in transition_counts[previous_tag]:\n",
    "        transition_prob[previous_tag][tag] = transition_counts[previous_tag][tag] / total_transitions\n",
    "```\n",
    "\n",
    "#### b. Probabilidades de Emissão\n",
    "\n",
    "A probabilidade de emissão de uma palavra $ w $ dado um estado $ s $ é calculada como:\n",
    "\n",
    "$$ P(w \\mid s) = \\frac{\\text{contagem}(s \\rightarrow w)}{\\text{contagem}(s)} $$\n",
    "\n",
    "```python\n",
    "# Probabilidades de emissão\n",
    "emission_prob = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for tag in emission_counts:\n",
    "    total_emissions = sum(emission_counts[tag].values())\n",
    "    for word in emission_counts[tag]:\n",
    "        emission_prob[tag][word] = emission_counts[tag][word] / total_emissions\n",
    "```\n",
    "\n",
    "### 4. Exemplos Práticos\n",
    "\n",
    "Vamos calcular as matrizes de transição e emissão usando os dados do corpus fornecido:\n",
    "\n",
    "#### Contagens\n",
    "\n",
    "Para o corpus fornecido, as contagens seriam:\n",
    "\n",
    "**Transições:**\n",
    "- $\"<s>\", \"DT\"$: 3\n",
    "- $\"DT\", \"NN\"$: 3\n",
    "- $\"NN\", \"VBZ\"$: 3\n",
    "- $\"VBZ\", \"</s>\"$: 3\n",
    "\n",
    "**Emissões:**\n",
    "- $\"DT\", \"The\"$: 2\n",
    "- $\"DT\", \"A\"$: 1\n",
    "- $\"NN\", \"cat\"$: 1\n",
    "- $\"NN\", \"dog\"$: 2\n",
    "- $\"VBZ\", \"sleeps\"$: 1\n",
    "- $\"VBZ\", \"barks\"$: 1\n",
    "- $\"VBZ\", \"runs\"$: 1\n",
    "\n",
    "#### Probabilidades de Transição\n",
    "\n",
    "```python\n",
    "# Exemplo de cálculo de probabilidades de transição\n",
    "transition_prob = {\n",
    "    \"<s>\": {\"DT\": 1.0},\n",
    "    \"DT\": {\"NN\": 1.0},\n",
    "    \"NN\": {\"VBZ\": 1.0},\n",
    "    \"VBZ\": {\"</s>\": 1.0}\n",
    "}\n",
    "```\n",
    "\n",
    "#### Probabilidades de Emissão\n",
    "\n",
    "```python\n",
    "# Exemplo de cálculo de probabilidades de emissão\n",
    "emission_prob = {\n",
    "    \"DT\": {\"The\": 2/3, \"A\": 1/3},\n",
    "    \"NN\": {\"cat\": 1/3, \"dog\": 2/3},\n",
    "    \"VBZ\": {\"sleeps\": 1/3, \"barks\": 1/3, \"runs\": 1/3}\n",
    "}\n",
    "```\n",
    "\n",
    "Este método de contagem e cálculo das probabilidades a partir de um corpus etiquetado é fundamental para treinar um HMM para tarefas de NLP, como POS tagging. Calculando as probabilidades de transição e emissão, podemos criar modelos que probabilisticamente decodificam sequências de observações em sequências de estados ocultos, fornecendo insights valiosos sobre a estrutura linguística dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6555329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## The Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95b40a",
   "metadata": {},
   "source": [
    "O Algoritmo de Viterbi é usado para encontrar a sequência mais provável de estados ocultos (etiquetas) dada uma sequência de observações (palavras ou sinais de áudio). Ele faz uso das probabilidades de transição e de emissão da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/viterbi1.png\">\n",
    "\n",
    "Para ir de $\\pi$ para $O$, precisamos multiplicar a probabilidade de transição correspondente (0.3) com a probabilidade de emissão correspondente (0.5), que resultará em 0.15. Isso é feito para todas as palavras da sequência, até termos a probabilidade da sequencia completa.\n",
    "\n",
    "<img src=\"./imgs/viterbi2.png\">\n",
    "\n",
    "O algoritmo Viterbi possiu os seguintes passos:\n",
    "\n",
    "1. **Inicialização:**\n",
    "   $$\n",
    "    V_1(i) = \\pi_i \\cdot B_i(O_1)\n",
    "   $$\n",
    "   \n",
    "    onde $ V_1(i) $ é a probabilidade do estado $ s_i $ ser o primeiro estado e $ O_1 $ é a primeira observação.\n",
    "\n",
    "2. **Recursão (Forward Pass):**\n",
    "   $$\n",
    "    V_t(j) = \\max_i (V_{t-1}(i) \\cdot A_{ij}) \\cdot B_j(O_t)\n",
    "   $$\n",
    "   \n",
    "    onde $ V_t(j) $ é a probabilidade máxima da sequência até o estado $ s_j $ no tempo $ t $.\n",
    "\n",
    "3. **Terminação (Backward Pass):**\n",
    "   $$\n",
    "    \\max_i V_T(i)\n",
    "   $$\n",
    "   \n",
    "    onde $ T $ é o tempo final.\n",
    "\n",
    "4. **Retropropagação:** Rastrear os estados que levaram à sequência de maior probabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad27a31",
   "metadata": {},
   "source": [
    "## Viterbi: Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04d8b98",
   "metadata": {},
   "source": [
    "Para inicializar utlizamos duas matrizes auxiliares, C e D, a C é uma matriz da **Probabilidade de Emissão**, enquanto que a D representa a **Probabilidade de Transição**. Começando com a C, iremos popular essa matriz de dimensão (num_tags, num_words), esta matriz terá as probabilidades que lhe dirão a que parte do discurso cada palavra pertence.\n",
    "\n",
    "<img src=\"./imgs/viterbi3.png\">\n",
    "\n",
    "Agora, para popular a primeira coluna, iremos multiplicar o estado inicial $\\pi$, para cada tag, vezes $b_{i, \\text{cindex}(w1})$. Aqui o i corresponde a tag da distriguição inicial, e o $\\text{cindex}()w1$ é o índice da palavra 1 na matriz de emissão. Agora precisamos acompanhar de qual parte do discurso estamos vindo. Para isso, utilizamos a matrix auxiliar D, que permite armazenar os rótulos que representam os diferentes estados pelos quais você está passando ao encontrar a sequência mais provável de tags POS para uma determinada sequência de palavras $w1, ..., wk$. A princípio a primeira coluna é definida como 0, pois não vem de nenhuma tag POS.\n",
    "\n",
    "<img src=\"./imgs/viterbi4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51664cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Viterbi: Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a131b8",
   "metadata": {},
   "source": [
    "A etapa forward pass pode ser representada da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/viterbi5.png\">\n",
    "\n",
    "Começando do ultimo termo:\n",
    "- $b_{1, \\text{cindex(w2)}}$: Representa simplesmente a probabilidade de emissão da tag t1 para a palavra w2.\n",
    "- $a_{k, 1}$: É a probabilidade de transição da tag POS para a tag atual t1.\n",
    "- $c_{k,1}$: que representa a probabilidade do caminho anterior percorrido.\n",
    "- Escolhemos o k que maximiza a fórmula completa. Como ele não é um estado inicial, pode ser 1, 2 ou 3\n",
    "\n",
    "Para popular a célula 1,2 da imagem acima, devemos pegar o máximo de k-ésimas células na coluna anterior, vezes a probabilidade de transição correspondente do k-ésimo POS para o primeiro POS vezes a probabilidade de emissão do primeiro POS e a palavra atual que você está vendo. Podemos faz isso para todas as células. \n",
    "\n",
    "A regra geral é \n",
    "\n",
    "$$ c_{i,j} = \\max_k c_{k,j-1} * a_{k,i} * b_{i, \\text{cindex(wj)}} $$\n",
    "\n",
    "onde:\n",
    "- **$c_{i,j}$:** A probabilidade máxima da sequência de estados (etiquetas) que termina no estado $i$ na posição $j$. Representa a melhor pontuação (probabilidade) para alcançar o estado $i$ na posição $j$ da sequência de observações.\n",
    "- **$\\max_k$:** A operação de maximização sobre todos os possíveis estados $k$ na posição anterior $j-1$. Para encontrar a melhor sequência, o algoritmo de Viterbi considera todas as possíveis transições dos estados na posição anterior e escolhe a que maximiza a probabilidade.\n",
    "- **$c_{k,j-1}$:** A probabilidade máxima da sequência de estados que termina no estado $k$ na posição $j-1$. Esta é a pontuação (probabilidade) acumulada até o estado $k$ na posição $j-1$. Ela representa o caminho mais provável até aquele ponto na sequência.\n",
    "- **$a_{k,i}$:** A probabilidade de transição do estado $k$ para o estado $i$. Esta é a probabilidade de que a sequência de estados transite do estado $k$ na posição $j-1$ para o estado $i$ na posição $j$.\n",
    "- **$b_{i, \\text{cindex}(w_j)}$:** A probabilidade de emissão do estado $i$ observando a palavra $w_j$.  Esta é a probabilidade de que o estado $i$ emita a palavra $w_j$ na posição $j$. A função $\\text{cindex}(w_j)$ mapeia a palavra $w_j$ para o índice apropriado na matriz de emissões.\n",
    "- **$\\text{cindex}(w_j)$:** Um mapeamento da palavra $w_j$ para um índice usado na matriz de emissões. Essa função converte a palavra observada $w_j$ para um índice que pode ser usado para acessar a matriz de emissões $b_{i,\\text{cindex}(w_j)}$.\n",
    "\n",
    "O objetivo é encontrar a sequência de estados que maximiza a probabilidade de observar a sequência dada. Para fazer isso, calculamos a probabilidade máxima de terminar em cada estado em cada posição da sequência, considerando todas as possíveis transições e emissões.\n",
    "\n",
    "Imagine que temos uma sequência de palavras \"o gato mia\" e queremos encontrar a sequência de etiquetas de POS mais provável usando Viterbi.\n",
    "\n",
    "- **Posição 1: \"o\"**\n",
    "  - Possíveis estados: $ DT $ (determinante)\n",
    "  - $ c_{\\text{DT}, 1} = a_{\\text{start}, \\text{DT}} \\cdot b_{\\text{DT}, \\text{cindex}(\"o\")} $\n",
    "  - Inicialmente, $ c_{\\text{DT}, 1} $ seria a probabilidade de começar com $ DT $ vezes a probabilidade de $ DT $ emitir \"o\".\n",
    "\n",
    "- **Posição 2: \"gato\"**\n",
    "  - Possíveis estados: $ NN $ (substantivo)\n",
    "  - Consideramos todas as transições possíveis de etiquetas anteriores para $ NN $:\n",
    "    - $ c_{\\text{NN}, 2} = \\max \\left( c_{\\text{DT}, 1} \\cdot a_{\\text{DT}, \\text{NN}} \\cdot b_{\\text{NN}, \\text{cindex}(\"gato\")} \\right) $\n",
    "  - Para calcular $ c_{\\text{NN}, 2} $, verificamos a probabilidade acumulada de estar em $ DT $ na posição 1 ($ c_{\\text{DT}, 1} $), a probabilidade de transição de $ DT $ para $ NN $ ($ a_{\\text{DT}, \\text{NN}} $) e a probabilidade de $ NN $ emitir \"gato\" ($ b_{\\text{NN}, \\text{cindex}(\"gato\")} $).\n",
    "\n",
    "- **Posição 3: \"mia\"**\n",
    "  - Possíveis estados: $ VBZ $ (verbo presente)\n",
    "  - Consideramos todas as transições possíveis de etiquetas anteriores para $ VBZ $:\n",
    "    - $ c_{\\text{VBZ}, 3} = \\max \\left( c_{\\text{NN}, 2} \\cdot a_{\\text{NN}, \\text{VBZ}} \\cdot b_{\\text{VBZ}, \\text{cindex}(\"mia\")} \\right) $\n",
    "  - Para calcular $ c_{\\text{VBZ}, 3} $, verificamos a probabilidade acumulada de estar em $ NN $ na posição 2 ($ c_{\\text{NN}, 2} $), a probabilidade de transição de $ NN $ para $ VBZ $ ($ a_{\\text{NN}, \\text{VBZ}} $) e a probabilidade de $ VBZ $ emitir \"mia\" ($ b_{\\text{VBZ}, \\text{cindex}(\"mia\")} $).\n",
    "\n",
    "A fórmula $ c_{i,j} = \\max_k \\left( c_{k,j-1} \\cdot a_{k,i} \\cdot b_{i, \\text{cindex}(w_j)} \\right) $ captura a ideia de encontrar a sequência de estados mais provável, acumulando as melhores pontuações (probabilidades) de transições e emissões em cada passo. Em cada posição $j$, ela considera todas as possíveis transições dos estados na posição anterior $j-1$, escolhe a melhor e multiplica pelas probabilidades relevantes para obter a maior pontuação possível para estar no estado $i$ na posição $j$.\n",
    "\n",
    "Agora, para preencher a matriz D, você acompanhará o argmax de onde você veio da seguinte maneira:\n",
    "\n",
    "<img src=\"./imgs/viterbi6.png\">\n",
    "\n",
    "Para o $d_{i, j}$, temos que:\n",
    "\n",
    "$$ d_{i,j} = \\text{argmax}_{k} c_{k,j-1} * a_{k,i} * b_{i, \\text{cindex(wj)}} $$\n",
    "\n",
    "Em cada di,j, você simplesmente salvamos o k que maximiza a entrada e ci,j. Aqui, existem três estados que não são o estado inicial. Então, k é um, dois ou três. Nessa função argmax, retorna o k que maximiza as funções do argumento em vez do valor máximo.\n",
    "\n",
    "**A matriz $D$ (ou matriz de rastreamento) é usada para armazenar os índices dos estados predecessores que maximizaram as probabilidades acumuladas durante o forward pass. Isso é essencial para reconstruir a sequência de estados ótima durante o backward pass.**\n",
    "\n",
    "Observe que a única diferença entre $c_ij$ e $d_ij$, é que no primeiro você calcula a probabilidade e no último  acompanhamos o índice da linha de onde essa probabilidade veio. Então acompanhamos qual k foi usado para obter essa probabilidade máxima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58aec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Viterbi: Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ea356",
   "metadata": {},
   "source": [
    "Agora, podemos agrupar tudo e construir o path que nos trará a POS tag da sentença.\n",
    "\n",
    "<img src=\"./imgs/viterbi7.png\">\n",
    "\n",
    "A equação acima fornece apenas o índice da linha mais alta na última coluna de C. Depois de fazer isso, podemos prosseguir e começar a usar a matriz D da seguinte maneira:\n",
    "\n",
    "<img src=\"./imgs/viterbi8.png\">\n",
    "\n",
    "Observe que, como começamos no índice um, a última palavra $(w5​)$ é $t1$​ . Então vamos para a primeira linha de D e qualquer que seja esse número, ele indica a linha da próxima tag de classe gramatical. Em seguida, a tag da próxima classe gramatical indica a linha da próxima e assim por diante. Isso nos permite reconstruir as tags POS da sua frase.\n",
    "\n",
    "**Inicialização:** Começa pelo estado final da sequência (última posição de palavras observadas).\n",
    "Encontra o estado (tag de POS) que tem a maior probabilidade final.\n",
    "\n",
    "**Rastreamento Retroativo:** A partir do estado final (última posição j), usa-se a matriz D para retroceder até o estado inicial (primeira posição j). Para cada posição j, D nos diz qual foi o estado predecessor que levou ao estado atual naquela posição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1550835a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-20T18:50:22.746278Z",
     "start_time": "2024-06-20T18:50:22.733634Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequência ótima de estados: [1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo simplificado para ilustração\n",
    "def viterbi_backward_pass(C, D):\n",
    "    num_states, num_positions = C.shape\n",
    "    best_sequence = []\n",
    "\n",
    "    # Encontra o estado com a maior pontuação na última posição\n",
    "    best_last_state = np.argmax(C[:, -1])\n",
    "    best_sequence.append(best_last_state)\n",
    "\n",
    "    # Retrocede usando a matriz D para reconstruir a sequência ótima\n",
    "    for j in range(num_positions - 1, 0, -1):\n",
    "        predecessor_state = D[best_last_state, j]\n",
    "        best_sequence.append(predecessor_state)\n",
    "        best_last_state = predecessor_state\n",
    "\n",
    "    # Inverte a sequência para obter a ordem correta (do início ao fim)\n",
    "    best_sequence.reverse()\n",
    "\n",
    "    return best_sequence\n",
    "\n",
    "# Exemplo de uso\n",
    "C = np.array([[0.1, 0.3, 0.4],\n",
    "              [0.2, 0.5, 0.7],\n",
    "              [0.4, 0.6, 0.8]])\n",
    "\n",
    "D = np.array([[-1, 0, 1],\n",
    "              [-1, 0, 1],\n",
    "              [-1, 1, 2]])\n",
    "\n",
    "best_sequence = viterbi_backward_pass(C, D)\n",
    "print(\"Sequência ótima de estados:\", best_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909d891",
   "metadata": {},
   "source": [
    "# Week 3 - Autocomplete and Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744f1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "Explique, descreva e dê exemplos de mispelled words e additive words em autocorrects em NLP\n",
    "Explique, descreva e dê exemplos de como calcular probabilidades de palavras corretas em autocorrects em NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49683bc",
   "metadata": {},
   "source": [
    "## Subtítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434af207",
   "metadata": {},
   "source": [
    "## Subtítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f90b067",
   "metadata": {},
   "source": [
    "## Subtítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecca813",
   "metadata": {},
   "source": [
    "## Subtítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a4e79",
   "metadata": {},
   "source": [
    "## Subtítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16aeb73",
   "metadata": {},
   "source": [
    "## Subtítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37329cff",
   "metadata": {},
   "source": [
    "## Subtítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa49fde",
   "metadata": {},
   "source": [
    "## Subtítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3646db73",
   "metadata": {},
   "source": [
    "## Subtítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56aaa37a",
   "metadata": {},
   "source": [
    "## Subtítulo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf25f3",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embedding with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bf9ab1",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Classification and Vector Spaces, disponível em https://www.coursera.org/learn/probabilistic-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b03fa83",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
