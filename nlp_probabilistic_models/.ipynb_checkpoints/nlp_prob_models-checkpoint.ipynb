{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2948eec",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Probabilistic Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Classification and Vector Spaces da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fcda10",
   "metadata": {},
   "source": [
    "# Week 1 - Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc8b70c",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Word probabilities\n",
    "- Dynamic programming\n",
    "- Minimum edit distance\n",
    "- Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a86dbf7",
   "metadata": {},
   "source": [
    "## Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44bc2f2",
   "metadata": {},
   "source": [
    "A autocorreção em NLP é uma técnica que corrige automaticamente erros ortográficos ou de digitação em texto. Ela é amplamente utilizada em aplicativos de mensagens, processadores de texto e mecanismos de busca para melhorar a experiência do usuário. Ela é geralmente implementada usando modelos de **linguagem probabilísticos**, como os modelos de N-gramas. Esses modelos calculam a probabilidade de uma sequência de palavras e sugerem a sequência mais provável, dada uma palavra mal digitada. Eles são treinados em grandes corpora de texto para aprender padrões comuns na linguagem.\n",
    "\n",
    "<img src=\"./imgs/autocorrect_phone.png\">\n",
    "\n",
    "\n",
    "Para implementar um sistema de autocorreção, precisamos seguir as seguintes etapas:\n",
    "\n",
    "1. **Identificação do Erro:**\n",
    "   - O primeiro passo é identificar que uma palavra pode estar incorreta. Isso é feito comparando a palavra digitada com um dicionário ou usando técnicas mais avançadas, como distância de edição/edit distance (Levenshtein), que mede a diferença entre duas palavras com base nas operações necessárias para transformar uma na outra (inserção, exclusão, substituição).\n",
    "\n",
    "2. **Sugestão de Correção:**\n",
    "   - Uma vez identificado o erro, o sistema de autocorreção sugere uma ou mais correções com base na probabilidade das palavras no contexto. Isso pode ser feito usando modelos de N-gramas, onde a probabilidade de uma palavra é calculada com base nas palavras anteriores.\n",
    "\n",
    "3. **Escolha da Melhor Correção:**\n",
    "   - A sugestão de correção é escolhida com base em critérios como a probabilidade da palavra correta no contexto, a distância de edição mínima da palavra digitada e outras heurísticas.\n",
    "\n",
    "4. **Aplicação da Correção:**\n",
    "   - A correção é então aplicada ao texto, substituindo a palavra mal digitada pela palavra correta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655b502d",
   "metadata": {},
   "source": [
    "**Exemplos**\n",
    "\n",
    "1. **Erro de Digitação Simples:**\n",
    "   - **Texto Digitado:** \"Eu gosto de mangaa.\"\n",
    "   - **Correção Proposta:** \"Eu gosto de manga.\"\n",
    "   - **Explicação:** O sistema identifica que \"mangaa\" pode ser um erro de digitação para \"manga\" com base na proximidade no teclado e sugere a correção.\n",
    "\n",
    "2. **Inversão de Letras:**\n",
    "   - **Texto Digitado:** \"Estarmos esperndo por você.\"\n",
    "   - **Correção Proposta:** \"Estaremos esperando por você.\"\n",
    "   - **Explicação:** O sistema percebe que \"esperndo\" pode ser um erro de inversão de letras em \"esperando\" e sugere a correção.\n",
    "\n",
    "3. **Erros de Espaçamento:**\n",
    "   - **Texto Digitado:** \"Nãoseinada\"\n",
    "   - **Correção Proposta:** \"Não sei nada\"\n",
    "   - **Explicação:** O sistema identifica que \"Nãoseinada\" pode ser uma combinação de palavras e sugere a separação correta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b8974d",
   "metadata": {},
   "source": [
    "**Limitações**\n",
    "- **Palavras Fora do Vocabulário:** Se uma palavra está muito fora do vocabulário do modelo, a autocorreção pode falhar em sugerir a correção correta.\n",
    "- **Ambiguidade:** Em casos de ambiguidade, onde várias correções são possíveis, a autocorreção pode sugerir uma correção incorreta.\n",
    "- **Contexto Limitado:** Modelos de N-gramas têm um contexto limitado, então a autocorreção pode não capturar nuances mais complexas da linguagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031b61e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d6226e",
   "metadata": {},
   "source": [
    "### Métodos de Identificação de Palavras Erradas\n",
    "\n",
    "Identificar palavras erradas é o primeiro passo crucial no processo de autocorreção em NLP. Isso envolve a detecção de erros ortográficos, de digitação, e até erros gramaticais. Vamos explorar os métodos mais comuns para identificar palavras erradas, com explicações e exemplos.\n",
    "\n",
    "\n",
    "1. **Comparação com um Dicionário:**\n",
    "   - **Descrição:** Um método básico é comparar cada palavra do texto com um dicionário de palavras válidas. Se uma palavra não estiver no dicionário, é marcada como errada.\n",
    "   - **Exemplo:** Se o texto contiver \"applle\", a palavra \"applle\" não será encontrada no dicionário e será marcada como incorreta.\n",
    "\n",
    "\n",
    "2. **Distância de Edição (Levenshtein):**\n",
    "   - **Descrição:** A distância de edição mede quantas operações (inserção, deleção, substituição) são necessárias para transformar uma palavra em outra. Palavras com uma pequena distância de edição em relação a palavras do dicionário são consideradas erradas.\n",
    "   - **Exemplo:** A palavra \"bok\" tem uma distância de edição de 1 em relação a \"book\" (substituição de 'k' por 'o'), indicando um possível erro de digitação.\n",
    "\n",
    "\n",
    "3. **Modelos de N-gramas:**\n",
    "   - **Descrição:** Modelos de N-gramas podem identificar palavras erradas com base na improbabilidade de uma sequência de palavras. Se uma palavra resulta em uma sequência de N-gramas que raramente ocorre no corpus de treinamento, pode ser considerada errada.\n",
    "   - **Exemplo:** Em \"I have a blak cat\", a sequência \"a blak cat\" pode ser menos provável que \"a black cat\", indicando que \"blak\" pode estar errado.\n",
    "\n",
    "\n",
    "4. **Redes Neurais e Modelos de Linguagem:**\n",
    "   - **Descrição:** Modelos de linguagem avançados, como BERT ou GPT, podem identificar palavras erradas ao avaliar a coerência do contexto. Eles são treinados em grandes quantidades de texto e podem detectar palavras que não fazem sentido no contexto dado.\n",
    "   - **Exemplo:** Em \"She drived the car\", um modelo de linguagem pode identificar que \"drived\" é incorreto no contexto e sugerir \"drove\".\n",
    "\n",
    "#### Exemplos Práticos de Identificação de Palavras Erradas\n",
    "\n",
    "1. **Exemplo com Dicionário:**\n",
    "   - **Texto:** \"I am lerning NLP.\"\n",
    "   - **Identificação:** A palavra \"lerning\" não está no dicionário.\n",
    "   - **Correção Proposta:** \"I am learning NLP.\"\n",
    "\n",
    "2. **Exemplo com Distância de Edição:**\n",
    "   - **Texto:** \"He went to the shcool.\"\n",
    "   - **Identificação:** A palavra \"shcool\" tem uma distância de edição de 1 em relação a \"school\".\n",
    "   - **Correção Proposta:** \"He went to the school.\"\n",
    "\n",
    "3. **Exemplo com Modelos de N-gramas:**\n",
    "   - **Texto:** \"The quick brown fox jmps over the lazy dog.\"\n",
    "   - **Identificação:** A sequência \"fox jmps\" é muito improvável.\n",
    "   - **Correção Proposta:** \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "4. **Exemplo com Modelos de Linguagem:**\n",
    "   - **Texto:** \"He gived her a gift.\"\n",
    "   - **Identificação:** O modelo de linguagem identifica que \"gived\" não faz sentido no contexto e sugere \"gave\".\n",
    "   - **Correção Proposta:** \"He gave her a gift.\"\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Palavras Novas ou Jargões:** Palavras que são novas, jargões ou nomes próprios podem não estar em um dicionário, levando a falsos positivos.\n",
    "- **Erros Contextuais:** Alguns erros só podem ser identificados corretamente no contexto adequado, o que modelos de linguagem mais avançados fazem melhor.\n",
    "- **Desempenho:** Métodos mais avançados, como modelos de linguagem, geralmente têm melhor desempenho, mas requerem mais poder computacional e dados para treinamento.\n",
    "\n",
    "\n",
    "Identificar palavras erradas é um processo que pode ser feito de várias maneiras, desde métodos simples como comparação com um dicionário até técnicas avançadas envolvendo modelos de linguagem. Cada método tem seus prós e contras, e a escolha do método adequado depende do contexto e dos requisitos específicos da aplicação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658a1ed0",
   "metadata": {},
   "source": [
    "### Cálculo da Distância de Edição\n",
    "\n",
    "A distância de edição, ou distância de Levenshtein, é uma métrica que mede o quão diferentes duas strings são, calculando o número mínimo de operações necessárias para transformar uma string em outra. As operações permitidas são inserção, deleção e substituição de caracteres.\n",
    "\n",
    "O algoritmo de Levenshtein usa uma abordagem de programação dinâmica para calcular a distância de edição entre duas strings. Aqui está uma descrição do algoritmo:\n",
    "\n",
    "1. **Inicialização:**\n",
    "   - Crie uma matriz $d$ de tamanho $(m+1) \\times (n+1)$, onde $m$ é o comprimento da primeira string $s$ e $n$ é o comprimento da segunda string $t$.\n",
    "   - Inicialize $d[i][0] = i$ para $i = 0, 1, ..., m$.\n",
    "   - Inicialize $d[0][j] = j$ para $j = 0, 1, ..., n$.\n",
    "\n",
    "2. **Recorrência:**\n",
    "   - Para cada $i = 1, ..., m$ e $j = 1, ..., n$:\n",
    "     - Se $s[i-1] = t[j-1]$, então $custo = 0$; caso contrário, $custo = 1$.\n",
    "     - $d[i][j] = \\min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + custo)$.\n",
    "\n",
    "3. **Resultado:**\n",
    "   - A distância de edição é encontrada em $d[m][n]$.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Aqui está uma implementação do cálculo da distância de edição em Python:\n",
    "\n",
    "```python\n",
    "def distancia_de_edicao(s, t):\n",
    "    m = len(s)\n",
    "    n = len(t)\n",
    "    \n",
    "    # Criação da matriz\n",
    "    d = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    \n",
    "    # Inicialização da matriz\n",
    "    for i in range(m + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        d[0][j] = j\n",
    "    \n",
    "    # Preenchimento da matriz com os valores de distância\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                custo = 0\n",
    "            else:\n",
    "                custo = 1\n",
    "            d[i][j] = min(d[i - 1][j] + 1,  # Deleção\n",
    "                          d[i][j - 1] + 1,  # Inserção\n",
    "                          d[i - 1][j - 1] + custo)  # Substituição\n",
    "    \n",
    "    # Distância de edição é encontrada na última célula da matriz\n",
    "    return d[m][n]\n",
    "\n",
    "# Exemplos\n",
    "s1 = \"kitten\"\n",
    "s2 = \"sitting\"\n",
    "print(f\"Distância de edição entre '{s1}' e '{s2}': {distancia_de_edicao(s1, s2)}\")\n",
    "\n",
    "s3 = \"flaw\"\n",
    "s4 = \"lawn\"\n",
    "print(f\"Distância de edição entre '{s3}' e '{s4}': {distancia_de_edicao(s3, s4)}\")\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Strings:** \"kitten\" e \"sitting\"\n",
    "   - **Cálculo:** Transformar \"kitten\" em \"sitting\" envolve as operações:\n",
    "     - Substituir 'k' por 's': \"sitten\"\n",
    "     - Substituir 'e' por 'i': \"sittin\"\n",
    "     - Inserir 'g' no final: \"sitting\"\n",
    "   - **Distância de Edição:** 3\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Strings:** \"flaw\" e \"lawn\"\n",
    "   - **Cálculo:** Transformar \"flaw\" em \"lawn\" envolve as operações:\n",
    "     - Substituir 'f' por 'l': \"law\"\n",
    "     - Inserir 'n' no final: \"lawn\"\n",
    "   - **Distância de Edição:** 2\n",
    "\n",
    "#### Visualização da Matriz\n",
    "\n",
    "Para ilustrar como a matriz $d$ é preenchida, vejamos o exemplo das strings \"kitten\" e \"sitting\":\n",
    "\n",
    "```\n",
    "     '' s i t t i n g\n",
    "  '' 0  1 2 3 4 5 6 7\n",
    "  k  1  1 2 3 4 5 6 7\n",
    "  i  2  2 1 2 3 4 5 6\n",
    "  t  3  3 2 1 2 3 4 5\n",
    "  t  4  4 3 2 1 2 3 4\n",
    "  e  5  5 4 3 2 2 3 4\n",
    "  n  6  6 5 4 3 3 2 3\n",
    "```\n",
    "\n",
    "A distância de edição é uma métrica poderosa para comparar a similaridade entre duas strings, sendo amplamente utilizada em tarefas de NLP, como correção ortográfica e reconhecimento de padrões. O algoritmo de Levenshtein é eficiente e fácil de implementar, fornecendo uma base sólida para muitas aplicações de processamento de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662886b3",
   "metadata": {},
   "source": [
    "[Vídeo](https://www.youtube.com/watch?v=kyeyjPlKzJM) com explicação intuitiva"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f11fb",
   "metadata": {},
   "source": [
    "### Métodos para Filtrar Palavras Candidatas\n",
    "\n",
    "Filtrar palavras candidatas é um passo crucial após identificar palavras erradas para fornecer sugestões de correção. Este processo envolve gerar uma lista de possíveis correções para uma palavra mal escrita e, em seguida, classificar ou selecionar as melhores opções com base em vários critérios. Aqui estão os principais métodos para filtrar palavras candidatas, com explicações e exemplos.\n",
    "\n",
    "\n",
    "1. **Distância de Edição:**\n",
    "   - **Descrição:** Calcular a distância de edição entre a palavra errada e cada palavra do dicionário. As palavras com a menor distância de edição são consideradas as melhores candidatas.\n",
    "   - **Exemplo:** Para a palavra \"speling\", palavras como \"spelling\" (distância 1) e \"spieling\" (distância 2) seriam consideradas, com \"spelling\" sendo a melhor candidata devido à menor distância de edição.\n",
    "\n",
    "2. **N-gramas e Modelos de Linguagem:**\n",
    "   - **Descrição:** Utilizar modelos de N-gramas ou modelos de linguagem para avaliar a probabilidade das palavras candidatas no contexto da frase.\n",
    "   - **Exemplo:** Em \"I have a blak cat\", o modelo pode sugerir \"black\" como correção para \"blak\" porque \"a black cat\" é uma sequência mais comum e provável do que \"a blak cat\".\n",
    "\n",
    "3. **Frequência no Corpus:**\n",
    "   - **Descrição:** As palavras candidatas são classificadas com base na sua frequência em um corpus de texto. Palavras mais comuns são mais prováveis de serem correções corretas.\n",
    "   - **Exemplo:** Se \"hte\" foi digitado incorretamente e \"the\" é muito mais frequente no corpus do que outras combinações possíveis, \"the\" será a principal sugestão.\n",
    "\n",
    "4. **Contexto Semântico:**\n",
    "   - **Descrição:** Analisar o contexto semântico utilizando embeddings de palavras ou modelos contextuais (como BERT) para selecionar palavras que façam sentido no contexto da frase.\n",
    "   - **Exemplo:** Em \"I visited the captial city\", o modelo pode sugerir \"capital\" em vez de \"captial\" porque \"visited the capital city\" faz sentido semanticamente.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Vamos ver como esses métodos podem ser combinados para filtrar palavras candidatas. Para simplificação, usaremos apenas a distância de edição e a frequência no corpus neste exemplo.\n",
    "\n",
    "1. Calcular a distância de edição para cada palavra do dicionário.\n",
    "2. Classificar palavras por distância de edição.\n",
    "3. Utilizar frequência no corpus para ordenar palavras com a mesma distância.\n",
    "\n",
    "```python\n",
    "def distancia_de_edicao(s, t):\n",
    "    m = len(s)  # Comprimento da primeira string\n",
    "    n = len(t)  # Comprimento da segunda string\n",
    "\n",
    "    # Inicializa uma matriz (m+1) x (n+1) com zeros\n",
    "    d = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "\n",
    "    # Inicializa a primeira coluna da matriz\n",
    "    for i in range(m + 1):\n",
    "        d[i][0] = i\n",
    "\n",
    "    # Inicializa a primeira linha da matriz\n",
    "    for j in range(n + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    # Preenche a matriz com os valores de distância de edição\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                custo = 0  # Nenhum custo se os caracteres são iguais\n",
    "            else:\n",
    "                custo = 1  # Custo de substituição se os caracteres são diferentes\n",
    "            d[i][j] = min(d[i - 1][j] + 1,    # Custo de deleção\n",
    "                          d[i][j - 1] + 1,    # Custo de inserção\n",
    "                          d[i - 1][j - 1] + custo)  # Custo de substituição\n",
    "\n",
    "    return d[m][n]  # Retorna a distância de edição entre as duas strings\n",
    "\n",
    "def filtrar_candidatos(palavra_errada, dicionario, frequencia_corpus):\n",
    "    candidatos = []\n",
    "    # Calcula a distância de edição para cada palavra do dicionário\n",
    "    for palavra in dicionario:\n",
    "        dist = distancia_de_edicao(palavra_errada, palavra)\n",
    "        candidatos.append((palavra, dist))  # Adiciona a palavra e sua distância à lista de candidatos\n",
    "\n",
    "    # Ordena os candidatos primeiro pela distância de edição e depois pela frequência no corpus (em ordem decrescente)\n",
    "    candidatos.sort(key=lambda x: (x[1], -frequencia_corpus.get(x[0], 0)))\n",
    "\n",
    "    # Retorna apenas as palavras candidatas, sem as distâncias\n",
    "    return [candidato[0] for candidato in candidatos]\n",
    "\n",
    "# Exemplo de dicionário e frequências\n",
    "dicionario = [\"spelling\", \"spieling\", \"selling\", \"smiling\"]\n",
    "frequencia_corpus = {\"spelling\": 500, \"spieling\": 5, \"selling\": 300, \"smiling\": 150}\n",
    "\n",
    "# Palavra errada\n",
    "palavra_errada = \"speling\"\n",
    "\n",
    "# Filtrar palavras candidatas\n",
    "candidatos = filtrar_candidatos(palavra_errada, dicionario, frequencia_corpus)\n",
    "print(f\"Candidatos para '{palavra_errada}': {candidatos}\")\n",
    "\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Palavra Errada:** \"speling\"\n",
    "   - **Dicionário:** [\"spelling\", \"spieling\", \"selling\", \"smiling\"]\n",
    "   - **Frequência no Corpus:** {\"spelling\": 500, \"spieling\": 5, \"selling\": 300, \"smiling\": 150}\n",
    "   - **Candidatos:** [\"spelling\", \"selling\", \"smiling\", \"spieling\"]\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Palavra Errada:** \"recieve\"\n",
    "   - **Dicionário:** [\"receive\", \"recipe\", \"recite\"]\n",
    "   - **Frequência no Corpus:** {\"receive\": 1000, \"recipe\": 400, \"recite\": 50}\n",
    "   - **Candidatos:** [\"receive\", \"recipe\", \"recite\"]\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Balanceamento entre Precisão e Performance:** Calcular a distância de edição para um grande dicionário pode ser computacionalmente caro. Técnicas como truncagem de dicionário ou uso de índices podem melhorar a performance.\n",
    "- **Combinação de Critérios:** Usar múltiplos critérios (distância de edição, frequência no corpus, contexto semântico) pode aumentar a precisão das sugestões.\n",
    "- **Personalização:** Ajustar a frequência do corpus com base no domínio específico (por exemplo, termos médicos para um dicionário médico) pode melhorar os resultados.\n",
    "\n",
    "\n",
    "Filtrar palavras candidatas é um processo multifacetado que combina técnicas de comparação de strings, análise de frequência e compreensão contextual. Implementar uma abordagem robusta pode significativamente melhorar a precisão das sugestões de correção em sistemas de autocorreção e outras aplicações de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa36e0a",
   "metadata": {},
   "source": [
    "### Métodos para Calcular as Probabilidades das Palavras\n",
    "\n",
    "Calcular as probabilidades das palavras é um passo importante para melhorar a precisão de sugestões em tarefas como correção ortográfica. Esse processo envolve a utilização de modelos de linguagem que atribuem uma probabilidade a cada palavra candidata com base em diferentes critérios, como frequência no corpus, contexto e modelos de n-gramas.\n",
    "\n",
    "\n",
    "1. **Frequência no Corpus:**\n",
    "   - **Descrição:** As palavras mais frequentes em um grande corpus de texto são consideradas mais prováveis.\n",
    "   - **Exemplo:** Se a palavra \"the\" aparece 5000 vezes em um corpus, sua probabilidade é maior do que a de uma palavra que aparece apenas 5 vezes.\n",
    "\n",
    "<img src=\"./imgs/calculating_word_prob.png\">\n",
    "\n",
    "2. **Modelos de N-gramas:**\n",
    "   - **Descrição:** Usar a frequência de sequências de palavras (n-gramas) para calcular a probabilidade de uma palavra em um dado contexto.\n",
    "   - **Exemplo:** Em um modelo de bigrama, a probabilidade de \"cat\" seguir \"the\" pode ser calculada como $(P(cat|the) = \\frac{C(the\\ cat)}{C(the)} $, onde $C$ denota contagens no corpus.\n",
    "\n",
    "\n",
    "3. **Modelos de Linguagem Baseados em Redes Neurais:**\n",
    "   - **Descrição:** Modelos como Word2Vec, GloVe, BERT, ou GPT podem gerar probabilidades para palavras com base em embeddings de palavras e contexto.\n",
    "   - **Exemplo:** Dado o contexto \"I have a bl__ cat\", um modelo de linguagem pode calcular a probabilidade de várias palavras preencherem o espaço em branco, como \"black\", \"blue\", etc.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Vamos ver como podemos implementar o cálculo de probabilidades de palavras usando frequências no corpus e um simples modelo de bigrama. Para isso precisamos seguir os seguintes passos:\n",
    "\n",
    "1. **Frequência no Corpus:** Calcular a probabilidade baseada na frequência relativa da palavra no corpus.\n",
    "2. **Modelo de Bigramas:** Calcular a probabilidade de uma palavra dado seu contexto anterior usando bigramas.\n",
    "\n",
    "\n",
    "```python\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Corpus de exemplo\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat chased the dog\",\n",
    "    \"the dog barked at the cat\"\n",
    "]\n",
    "\n",
    "# Função para calcular frequências unigrama e bigrama\n",
    "def calcular_frequencias(corpus):\n",
    "    unigramas = Counter()\n",
    "    bigramas = defaultdict(Counter)\n",
    "    \n",
    "    for frase in corpus:\n",
    "        palavras = frase.split()\n",
    "        for i in range(len(palavras)):\n",
    "            unigramas[palavras[i]] += 1\n",
    "            if i > 0:\n",
    "                bigramas[palavras[i-1]][palavras[i]] += 1\n",
    "    \n",
    "    total_palavras = sum(unigramas.values())\n",
    "    return unigramas, bigramas, total_palavras\n",
    "\n",
    "# Calcular frequências no corpus\n",
    "unigramas, bigramas, total_palavras = calcular_frequencias(corpus)\n",
    "\n",
    "# Função para calcular probabilidade unigrama\n",
    "def probabilidade_unigrama(palavra):\n",
    "    return unigramas[palavra] / total_palavras\n",
    "\n",
    "# Função para calcular probabilidade bigrama\n",
    "def probabilidade_bigrama(palavra_anterior, palavra):\n",
    "    if palavra_anterior in bigramas and palavra in bigramas[palavra_anterior]:\n",
    "        return bigramas[palavra_anterior][palavra] / unigramas[palavra_anterior]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Exemplo de cálculo de probabilidades\n",
    "palavra = \"cat\"\n",
    "palavra_anterior = \"the\"\n",
    "\n",
    "print(f\"Probabilidade unigrama de '{palavra}': {probabilidade_unigrama(palavra)}\")\n",
    "print(f\"Probabilidade bigrama de '{palavra}' dado '{palavra_anterior}': {probabilidade_bigrama(palavra_anterior, palavra)}\")\n",
    "\n",
    "output:\n",
    "Probabilidade unigrama de 'cat': 0.13043478260869565\n",
    "Probabilidade bigrama de 'cat' dado 'the': 0.375\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Palavra:** \"cat\"\n",
    "   - **Probabilidade Unigrama:** Calculada como a frequência da palavra \"cat\" dividida pelo total de palavras no corpus.\n",
    "   - **Probabilidade Bigrama:** Calculada como a frequência da sequência \"the cat\" dividida pela frequência de \"the\".\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Palavra:** \"dog\"\n",
    "   - **Probabilidade Unigrama:** Calculada como a frequência da palavra \"dog\" dividida pelo total de palavras no corpus.\n",
    "   - **Probabilidade Bigrama:** Calculada como a frequência da sequência \"the dog\" dividida pela frequência de \"the\".\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Balanceamento de Dados:** Um corpus pequeno ou desequilibrado pode levar a estimativas de probabilidade imprecisas. É importante utilizar um corpus representativo e suficientemente grande.\n",
    "- **Suavização:** Técnicas de suavização, como Laplace ou Good-Turing, são usadas para lidar com bigramas ou n-gramas que não aparecem no corpus.\n",
    "- **Modelos Avançados:** Modelos de linguagem baseados em redes neurais (como BERT ou GPT) podem fornecer probabilidades mais precisas ao considerar o contexto completo de uma frase.\n",
    "\n",
    "Calcular as probabilidades das palavras é um componente essencial em muitas aplicações de NLP, incluindo correção ortográfica e autocompletar. Usando frequências no corpus e modelos de n-gramas, podemos estimar essas probabilidades de forma eficaz. Em aplicações mais avançadas, modelos de linguagem baseados em redes neurais podem proporcionar uma compreensão mais profunda e precisa do contexto, resultando em sugestões de palavras ainda mais acertadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ce3fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Minimum edit distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1814d9",
   "metadata": {},
   "source": [
    "### Operações de Distância Mínima de Edição\n",
    "\n",
    "A distância mínima de edição (ou distância de Levenshtein) é uma métrica que mede o número mínimo de operações necessárias para transformar uma string em outra. As operações permitidas são inserção, deleção e substituição de caracteres. Vamos detalhar cada operação e como elas são usadas para calcular a distância mínima de edição, seguido de exemplos práticos.\n",
    "\n",
    "\n",
    "1. **Inserção (Insert):** Adicionar um caractere à string. Exemplo: Transformar \"cat\" em \"cats\" requer uma inserção de 's' no final. A operação é `Insert('s')`.\n",
    "\n",
    "2. **Deleção (Delete):** Remover um caractere da string. Exemplo: Transformar \"cats\" em \"cat\" requer uma deleção de 's'. A operação é `Delete('s')`.\n",
    "\n",
    "3. **Substituição (Replace):** Substituir um caractere por outro. Exemplo: Transformar \"cat\" em \"cut\" requer uma substituição de 'a' por 'u'. A operação é `Replace('a', 'u')`.\n",
    "\n",
    "Podemos atribuir pesos diferentes para cada operação, para assim, buscar otimizar a menor distancia. Por exemplo, atribuir os pesos de 1, 1 e 2 para insert, delete e replace. Para buscar a menor distancia utilizamos o algoritmod e Levenshtein.\n",
    "\n",
    "<img src=\"./imgs/edit_distance_w.png\">\n",
    "\n",
    "O algoritmo de Levenshtein usa programação dinâmica para calcular a distância mínima de edição entre duas strings. Como já citado acima, segue os seguintes passos:\n",
    "\n",
    "1. **Inicialização:**\n",
    "   - Crie uma matriz `d` de tamanho `(m+1) x (n+1)`, onde `m` é o comprimento da primeira string `s` e `n` é o comprimento da segunda string `t`.\n",
    "   - Inicialize `d[i][0] = i` para `i = 0, 1, ..., m`.\n",
    "   - Inicialize `d[0][j] = j` para `j = 0, 1, ..., n`.\n",
    "\n",
    "2. **Preenchimento da Matriz:**\n",
    "   - Para cada `i = 1, ..., m` e `j = 1, ..., n`:\n",
    "     - Se `s[i-1] == t[j-1]`, então `custo = 0`; caso contrário, `custo = 1`.\n",
    "     - `d[i][j] = min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + custo)`.\n",
    "\n",
    "3. **Resultado:**\n",
    "   - A distância de edição é encontrada em `d[m][n]`.\n",
    "\n",
    "(Ver #2.3.2.1 a implementação e exemplos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea28f06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Minimum edit distance algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5a9adf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T18:46:18.739139Z",
     "start_time": "2024-06-18T18:46:18.722360Z"
    }
   },
   "source": [
    "Quando estamos computando a distancia mínima de edição, começamos com a palavra fonte e temos que transformá-la na palavra alvo. \n",
    "\n",
    "<img src=\"./imgs/minimum_edit_target.png\">\n",
    "     \n",
    "Partindo de # para #, teremos um custo de 0. Partindo de p para #, teremos um custo de 1, porque faremos uma deleção. De p para s, teremos um custo de 2, porque esse é o custo mínimo que se poderia ter para ir de p a s. Podemos continuar assim preenchendo um elemento por vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc69c5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T18:53:33.139135Z",
     "start_time": "2024-06-18T18:53:33.133892Z"
    }
   },
   "source": [
    "Para preencher a seguinte tabela:\n",
    "\n",
    "<img src=\"./imgs/minimum_edit1.png\">\n",
    "\n",
    "\n",
    "Existem três equações:\n",
    "\n",
    "- $D[i,j] = D[i-1, j] + \\text{del_cost}$: indica que você deseja preencher a célula atual (i,j) usando o custo na célula encontrada diretamente acima.\n",
    "\n",
    "- $D[i,j] = D[i, j-1] + \\text{ins_cost}$: indica que você deseja preencher a célula atual (i,j) usando o custo na célula localizada diretamente à sua esquerda.\n",
    "\n",
    "- $D[i,j] = D[i-1, j-1] + \\text{rep_cost}$: o custo rep pode ser 2 ou 0 dependendo se você vai realmente substituí-lo ou não.\n",
    "\n",
    "A cada passo você verifica os três caminhos possíveis de onde pode vir e seleciona o menos caro. Quando terminar, você obterá o seguinte:\n",
    "\n",
    "<img src=\"./imgs/minimum_edit2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d4c598",
   "metadata": {},
   "source": [
    "# Week 2 - Part of Speech Tagging and Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fc63f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Part of Speech Tagging (POS Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb531e3b",
   "metadata": {},
   "source": [
    "**Part of Speech Tagging** (ou marcação de partes do discurso) é o processo de atribuir uma etiqueta a cada palavra de um texto, indicando a sua função gramatical. As etiquetas típicas incluem categorias como substantivo, verbo, adjetivo, advérbio, pronome, preposição, conjunção, entre outros.\n",
    "\n",
    "O processo de POS Tagging envolve várias etapas:\n",
    "\n",
    "1. **Tokenização:** Dividir o texto em unidades menores, chamadas tokens (geralmente palavras).\n",
    "2. **Análise Contextual:** Avaliar o contexto em que cada palavra aparece para determinar a sua função gramatical.\n",
    "3. **Aplicação de Regras e Modelos Estatísticos:** Utilizar regras linguísticas e/ou modelos treinados em grandes corpora de textos anotados para prever a etiqueta correta.\n",
    "\n",
    "**Exemplos de POS Tagging**\n",
    "\n",
    "Considere a frase: \n",
    "- ``\"The quick brown fox jumps over the lazy dog.\"``\n",
    "\n",
    "Após a tokenização e POS Tagging, a frase pode ser anotada da seguinte forma:\n",
    "- ``The/DT quick/JJ brown/JJ fox/NN jumps/VBZ over/IN the/DT lazy/JJ dog/NN``\n",
    "\n",
    "Onde:\n",
    "- DT: Determinante\n",
    "- JJ: Adjetivo\n",
    "- NN: Substantivo\n",
    "- VBZ: Verbo, 3ª pessoa do singular presente\n",
    "- IN: Preposição\n",
    "\n",
    "POS tagging possui diversas aplicações, como:\n",
    "\n",
    "### 1. Named Entity Recognition (NER)\n",
    "\n",
    "**Named Entity Recognition** é a tarefa de identificar e classificar entidades mencionadas no texto em categorias predefinidas, como nomes de pessoas, organizações, localizações, datas, etc. O POS Tagging é uma etapa importante no NER, pois ajuda a identificar as entidades com base na função gramatical das palavras.\n",
    "\n",
    "**Exemplo:**\n",
    "- Texto: \"Barack Obama was born in Hawaii.\"\n",
    "- NER Resultado: [Barack Obama/PERSON, Hawaii/LOCATION]\n",
    "\n",
    "### 2. Coreference Resolution\n",
    "\n",
    "**Coreference Resolution** refere-se à tarefa de identificar quando diferentes expressões no texto referem-se à mesma entidade. O POS Tagging é essencial aqui para identificar pronomes e outras referências que podem apontar para entidades mencionadas anteriormente no texto.\n",
    "\n",
    "**Exemplo:**\n",
    "- Texto: \"John said he would come. He is very reliable.\"\n",
    "- Coreference Resultado: [John -> he]\n",
    "\n",
    "### 3. Speech Recognition\n",
    "\n",
    "**Speech Recognition** é o processo de converter áudio em texto. O POS Tagging pode ser usado para melhorar a precisão do reconhecimento, fornecendo pistas contextuais sobre a estrutura gramatical esperada das frases.\n",
    "\n",
    "**Exemplo:**\n",
    "- Áudio: \"I can see the sea.\"\n",
    "- Reconhecimento com POS: \"I/PRP can/MD see/VB the/DT sea/NN.\"\n",
    "\n",
    "### Como Implementar POS Tagging\n",
    "\n",
    "Existem várias bibliotecas e ferramentas para realizar POS Tagging. Um exemplo popular é a biblioteca `nltk` (Natural Language Toolkit) em Python.\n",
    "\n",
    "**Exemplo com `nltk`:**\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Texto a ser analisado\n",
    "texto = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Tokenização\n",
    "tokens = nltk.word_tokenize(texto)\n",
    "\n",
    "# POS Tagging\n",
    "tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Exibição dos resultados\n",
    "print(tags)\n",
    "```\n",
    "\n",
    "**Saída esperada:**\n",
    "```\n",
    "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'JJ'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n",
    "```\n",
    "\n",
    "O POS Tagging é uma ferramenta fundamental em NLP, fornecendo informações gramaticais cruciais que são usadas em várias aplicações avançadas, como NER, Coreference Resolution e Speech Recognition. Ele permite uma análise mais profunda e precisa do texto, facilitando a construção de sistemas que compreendem e processam a linguagem natural de forma mais eficaz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6080e85b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Markov Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a9ef4",
   "metadata": {},
   "source": [
    "Markov Chains é um tipo de modelo estocástico que descreve a sequência de possíveis eventos. Nele, a probabilidade do evento seguinte, depende apenas do estado do evento anterior. Um estado é a representação da condição no momento presente, e, esse modelo pode ser representado como a estrutura de dados DAG/grafo. Nessa DAG/grafo, cada círculo representa o estado do modelo e as flechas representam a direção. Cada caminho de cada estado possui uma probabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a2006",
   "metadata": {},
   "source": [
    "**Cadeias de Markov** são modelos probabilísticos que descrevem uma sequência de possíveis eventos, onde a probabilidade de cada evento depende apenas do estado atual e não dos eventos anteriores **(propriedade de Markov)**. Em outras palavras, uma cadeia de Markov é um sistema que transita de um estado para outro com base em probabilidades fixas. Esses modelos possuem os seguintes conceitos básicos:\n",
    "\n",
    "1. **Estados:** Conjunto de todos os possíveis estados do sistema.\n",
    "2. **Transições:** Mudanças de um estado para outro.\n",
    "3. **Probabilidades de Transição:** Probabilidades associadas a cada transição entre estados.\n",
    "\n",
    "\n",
    "Imagine um clima que pode ser \"ensolarado\", \"nublado\" ou \"chuvoso\". As probabilidades de transição entre esses estados podem ser representadas por uma matriz de transição:\n",
    "\n",
    "|        | Ensolarado | Nublado | Chuvoso |\n",
    "|--------|-------------|---------|---------|\n",
    "| **Ensolarado** | 0.8         | 0.15    | 0.05    |\n",
    "| **Nublado**    | 0.2         | 0.6     | 0.2     |\n",
    "| **Chuvoso**    | 0.25        | 0.25    | 0.5     |\n",
    "\n",
    "Se hoje está ensolarado, a probabilidade de estar ensolarado novamente amanhã é 0.8, nublado 0.15 e chuvoso 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff37d19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Markov Chains and POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c1ee80",
   "metadata": {},
   "source": [
    "No contexto de **Part of Speech Tagging**, as cadeias de Markov são usadas para modelar as sequências de etiquetas gramaticais (tags). A ideia é que a etiqueta de uma palavra depende da etiqueta da palavra anterior.\n",
    "\n",
    "<img src=\"./imgs/markov_chain_words.png\">\n",
    "<img src=\"./imgs/markov_chain_dag.png\">\n",
    "\n",
    "<img src=\"./imgs/markov_chain_words_dag.png\">\n",
    "<img src=\"./imgs/markov_chain_transition_matrix.png\">\n",
    "\n",
    "Na matriz de transição acima, temos 40% de probabilidade da frase iniciar com NN (noun = substantivo), 10% de começar com um verbo e 50% de iniciar com outro POS.\n",
    "\n",
    "Para encontrar a sequência mais provável de etiquetas em POS Tagging, comummente é utilizado o Algoritmo de Viterbi, usando um modelo de **Cadeia de Markov Oculta (HMM)**.\n",
    "\n",
    "**Passos do Algoritmo:**\n",
    "\n",
    "1. **Inicialização:** Definir as probabilidades iniciais para o primeiro estado (etiqueta da primeira palavra).\n",
    "2. **Recursão:** Para cada palavra subsequente, calcular a probabilidade de cada etiqueta baseada nas etiquetas anteriores e na probabilidade de transição.\n",
    "3. **Terminação:** Selecionar a sequência de etiquetas com a maior probabilidade.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Considere uma frase \"The cat sleeps\".\n",
    "\n",
    "1. **Probabilidades de Emissão:** Probabilidade de uma palavra ser uma certa etiqueta (e.g., P(\"The\"|DT) = 0.9).\n",
    "2. **Probabilidades de Transição:** Probabilidade de uma etiqueta seguir outra (e.g., P(DT -> NN) = 0.8).\n",
    "\n",
    "Aplicando o algoritmo de Viterbi, podemos calcular a sequência mais provável de etiquetas: \"The/DT cat/NN sleeps/VBZ\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850883c",
   "metadata": {},
   "source": [
    "### Código Exemplo de POS Tagging com HMM\n",
    "\n",
    "Aqui está um exemplo simplificado de como um HMM pode ser usado para POS tagging em Python:\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "# Treinamento de dados de exemplo\n",
    "train_data = [\n",
    "    [('The', 'DT'), ('cat', 'NN'), ('sleeps', 'VBZ')],\n",
    "    [('A', 'DT'), ('dog', 'NN'), ('barks', 'VBZ')],\n",
    "]\n",
    "\n",
    "# Criação e treino do modelo HMM\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)\n",
    "\n",
    "# Teste com uma nova sentença\n",
    "test_sentence = ['The', 'dog', 'sleeps']\n",
    "tagged_sentence = hmm_tagger.tag(test_sentence)\n",
    "print(tagged_sentence)\n",
    "```\n",
    "\n",
    "**Saída esperada:**\n",
    "```\n",
    "[('The', 'DT'), ('dog', 'NN'), ('sleeps', 'VBZ')]\n",
    "```\n",
    "\n",
    "Este exemplo treina um HMM simples para POS tagging e usa o modelo para etiquetar uma nova sentença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba15767",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:58:13.916545Z",
     "start_time": "2024-06-26T16:58:11.302807Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('dog', 'NN'), ('sleeps', 'VBZ')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import hmm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "#%config Application.verbose_crash=False\n",
    "\n",
    "# Treinamento de dados de exemplo\n",
    "train_data = [\n",
    "    [('The', 'DT'), ('cat', 'NN'), ('sleeps', 'VBZ')],\n",
    "    [('A', 'DT'), ('dog', 'NN'), ('barks', 'VBZ')],\n",
    "]\n",
    "\n",
    "# Criação e treino do modelo HMM\n",
    "trainer = hmm.HiddenMarkovModelTrainer()\n",
    "hmm_tagger = trainer.train(train_data)\n",
    "\n",
    "# Teste com uma nova sentença\n",
    "test_sentence = ['The', 'dog', 'sleeps']\n",
    "tagged_sentence = hmm_tagger.tag(test_sentence)\n",
    "print(tagged_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4a70cf",
   "metadata": {},
   "source": [
    "### Markov Chains in Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d15384",
   "metadata": {},
   "source": [
    "No **Reconhecimento de Fala**, as cadeias de Markov são usadas para modelar a sequência de fonemas ou palavras. Um exemplo comum é o uso de **Modelos de Markov Ocultos (HMMs)** para representar a relação entre o áudio (observações) e as sequências de palavras ou fonemas (estados ocultos).\n",
    "\n",
    "**Passos no Reconhecimento de Fala:**\n",
    "\n",
    "1. **Treinamento:** Usar dados de áudio etiquetados para treinar o modelo HMM, ajustando as probabilidades de transição entre fonemas e as probabilidades de emissão de observações (características do áudio).\n",
    "2. **Decodificação:** Dado um novo trecho de áudio, usar o algoritmo de Viterbi para encontrar a sequência mais provável de fonemas ou palavras que correspondem ao áudio.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Para a frase falada \"Hello world\":\n",
    "\n",
    "1. **Probabilidades de Emissão:** Probabilidades de um segmento de áudio ser emitido por um determinado fonema.\n",
    "2. **Probabilidades de Transição:** Probabilidades de um fonema seguir outro fonema.\n",
    "\n",
    "Usando HMMs, o sistema pode decodificar o áudio para determinar a sequência de palavras \"Hello world\".\n",
    "\n",
    "As Cadeias de Markov, particularmente os Modelos de Markov Ocultos, são ferramentas poderosas em NLP e reconhecimento de fala, permitindo modelar dependências sequenciais em dados. Em POS Tagging, elas ajudam a prever a sequência de etiquetas gramaticais baseadas no contexto, enquanto no reconhecimento de fala, elas facilitam a interpretação de sequências de áudio em texto significativo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f5de99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65e3d2e",
   "metadata": {},
   "source": [
    "**Hidden Markov Model** (HMM) é uma extensão das Cadeias de Markov, onde os estados não são diretamente observáveis (ocultos). Em vez disso, observamos sinais ou evidências que são probabilisticamente dependentes dos estados ocultos. HMM é amplamente utilizado em tarefas como reconhecimento de fala, POS tagging, entre outras. Conceitos Básicos do HMM:\n",
    "\n",
    "1. **Estados Ocultos:** Conjunto de estados que não são diretamente observáveis.\n",
    "2. **Observações:** Conjunto de observações que são visíveis e dependem dos estados ocultos.\n",
    "3. **Probabilidades de Transição:** Probabilidades de transitar de um estado oculto para outro.\n",
    "4. **Probabilidades de Emissão:** Probabilidades de um estado oculto gerar uma determinada observação.\n",
    "5. **Probabilidades Iniciais:** Probabilidades de começar em cada estado oculto.\n",
    "\n",
    "Um HMM é definido por:\n",
    "- Um conjunto de estados ocultos $ S = \\{s_1, s_2, \\ldots, s_N\\} $ ou $ Q = \\{q_1, q_2, \\ldots, s_N\\} $.\n",
    "- Um conjunto de observações $ O = \\{o_1, o_2, \\ldots, o_M\\} $.\n",
    "- Uma matriz de transição $ A $, onde $ A_{ij} $ é a probabilidade de transitar de $ s_i $ para $ s_j $.\n",
    "- Uma matriz de emissão $ B $, onde $ B_{jk} $ é a probabilidade de observar $ o_k $ dado o estado $ s_j $.\n",
    "- Um vetor de probabilidades iniciais $ \\pi $, onde $ \\pi_i $ é a probabilidade inicial de estar no estado $ s_i $.\n",
    "\n",
    "As **probabilidades de transição** permitiram identificar a probabilidade de transição de um POS para outro. Já nos **modelos de Markov ocultos**, usamos **probabilidades de emissão** que dão a probabilidade de passar de um estado (tag POS) **para uma palavra específica**.\n",
    "\n",
    "<img src=\"./imgs/hmm_states.png\">\n",
    "\n",
    "\n",
    "Por exemplo, dado que você está em um estado verb, podemos passar para outras palavras com certas probabilidades. Esta matriz de emissão B será usada com sua matriz de transição A, para ajudar a identificar a classe gramatical de uma palavra em uma frase. Para preencher sua matriz B, podemos simplesmente ter um conjunto de dados rotulado e calcular as probabilidades de ir de um POS para cada palavra do seu vocabulário. \n",
    "\n",
    "<img src=\"./imgs/hmm_matrix.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475f601b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-19T20:32:03.075918Z",
     "start_time": "2024-06-19T20:32:03.067065Z"
    }
   },
   "source": [
    "### Aplicações do HMM\n",
    "\n",
    "No **POS Tagging**, as palavras em uma sentença são as observações e as etiquetas gramaticais (partes do discurso) são os estados ocultos.\n",
    "\n",
    "Considere a frase \"The cat sleeps\".\n",
    "\n",
    "- Observações: [\"The\", \"cat\", \"sleeps\"]\n",
    "- Estados Ocultos: [\"DT\", \"NN\", \"VBZ\"]\n",
    "\n",
    "**Passos:**\n",
    "\n",
    "1. **Inicialização:** Definir probabilidades iniciais para cada etiqueta gramatical (estado oculto).\n",
    "2. **Transição:** Definir probabilidades de transição entre etiquetas gramaticais.\n",
    "3. **Emissão:** Definir probabilidades de emissão das palavras dado cada etiqueta gramatical.\n",
    "\n",
    "No **reconhecimento de fala**, os sinais de áudio (características extraídas do áudio) são as observações e as sequências de palavras ou fonemas são os estados ocultos.\n",
    "\n",
    "**Exemplo:**\n",
    "\n",
    "Considere a sequência de áudio correspondente a \"Hello world\".\n",
    "\n",
    "- Observações: [características do áudio]\n",
    "- Estados Ocultos: [\"H\", \"e\", \"l\", \"l\", \"o\", \" \", \"w\", \"o\", \"r\", \"l\", \"d\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5c1725",
   "metadata": {},
   "source": [
    "### Exemplo Prático em Python\n",
    "\n",
    "Vamos implementar um exemplo simples de HMM para POS tagging usando a biblioteca `hmmlearn`.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Definir os estados (etiquetas gramaticais)\n",
    "states = [\"DT\", \"NN\", \"VBZ\"]\n",
    "n_states = len(states)\n",
    "\n",
    "# Definir as observações (palavras)\n",
    "observations = [\"The\", \"cat\", \"sleeps\"]\n",
    "n_observations = len(observations)\n",
    "\n",
    "# Definir as sequências observadas em números\n",
    "obs_map = {word: idx for idx, word in enumerate(observations)}\n",
    "obs_seq = [obs_map[word] for word in observations]\n",
    "\n",
    "# Inicializar o modelo HMM\n",
    "model = hmm.MultinomialHMM(n_components=n_states, n_iter=100)\n",
    "\n",
    "# Probabilidades iniciais\n",
    "model.startprob_ = np.array([0.6, 0.3, 0.1])\n",
    "\n",
    "# Probabilidades de transição\n",
    "model.transmat_ = np.array([\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.3, 0.4, 0.3],\n",
    "    [0.5, 0.2, 0.3]\n",
    "])\n",
    "\n",
    "# Probabilidades de emissão\n",
    "model.emissionprob_ = np.array([\n",
    "    [0.6, 0.3, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "# Ajustar o modelo com a sequência de observação\n",
    "obs_seq = np.array(obs_seq).reshape(-1, 1)\n",
    "logprob, state_seq = model.decode(obs_seq, algorithm=\"viterbi\")\n",
    "\n",
    "# Mapeando os estados de volta para as etiquetas gramaticais\n",
    "state_map = {idx: state for idx, state in enumerate(states)}\n",
    "tagged_seq = [state_map[state] for state in state_seq]\n",
    "\n",
    "print(\"Observações:\", observations)\n",
    "print(\"Estados preditos:\", tagged_seq)\n",
    "```\n",
    "\n",
    "**Saída esperada:**\n",
    "```\n",
    "Observações: ['The', 'cat', 'sleeps']\n",
    "Estados preditos: ['DT', 'NN', 'VBZ']\n",
    "```\n",
    "\n",
    "Neste exemplo, usamos um HMM simples para etiquetar uma frase. O HMM é treinado com probabilidades de transição e emissão predefinidas e, em seguida, decodificamos a sequência de observações para encontrar a sequência mais provável de estados ocultos (etiquetas gramaticais)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d28b6",
   "metadata": {},
   "source": [
    "## Markov Chain vs Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10168536",
   "metadata": {},
   "source": [
    "Saiba mais em:\n",
    "https://stackoverflow.com/questions/10748426/what-is-the-difference-between-markov-chains-and-hidden-markov-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10828d99",
   "metadata": {},
   "source": [
    "Embora as **Cadeias de Markov** e os **Modelos de Markov Ocultos (HMMs)** compartilhem conceitos fundamentais, como estados e probabilidades de transição, eles são aplicados em diferentes contextos e têm diferenças importantes em termos de visibilidade dos estados e aplicação prática.\n",
    "\n",
    "### Cadeia de Markov - Definição\n",
    "Uma **Cadeia de Markov** é um modelo matemático que descreve um processo onde a probabilidade de transição para o próximo estado depende apenas do estado atual, e não dos estados anteriores (propriedade de Markov).\n",
    "\n",
    "1. **Estados Visíveis:** Todos os estados são diretamente observáveis.\n",
    "2. **Probabilidades de Transição:** Matriz de transição que define a probabilidade de passar de um estado para outro.\n",
    "3. **Propriedade de Markov:** A transição para o próximo estado depende apenas do estado atual.\n",
    "\n",
    "Imagine um sistema climático simplificado com três estados: ensolarado, nublado e chuvoso. A matriz de transição pode ser:\n",
    "\n",
    "|        | Ensolarado | Nublado | Chuvoso |\n",
    "|--------|------------|---------|---------|\n",
    "| **Ensolarado** | 0.8        | 0.15    | 0.05    |\n",
    "| **Nublado**    | 0.2        | 0.6     | 0.2     |\n",
    "| **Chuvoso**    | 0.25       | 0.25    | 0.5     |\n",
    "\n",
    "Se hoje está ensolarado, a probabilidade de amanhã também estar ensolarado é 0.8, nublado é 0.15, e chuvoso é 0.05.\n",
    "\n",
    "### Modelos de Markov Ocultos (Hidden Markov Models - HMMs) -  Definição\n",
    "Um **Modelo de Markov Oculto** é uma extensão das Cadeias de Markov, onde os estados não são diretamente observáveis. Em vez disso, observamos sinais ou evidências que são probabilisticamente dependentes dos estados ocultos.\n",
    "\n",
    "1. **Estados Ocultos:** Os estados não são diretamente observáveis.\n",
    "2. **Observações Visíveis:** O que observamos são sinais ou evidências que dependem dos estados ocultos.\n",
    "3. **Probabilidades de Transição:** Matriz de transição que define a probabilidade de passar de um estado oculto para outro.\n",
    "4. **Probabilidades de Emissão:** Matriz de emissão que define a probabilidade de uma observação ser gerada a partir de um estado oculto.\n",
    "\n",
    "No contexto de reconhecimento de fala, os estados ocultos podem ser fonemas, enquanto as observações são características extraídas do sinal de áudio.\n",
    "\n",
    "### Comparação Direta\n",
    "\n",
    "| Aspecto                   | Cadeia de Markov                  | Modelo de Markov Oculto (HMM)         |\n",
    "|---------------------------|-----------------------------------|---------------------------------------|\n",
    "| **Estados**               | Visíveis                          | Ocultos                               |\n",
    "| **Observações**           | Estados                           | Dependem probabilisticamente dos estados ocultos |\n",
    "| **Aplicação**             | Modelagem de processos observáveis diretamente | Modelagem de processos onde a sequência de estados não é diretamente observável |\n",
    "| **Exemplo**               | Clima (ensolarado, nublado, chuvoso) | Reconhecimento de fala (fonemas e áudio)           |\n",
    "\n",
    "### Aplicações em NLP - Cadeias de Markov\n",
    "As Cadeias de Markov podem ser usadas em tarefas onde a sequência de estados é observável e a propriedade de Markov (dependência do estado atual) se mantém. Por exemplo:\n",
    "- **Modelo de Texto:** Gerar texto baseado em bigramas ou trigrama onde cada estado é uma palavra.\n",
    "\n",
    "#### HMMs\n",
    "Os HMMs são aplicados em tarefas onde os estados são ocultos e apenas as observações estão disponíveis. Exemplos incluem:\n",
    "- **Part of Speech Tagging (POS Tagging):** Os estados ocultos são as etiquetas gramaticais, e as observações são as palavras.\n",
    "- **Reconhecimento de Fala:** Os estados ocultos são os fonemas ou palavras, e as observações são características do áudio.\n",
    "\n",
    "### Exemplos Práticos - Cadeia de Markov - Modelo de Texto\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "# Exemplo simples de Cadeia de Markov para geração de texto\n",
    "def generate_text(model, start_word, length=10):\n",
    "    current_word = start_word\n",
    "    result = [current_word]\n",
    "    \n",
    "    for _ in range(length - 1):\n",
    "        current_word = random.choices(list(model[current_word].keys()), list(model[current_word].values()))[0]\n",
    "        result.append(current_word)\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "# Modelo de transição (simplificado)\n",
    "markov_model = {\n",
    "    \"I\": {\"am\": 1.0},\n",
    "    \"am\": {\"a\": 0.5, \"happy\": 0.5},\n",
    "    \"a\": {\"student\": 1.0},\n",
    "    \"happy\": {\"student\": 1.0},\n",
    "    \"student\": {\"I\": 1.0}\n",
    "}\n",
    "\n",
    "# Gerar texto\n",
    "print(generate_text(markov_model, \"I\"))\n",
    "```\n",
    "\n",
    "#### HMM - POS Tagging com `hmmlearn`\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from hmmlearn import hmm\n",
    "\n",
    "# Definir os estados (etiquetas gramaticais)\n",
    "states = [\"DT\", \"NN\", \"VBZ\"]\n",
    "n_states = len(states)\n",
    "\n",
    "# Definir as observações (palavras)\n",
    "observations = [\"The\", \"cat\", \"sleeps\"]\n",
    "n_observations = len(observations)\n",
    "\n",
    "# Definir as sequências observadas em números\n",
    "obs_map = {word: idx for idx, word in enumerate(observations)}\n",
    "obs_seq = [obs_map[word] for word in observations]\n",
    "\n",
    "# Inicializar o modelo HMM\n",
    "model = hmm.MultinomialHMM(n_components=n_states, n_iter=100)\n",
    "\n",
    "# Probabilidades iniciais\n",
    "model.startprob_ = np.array([0.6, 0.3, 0.1])\n",
    "\n",
    "# Probabilidades de transição\n",
    "model.transmat_ = np.array([\n",
    "    [0.1, 0.8, 0.1],\n",
    "    [0.3, 0.4, 0.3],\n",
    "    [0.5, 0.2, 0.3]\n",
    "])\n",
    "\n",
    "# Probabilidades de emissão\n",
    "model.emissionprob_ = np.array([\n",
    "    [0.6, 0.3, 0.1],\n",
    "    [0.1, 0.7, 0.2],\n",
    "    [0.0, 0.0, 1.0]\n",
    "])\n",
    "\n",
    "# Ajustar o modelo com a sequência de observação\n",
    "obs_seq = np.array(obs_seq).reshape(-1, 1)\n",
    "logprob, state_seq = model.decode(obs_seq, algorithm=\"viterbi\")\n",
    "\n",
    "# Mapeando os estados de volta para as etiquetas gramaticais\n",
    "state_map = {idx: state for idx, state in enumerate(states)}\n",
    "tagged_seq = [state_map[state] for state in state_seq]\n",
    "\n",
    "print(\"Observações:\", observations)\n",
    "print(\"Estados preditos:\", tagged_seq)\n",
    "```\n",
    "\n",
    "Embora as Cadeias de Markov e os Modelos de Markov Ocultos compartilhem uma base teórica comum, eles diferem significativamente em suas aplicações e complexidade. As Cadeias de Markov são mais simples e usadas quando os estados são diretamente observáveis. Os HMMs são mais complexos e poderosos, permitindo modelar processos onde os estados não são diretamente observáveis, como em tarefas de NLP e reconhecimento de fala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5496e8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Calculating Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a396d3",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/calculating_prob1.png\">\n",
    "\n",
    "O número de vezes que o quadrado azul e roxo ocorrem juntos é 2/3. Usaremos a mesma lógica para popular as matrizes de transição e emissão. Numa matriz de transição, contaremos o número de vezes que a tag $t_{(i-1)}, t_{(i)}$ ocorrem juntos e dividiremos pelo total de vezes que $t_{(i-1)}$ ocorre (que é igual ao número de vezes que aparece seguido por qualquer outra coisa).\n",
    "\n",
    "<img src=\"./imgs/calculating_prob1.png\">\n",
    "\n",
    "$C(t_{(i-1)}, t_{(i)}$ é a quantidade de vezes que a tag $(i-1)$ ocorreu antes da $\\text{tag i}$. A partir disso podemos computar a probabilidade que a tag ocorre antes de outra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068e029",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Populating the Trasition and Emission Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8846af",
   "metadata": {},
   "source": [
    "Para popular a **matriz de transição**, temos que trackear a quantidade de vezes que uma tag aparece antes da outra tag.\n",
    "\n",
    "<img src=\"./imgs/transition_matrix1.png\">\n",
    "\n",
    "Na tabela acima, cada cor representa uma tag, e as linhas são os estados atuais e as colunas são so estado seguintes, sendo o laranja o estado inicial. O números na matriz correspondem a quantidade de vezes que a tag ocorre antes da outra.\n",
    "\n",
    "Para ir de O para NN, ou seja, para calcular $P(NN|O)$, temos que computar o seguinte:\n",
    "\n",
    "<img src=\"./imgs/transition_matrix2.png\">\n",
    "\n",
    "Para generalizar:\n",
    "\n",
    "$$ P(t_{i}|t_{i-1}) = \\frac{C(t_{i-1}, t_{i})}{\\sum^{N}_{j=1}C(t_{i-1}, t_{i})} $$\n",
    "\n",
    "Por vezes vai correr do estado atual e o seguinte ser zero, dando uma probabilidade de 0. Para resolver isso esse problema, podemos utilizar uma **suavização (smoothing)**, como a seguir.\n",
    "\n",
    "\n",
    "<img src=\"./imgs/transition_matrix3.png\">\n",
    "\n",
    "O $\\epsilon$ permite que não tenhamos nenhuma probabilidade zero. Mas, em aplicações reais, não aplicaríamos a suavização na primeira linha, por que, ao aplicar, iriamos per da frase começar com qualquer TAG de POS, incluindo pontuações.\n",
    "\n",
    "Para popular uma **matriz de emissão**, faremos de forma semelhante. Mas iremos quantificar as palavras associadas com suas tags POS.\n",
    "\n",
    "<img src=\"./imgs/emission_matrix1.png\">\n",
    "\n",
    "Para popular a matriz, também iremos usar a suavização, sendo:\n",
    "\n",
    "$$ P(w_{i}|t_{i}) = \\frac{C(t_{i}, w_{i}) + \\epsilon}{\\sum^{V}_{j=1} C(t_{i}, w_{j} + N * \\epsilon}$$\n",
    "\n",
    "$$ = \\frac{C(t_{i}, w_{i}) + \\epsilon}{C(t_{i}) + N * \\epsilon}$$\n",
    "\n",
    "Onde $C(t_{i}, w_{i})$ é a contagem associada a quantas vezes a tag $t_{i}$ está associada a palavra $w_{i}$. O epsilon acima é o caractere de suavização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9f74c",
   "metadata": {},
   "source": [
    "## Calculating the Trasition and Emission Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa09ba0",
   "metadata": {},
   "source": [
    "Para treinar um Hidden Markov Model (HMM) para tarefas como Part of Speech (POS) tagging, precisamos calcular as probabilidades das matrizes de transição e emissão com base em um corpus de texto anotado com etiquetas POS. Aqui estão os passos e exemplos detalhados para calcular essas probabilidades.\n",
    "\n",
    "### 1. Preparação do Corpus\n",
    "\n",
    "Primeiro, precisamos de um corpus anotado com etiquetas POS. Suponha que temos o seguinte corpus simples:\n",
    "\n",
    "```python\n",
    "corpus = [\n",
    "    [(\"The\", \"DT\"), (\"cat\", \"NN\"), (\"sleeps\", \"VBZ\")],\n",
    "    [(\"A\", \"DT\"), (\"dog\", \"NN\"), (\"barks\", \"VBZ\")],\n",
    "    [(\"The\", \"DT\"), (\"dog\", \"NN\"), (\"runs\", \"VBZ\")]\n",
    "]\n",
    "```\n",
    "\n",
    "### 2. Contagem de Transições e Emissões\n",
    "\n",
    "#### a. Contagem de Transições entre Tags POS\n",
    "\n",
    "Contamos como as etiquetas de POS transicionam de uma palavra para outra. Adicionamos um estado inicial fictício (`<s>`) para representar o início de uma frase.\n",
    "\n",
    "```python\n",
    "from collections import defaultdict\n",
    "\n",
    "# Contadores de transições e emissões\n",
    "transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "emission_counts = defaultdict(lambda: defaultdict(int))\n",
    "tag_counts = defaultdict(int)\n",
    "\n",
    "for sentence in corpus:\n",
    "    previous_tag = \"<s>\"\n",
    "    tag_counts[previous_tag] += 1\n",
    "    for word, tag in sentence:\n",
    "        # Contagem de transições\n",
    "        transition_counts[previous_tag][tag] += 1\n",
    "        \n",
    "        # Contagem de emissões\n",
    "        emission_counts[tag][word] += 1\n",
    "        \n",
    "        # Contagem de tags\n",
    "        tag_counts[tag] += 1\n",
    "        previous_tag = tag\n",
    "    transition_counts[previous_tag][\"</s>\"] += 1\n",
    "    tag_counts[\"</s>\"] += 1\n",
    "```\n",
    "\n",
    "#### b. Contagem de Emissões (palavra dada a tag)\n",
    "\n",
    "Cada vez que uma palavra ocorre com uma determinada etiqueta POS, incrementamos a contagem de emissão correspondente.\n",
    "\n",
    "### 3. Calculando as Probabilidades\n",
    "\n",
    "#### a. Probabilidades de Transição\n",
    "\n",
    "A probabilidade de transição de um estado $ s_i $ para um estado $ s_j $ é calculada como:\n",
    "\n",
    "$$ P(s_j \\mid s_i) = \\frac{\\text{contagem}(s_i \\rightarrow s_j)}{\\text{contagem}(s_i)} $$\n",
    "\n",
    "```python\n",
    "# Probabilidades de transição\n",
    "transition_prob = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for previous_tag in transition_counts:\n",
    "    total_transitions = sum(transition_counts[previous_tag].values())\n",
    "    for tag in transition_counts[previous_tag]:\n",
    "        transition_prob[previous_tag][tag] = transition_counts[previous_tag][tag] / total_transitions\n",
    "```\n",
    "\n",
    "#### b. Probabilidades de Emissão\n",
    "\n",
    "A probabilidade de emissão de uma palavra $ w $ dado um estado $ s $ é calculada como:\n",
    "\n",
    "$$ P(w \\mid s) = \\frac{\\text{contagem}(s \\rightarrow w)}{\\text{contagem}(s)} $$\n",
    "\n",
    "```python\n",
    "# Probabilidades de emissão\n",
    "emission_prob = defaultdict(lambda: defaultdict(float))\n",
    "\n",
    "for tag in emission_counts:\n",
    "    total_emissions = sum(emission_counts[tag].values())\n",
    "    for word in emission_counts[tag]:\n",
    "        emission_prob[tag][word] = emission_counts[tag][word] / total_emissions\n",
    "```\n",
    "\n",
    "### 4. Exemplos Práticos\n",
    "\n",
    "Vamos calcular as matrizes de transição e emissão usando os dados do corpus fornecido:\n",
    "\n",
    "#### Contagens\n",
    "\n",
    "Para o corpus fornecido, as contagens seriam:\n",
    "\n",
    "**Transições:**\n",
    "- $\"<s>\", \"DT\"$: 3\n",
    "- $\"DT\", \"NN\"$: 3\n",
    "- $\"NN\", \"VBZ\"$: 3\n",
    "- $\"VBZ\", \"</s>\"$: 3\n",
    "\n",
    "**Emissões:**\n",
    "- $\"DT\", \"The\"$: 2\n",
    "- $\"DT\", \"A\"$: 1\n",
    "- $\"NN\", \"cat\"$: 1\n",
    "- $\"NN\", \"dog\"$: 2\n",
    "- $\"VBZ\", \"sleeps\"$: 1\n",
    "- $\"VBZ\", \"barks\"$: 1\n",
    "- $\"VBZ\", \"runs\"$: 1\n",
    "\n",
    "#### Probabilidades de Transição\n",
    "\n",
    "```python\n",
    "# Exemplo de cálculo de probabilidades de transição\n",
    "transition_prob = {\n",
    "    \"<s>\": {\"DT\": 1.0},\n",
    "    \"DT\": {\"NN\": 1.0},\n",
    "    \"NN\": {\"VBZ\": 1.0},\n",
    "    \"VBZ\": {\"</s>\": 1.0}\n",
    "}\n",
    "```\n",
    "\n",
    "#### Probabilidades de Emissão\n",
    "\n",
    "```python\n",
    "# Exemplo de cálculo de probabilidades de emissão\n",
    "emission_prob = {\n",
    "    \"DT\": {\"The\": 2/3, \"A\": 1/3},\n",
    "    \"NN\": {\"cat\": 1/3, \"dog\": 2/3},\n",
    "    \"VBZ\": {\"sleeps\": 1/3, \"barks\": 1/3, \"runs\": 1/3}\n",
    "}\n",
    "```\n",
    "\n",
    "Este método de contagem e cálculo das probabilidades a partir de um corpus etiquetado é fundamental para treinar um HMM para tarefas de NLP, como POS tagging. Calculando as probabilidades de transição e emissão, podemos criar modelos que probabilisticamente decodificam sequências de observações em sequências de estados ocultos, fornecendo insights valiosos sobre a estrutura linguística dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d4eefd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## The Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31663c3",
   "metadata": {},
   "source": [
    "O Algoritmo de Viterbi é usado para encontrar a sequência mais provável de estados ocultos (etiquetas) dada uma sequência de observações (palavras ou sinais de áudio). Ele faz uso das probabilidades de transição e de emissão da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/viterbi1.png\">\n",
    "\n",
    "Para ir de $\\pi$ para $O$, precisamos multiplicar a probabilidade de transição correspondente (0.3) com a probabilidade de emissão correspondente (0.5), que resultará em 0.15. Isso é feito para todas as palavras da sequência, até termos a probabilidade da sequencia completa.\n",
    "\n",
    "<img src=\"./imgs/viterbi2.png\">\n",
    "\n",
    "O algoritmo Viterbi possiu os seguintes passos:\n",
    "\n",
    "1. **Inicialização:**\n",
    "   $$\n",
    "    V_1(i) = \\pi_i \\cdot B_i(O_1)\n",
    "   $$\n",
    "   \n",
    "    onde $ V_1(i) $ é a probabilidade do estado $ s_i $ ser o primeiro estado e $ O_1 $ é a primeira observação.\n",
    "\n",
    "2. **Recursão (Forward Pass):**\n",
    "   $$\n",
    "    V_t(j) = \\max_i (V_{t-1}(i) \\cdot A_{ij}) \\cdot B_j(O_t)\n",
    "   $$\n",
    "   \n",
    "    onde $ V_t(j) $ é a probabilidade máxima da sequência até o estado $ s_j $ no tempo $ t $.\n",
    "\n",
    "3. **Terminação (Backward Pass):**\n",
    "   $$\n",
    "    \\max_i V_T(i)\n",
    "   $$\n",
    "   \n",
    "    onde $ T $ é o tempo final.\n",
    "\n",
    "4. **Retropropagação:** Rastrear os estados que levaram à sequência de maior probabilidade."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b84d8c6",
   "metadata": {},
   "source": [
    "## Viterbi: Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9309e189",
   "metadata": {},
   "source": [
    "Para inicializar utlizamos duas matrizes auxiliares, C e D, a C é uma matriz da **Probabilidade de Emissão**, enquanto que a D representa a **Probabilidade de Transição**. Começando com a C, iremos popular essa matriz de dimensão (num_tags, num_words), esta matriz terá as probabilidades que lhe dirão a que parte do discurso cada palavra pertence.\n",
    "\n",
    "<img src=\"./imgs/viterbi3.png\">\n",
    "\n",
    "Agora, para popular a primeira coluna, iremos multiplicar o estado inicial $\\pi$, para cada tag, vezes $b_{i, \\text{cindex}(w1})$. Aqui o i corresponde a tag da distriguição inicial, e o $\\text{cindex}()w1$ é o índice da palavra 1 na matriz de emissão. Agora precisamos acompanhar de qual parte do discurso estamos vindo. Para isso, utilizamos a matrix auxiliar D, que permite armazenar os rótulos que representam os diferentes estados pelos quais você está passando ao encontrar a sequência mais provável de tags POS para uma determinada sequência de palavras $w1, ..., wk$. A princípio a primeira coluna é definida como 0, pois não vem de nenhuma tag POS.\n",
    "\n",
    "<img src=\"./imgs/viterbi4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cba90f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Viterbi: Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ed399",
   "metadata": {},
   "source": [
    "A etapa forward pass pode ser representada da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/viterbi5.png\">\n",
    "\n",
    "Começando do ultimo termo:\n",
    "- $b_{1, \\text{cindex(w2)}}$: Representa simplesmente a probabilidade de emissão da tag t1 para a palavra w2.\n",
    "- $a_{k, 1}$: É a probabilidade de transição da tag POS para a tag atual t1.\n",
    "- $c_{k,1}$: que representa a probabilidade do caminho anterior percorrido.\n",
    "- Escolhemos o k que maximiza a fórmula completa. Como ele não é um estado inicial, pode ser 1, 2 ou 3\n",
    "\n",
    "Para popular a célula 1,2 da imagem acima, devemos pegar o máximo de k-ésimas células na coluna anterior, vezes a probabilidade de transição correspondente do k-ésimo POS para o primeiro POS vezes a probabilidade de emissão do primeiro POS e a palavra atual que você está vendo. Podemos faz isso para todas as células. \n",
    "\n",
    "A regra geral é \n",
    "\n",
    "$$ c_{i,j} = \\max_k c_{k,j-1} * a_{k,i} * b_{i, \\text{cindex(wj)}} $$\n",
    "\n",
    "onde:\n",
    "- **$c_{i,j}$:** A probabilidade máxima da sequência de estados (etiquetas) que termina no estado $i$ na posição $j$. Representa a melhor pontuação (probabilidade) para alcançar o estado $i$ na posição $j$ da sequência de observações.\n",
    "- **$\\max_k$:** A operação de maximização sobre todos os possíveis estados $k$ na posição anterior $j-1$. Para encontrar a melhor sequência, o algoritmo de Viterbi considera todas as possíveis transições dos estados na posição anterior e escolhe a que maximiza a probabilidade.\n",
    "- **$c_{k,j-1}$:** A probabilidade máxima da sequência de estados que termina no estado $k$ na posição $j-1$. Esta é a pontuação (probabilidade) acumulada até o estado $k$ na posição $j-1$. Ela representa o caminho mais provável até aquele ponto na sequência.\n",
    "- **$a_{k,i}$:** A probabilidade de transição do estado $k$ para o estado $i$. Esta é a probabilidade de que a sequência de estados transite do estado $k$ na posição $j-1$ para o estado $i$ na posição $j$.\n",
    "- **$b_{i, \\text{cindex}(w_j)}$:** A probabilidade de emissão do estado $i$ observando a palavra $w_j$.  Esta é a probabilidade de que o estado $i$ emita a palavra $w_j$ na posição $j$. A função $\\text{cindex}(w_j)$ mapeia a palavra $w_j$ para o índice apropriado na matriz de emissões.\n",
    "- **$\\text{cindex}(w_j)$:** Um mapeamento da palavra $w_j$ para um índice usado na matriz de emissões. Essa função converte a palavra observada $w_j$ para um índice que pode ser usado para acessar a matriz de emissões $b_{i,\\text{cindex}(w_j)}$.\n",
    "\n",
    "O objetivo é encontrar a sequência de estados que maximiza a probabilidade de observar a sequência dada. Para fazer isso, calculamos a probabilidade máxima de terminar em cada estado em cada posição da sequência, considerando todas as possíveis transições e emissões.\n",
    "\n",
    "Imagine que temos uma sequência de palavras \"o gato mia\" e queremos encontrar a sequência de etiquetas de POS mais provável usando Viterbi.\n",
    "\n",
    "- **Posição 1: \"o\"**\n",
    "  - Possíveis estados: $ DT $ (determinante)\n",
    "  - $ c_{\\text{DT}, 1} = a_{\\text{start}, \\text{DT}} \\cdot b_{\\text{DT}, \\text{cindex}(\"o\")} $\n",
    "  - Inicialmente, $ c_{\\text{DT}, 1} $ seria a probabilidade de começar com $ DT $ vezes a probabilidade de $ DT $ emitir \"o\".\n",
    "\n",
    "- **Posição 2: \"gato\"**\n",
    "  - Possíveis estados: $ NN $ (substantivo)\n",
    "  - Consideramos todas as transições possíveis de etiquetas anteriores para $ NN $:\n",
    "    - $ c_{\\text{NN}, 2} = \\max \\left( c_{\\text{DT}, 1} \\cdot a_{\\text{DT}, \\text{NN}} \\cdot b_{\\text{NN}, \\text{cindex}(\"gato\")} \\right) $\n",
    "  - Para calcular $ c_{\\text{NN}, 2} $, verificamos a probabilidade acumulada de estar em $ DT $ na posição 1 ($ c_{\\text{DT}, 1} $), a probabilidade de transição de $ DT $ para $ NN $ ($ a_{\\text{DT}, \\text{NN}} $) e a probabilidade de $ NN $ emitir \"gato\" ($ b_{\\text{NN}, \\text{cindex}(\"gato\")} $).\n",
    "\n",
    "- **Posição 3: \"mia\"**\n",
    "  - Possíveis estados: $ VBZ $ (verbo presente)\n",
    "  - Consideramos todas as transições possíveis de etiquetas anteriores para $ VBZ $:\n",
    "    - $ c_{\\text{VBZ}, 3} = \\max \\left( c_{\\text{NN}, 2} \\cdot a_{\\text{NN}, \\text{VBZ}} \\cdot b_{\\text{VBZ}, \\text{cindex}(\"mia\")} \\right) $\n",
    "  - Para calcular $ c_{\\text{VBZ}, 3} $, verificamos a probabilidade acumulada de estar em $ NN $ na posição 2 ($ c_{\\text{NN}, 2} $), a probabilidade de transição de $ NN $ para $ VBZ $ ($ a_{\\text{NN}, \\text{VBZ}} $) e a probabilidade de $ VBZ $ emitir \"mia\" ($ b_{\\text{VBZ}, \\text{cindex}(\"mia\")} $).\n",
    "\n",
    "A fórmula $ c_{i,j} = \\max_k \\left( c_{k,j-1} \\cdot a_{k,i} \\cdot b_{i, \\text{cindex}(w_j)} \\right) $ captura a ideia de encontrar a sequência de estados mais provável, acumulando as melhores pontuações (probabilidades) de transições e emissões em cada passo. Em cada posição $j$, ela considera todas as possíveis transições dos estados na posição anterior $j-1$, escolhe a melhor e multiplica pelas probabilidades relevantes para obter a maior pontuação possível para estar no estado $i$ na posição $j$.\n",
    "\n",
    "Agora, para preencher a matriz D, você acompanhará o argmax de onde você veio da seguinte maneira:\n",
    "\n",
    "<img src=\"./imgs/viterbi6.png\">\n",
    "\n",
    "Para o $d_{i, j}$, temos que:\n",
    "\n",
    "$$ d_{i,j} = \\text{argmax}_{k} c_{k,j-1} * a_{k,i} * b_{i, \\text{cindex(wj)}} $$\n",
    "\n",
    "Em cada di,j, você simplesmente salvamos o k que maximiza a entrada e ci,j. Aqui, existem três estados que não são o estado inicial. Então, k é um, dois ou três. Nessa função argmax, retorna o k que maximiza as funções do argumento em vez do valor máximo.\n",
    "\n",
    "**A matriz $D$ (ou matriz de rastreamento) é usada para armazenar os índices dos estados predecessores que maximizaram as probabilidades acumuladas durante o forward pass. Isso é essencial para reconstruir a sequência de estados ótima durante o backward pass.**\n",
    "\n",
    "Observe que a única diferença entre $c_ij$ e $d_ij$, é que no primeiro você calcula a probabilidade e no último  acompanhamos o índice da linha de onde essa probabilidade veio. Então acompanhamos qual k foi usado para obter essa probabilidade máxima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8c3075",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Viterbi: Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca510e46",
   "metadata": {},
   "source": [
    "Agora, podemos agrupar tudo e construir o path que nos trará a POS tag da sentença.\n",
    "\n",
    "<img src=\"./imgs/viterbi7.png\">\n",
    "\n",
    "A equação acima fornece apenas o índice da linha mais alta na última coluna de C. Depois de fazer isso, podemos prosseguir e começar a usar a matriz D da seguinte maneira:\n",
    "\n",
    "<img src=\"./imgs/viterbi8.png\">\n",
    "\n",
    "Observe que, como começamos no índice um, a última palavra $(w5​)$ é $t1$​ . Então vamos para a primeira linha de D e qualquer que seja esse número, ele indica a linha da próxima tag de classe gramatical. Em seguida, a tag da próxima classe gramatical indica a linha da próxima e assim por diante. Isso nos permite reconstruir as tags POS da sua frase.\n",
    "\n",
    "**Inicialização:** Começa pelo estado final da sequência (última posição de palavras observadas).\n",
    "Encontra o estado (tag de POS) que tem a maior probabilidade final.\n",
    "\n",
    "**Rastreamento Retroativo:** A partir do estado final (última posição j), usa-se a matriz D para retroceder até o estado inicial (primeira posição j). Para cada posição j, D nos diz qual foi o estado predecessor que levou ao estado atual naquela posição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f6d96e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:58:13.935840Z",
     "start_time": "2024-06-26T16:58:13.920057Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequência ótima de estados: [1, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "# Exemplo simplificado para ilustração\n",
    "def viterbi_backward_pass(C, D):\n",
    "    num_states, num_positions = C.shape\n",
    "    best_sequence = []\n",
    "\n",
    "    # Encontra o estado com a maior pontuação na última posição\n",
    "    best_last_state = np.argmax(C[:, -1])\n",
    "    best_sequence.append(best_last_state)\n",
    "\n",
    "    # Retrocede usando a matriz D para reconstruir a sequência ótima\n",
    "    for j in range(num_positions - 1, 0, -1):\n",
    "        predecessor_state = D[best_last_state, j]\n",
    "        best_sequence.append(predecessor_state)\n",
    "        best_last_state = predecessor_state\n",
    "\n",
    "    # Inverte a sequência para obter a ordem correta (do início ao fim)\n",
    "    best_sequence.reverse()\n",
    "\n",
    "    return best_sequence\n",
    "\n",
    "# Exemplo de uso\n",
    "C = np.array([[0.1, 0.3, 0.4],\n",
    "              [0.2, 0.5, 0.7],\n",
    "              [0.4, 0.6, 0.8]])\n",
    "\n",
    "D = np.array([[-1, 0, 1],\n",
    "              [-1, 0, 1],\n",
    "              [-1, 1, 2]])\n",
    "\n",
    "best_sequence = viterbi_backward_pass(C, D)\n",
    "print(\"Sequência ótima de estados:\", best_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cce488",
   "metadata": {},
   "source": [
    "# Week 3 - Autocomplete and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b67e218",
   "metadata": {},
   "source": [
    "## N-grams overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbd49dd",
   "metadata": {},
   "source": [
    "**N-grams** são sequências contínuas de $ n $ itens (ou palavras) extraídas de um dado texto ou fala. Eles são usados em processamento de linguagem natural (NLP) e outras áreas para modelar a probabilidade de uma palavra dada a sequência de palavras anteriores.\n",
    "\n",
    "N-grams são amplamente utilizados em modelos de linguagens. Modelos de linguagem são uma ferramenta que **estima a probabilidade de uma sequência de palavras**. Ao utilizar N-grams, podemos modelar a probabilidade de de certas palavras ocorrerem em uma sequencia espececífica. Algumas aplicações do uso de n-grams incluem sistemas de autocomplete, speech recognition, spelling correction, augmentative communication, entre outros.\n",
    "\n",
    "- **N**: O número de itens na sequência.\n",
    "  - **Unigrama (1-gram)**: Sequência de 1 item.\n",
    "  - **Bigramas (2-gram)**: Sequência de 2 itens.\n",
    "  - **Trigramas (3-gram)**: Sequência de 3 itens.\n",
    "  - E assim por diante.\n",
    "  \n",
    "<img src=\"./imgs/n-grams.png\">  \n",
    "  \n",
    "\n",
    "Considere a frase: \"Eu gosto de aprender\".\n",
    "\n",
    "- **Unigrama (1-gram)**:\n",
    "  - \"Eu\", \"gosto\", \"de\", \"aprender\"\n",
    "\n",
    "- **Bigramas (2-gram)**:\n",
    "  - \"Eu gosto\", \"gosto de\", \"de aprender\"\n",
    "\n",
    "- **Trigramas (3-gram)**:\n",
    "  - \"Eu gosto de\", \"gosto de aprender\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa176f7",
   "metadata": {},
   "source": [
    "## N-grams Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4983677",
   "metadata": {},
   "source": [
    "Para um modelo baseado em unigrams, a probabilidade se dá por:\n",
    "\n",
    "$$ P(w) = \\frac{C(w)}{m} $$\n",
    "\n",
    "Onde:\n",
    "- C = Contagem\n",
    "- m: Tamanho do corpus\n",
    "- w: Frequencia de ocorrencia da palavra\n",
    "\n",
    "Para um modelo baseado em bigramas, a probabilidade de uma palavra $ w_n $ dado $ w_{n-1} $ é calculada como:\n",
    "\n",
    "$$\n",
    "P(w_n | w_{n-1}) = \\frac{\\text{C}(w_{n-1}, w_n)}{\\text{C}(w_{n-1})}\n",
    "$$\n",
    "\n",
    "Podemos generalizar a expressão para:\n",
    "\n",
    "$$ P(y|x) = \\frac{C(x, y)}{\\sum_{w} C(x, w)} = \\frac{C(x, y)}{C(x)}$$\n",
    "\n",
    "Para um modelo baseado em trigramas:\n",
    "\n",
    "$$\n",
    "P(w_3 | w^{2}_{1}) = \\frac{\\text{C}(w^{2}_{1}, w_3)}{\\text{C}(w^{2}_{1})}\n",
    "$$\n",
    "\n",
    "Enfim, podemos generalizar para tamanhos maiores que trigramas, sendo:\n",
    "\n",
    "$$\n",
    "P(w_N | w^{N-1}_{1}) = \\frac{\\text{C}(w^{N-1}_{1}, w_N)}{\\text{C}(w^{N-1}_{1})}\n",
    "$$\n",
    "\n",
    "\n",
    "Vamos supor que temos o seguinte corpus: \"Eu gosto de aprender, gosto de ensinar\".\n",
    "\n",
    "- Contagem de bigramas:\n",
    "  - \"Eu gosto\": 1\n",
    "  - \"gosto de\": 2\n",
    "  - \"de aprender\": 1\n",
    "  - \"de ensinar\": 1\n",
    "\n",
    "- Contagem de unigramas:\n",
    "  - \"Eu\": 1\n",
    "  - \"gosto\": 2\n",
    "  - \"de\": 2\n",
    "  - \"aprender\": 1\n",
    "  - \"ensinar\": 1\n",
    "\n",
    "Para calcular a probabilidade do bigrama \"gosto de\":\n",
    "$$\n",
    "P(\\text{de} | \\text{gosto}) = \\frac{\\text{C}(\\text{gosto de})}{\\text{C}(\\text{gosto})} = \\frac{2}{2} = 1\n",
    "$$\n",
    "\n",
    "Esses modelos possuem algumas vantagens e limitações, como:\n",
    "\n",
    "**Vantagens**:\n",
    "- **Simples de Implementar**: Fácil de construir e entender.\n",
    "- **Eficiência Computacional**: Menos complexo que modelos mais avançados.\n",
    "\n",
    "**Limitações**\n",
    "- **Dependência de Contexto**: Os N-grams limitam-se ao contexto imediato e podem não capturar dependências de longo alcance.\n",
    "- **Explosão Combinatória**: O número de N-grams cresce rapidamente com o aumento de $ n $, tornando difícil a modelagem para grandes valores de $ n $."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af12674",
   "metadata": {},
   "source": [
    "## Sequence Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868360e1",
   "metadata": {},
   "source": [
    "Para calcular a probabilidade de uma sentença, precisamos calcular a probabilidade de todas as palavras da sequencia, como:\n",
    "\n",
    "$$ P(B \\mid A) = \\frac{P(A, B)}{P(A)} \\implies P(A, B) = P(A) P(B \\mid A) $$\n",
    "\n",
    "$$ P(A, B, C, D) = P(A) P(B \\mid A) P(C \\mid A, B) P(D \\mid A, B, C) $$\n",
    "\n",
    "Para calcular a probabilidade de uma sequência, podemos calcular o seguinte:\n",
    "\n",
    "$$ P(\\text{the teacher drinks tea}) = P(\\text{the}) P(\\text{teacher} \\mid \\text{the}) P(\\text{drinks} \\mid \\text{the teacher}) P(\\text{tea} \\mid \\text{the teacher drinks}) $$\n",
    "\n",
    "Um dos principais problemas com o cálculo das probabilidades acima é que **o corpus raramente contém exatamente as mesmas frases nas quais você calculou suas probabilidades**. Conseqüentemente, podemos facilmente obter uma probabilidade de 0. A **suposição de Markov** indica que **apenas a última palavra importa**, assim, estamos criando uma aproximação da probabilidade da sequência. Por isso:\n",
    "\n",
    "$$ Bigram: P(w_n \\mid w_1^{n-1}) \\approx P(w_n \\mid w_{n-1}) $$\n",
    "\n",
    "$$ N-gram: P(w_n \\mid w_1^{n-1}) \\approx P(w_n \\mid w_{n-N+1}^{n-1}) $$\n",
    "\n",
    "Podemos modelar a frase inteira da seguinte maneira:\n",
    "\n",
    "$$ P(w_1^n) \\approx \\prod_{i=1}^{n} P(w_i \\mid w_{i-1}) $$\n",
    "$$ = $$\n",
    "$$ P(w_1^n) \\approx P(w_1) P(w_2 \\mid w_1) \\cdots P(w_n \\mid w_{n-1}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704da0d2",
   "metadata": {},
   "source": [
    "## Starting and Ending Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba054f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T19:31:20.795525Z",
     "start_time": "2024-06-21T19:31:20.787353Z"
    }
   },
   "source": [
    "### Utilizando N-grams com Caracteres Especiais `<s>` e `</s>`\n",
    "\n",
    "Os caracteres especiais `<s>` e `</s>` são frequentemente usados em modelos de N-grams para indicar o início e o fim de uma frase, respectivamente. Isso ajuda a capturar contextos específicos do início e do fim das frases, melhorando a modelagem de linguagem.\n",
    "\n",
    "Adicionar esses caracteres especiais permite que o modelo aprenda a transição de \"nada\" para a primeira palavra e da última palavra para \"nada\", o que é útil para prever o início e o fim das frases corretamente.\n",
    "\n",
    "Vamos considerar a frase \"eu gosto de aprender\". \n",
    "\n",
    "Simplesmente as palavras incluindo os marcadores:\n",
    "\n",
    "`<s>, eu, gosto, de, aprender, </s>`\n",
    "\n",
    "Inclui pares de palavras adjacentes:\n",
    "\n",
    "`<s> eu, eu gosto, gosto de, de aprender, aprender </s>`\n",
    "\n",
    "Inclui trios de palavras adjacentes:\n",
    "\n",
    "`<s> eu gosto, eu gosto de, gosto de aprender, de aprender </s>`\n",
    "\n",
    "<img src=\"./imgs/start_end_sentence.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b0de89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-21T19:34:32.514391Z",
     "start_time": "2024-06-21T19:34:32.508832Z"
    }
   },
   "source": [
    "Exemplo:\n",
    "\n",
    "```python\n",
    "def generate_ngrams(sentence, n):\n",
    "    # Adicionar os caracteres especiais\n",
    "    tokens = ['<s>'] + sentence.split() + ['</s>']\n",
    "    \n",
    "    # Gerar N-grams\n",
    "    ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "    \n",
    "    return ngrams\n",
    "\n",
    "# Exemplo de frase\n",
    "sentence = \"eu gosto de aprender\"\n",
    "\n",
    "# Gerar bigramas (2-grams)\n",
    "bigrams = generate_ngrams(sentence, 2)\n",
    "print(\"Bigramas:\", bigrams)\n",
    "\n",
    "# Gerar trigramas (3-grams)\n",
    "trigrams = generate_ngrams(sentence, 3)\n",
    "print(\"Trigramas:\", trigrams)\n",
    "```\n",
    "\n",
    "Saída Esperada para a frase \"eu gosto de aprender\":\n",
    "\n",
    "- **Bigramas:**\n",
    "\n",
    "`[('<s>', 'eu'), ('eu', 'gosto'), ('gosto', 'de'), ('de', 'aprender'), ('aprender', '</s>')]`\n",
    "\n",
    "- **Trigramas:**\n",
    "\n",
    "`[('<s>', 'eu', 'gosto'), ('eu', 'gosto', 'de'), ('gosto', 'de', 'aprender'), ('de', 'aprender', '</s>')]`\n",
    "\n",
    "**Aplicações*:*\n",
    "1. **Modelagem de Linguagem**: Melhorar a predição de palavras no início e fim das frases.\n",
    "2. **Reconhecimento de Fala**: Ajuda a identificar corretamente o começo e o fim de frases faladas.\n",
    "3. **Tradução Automática**: Captura a estrutura completa das frases para melhor tradução.\n",
    "4. **Análise de Textos**: Melhora a segmentação e a análise de sentenças.\n",
    "\n",
    "O uso de caracteres especiais `<s>` e `</s>` em N-grams é uma técnica poderosa para capturar a estrutura das frases, especialmente no início e no fim. Isso melhora a precisão dos modelos de linguagem e tem ampla aplicação em várias áreas de processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0c0e01",
   "metadata": {},
   "source": [
    "## N-gram Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee56bbb",
   "metadata": {},
   "source": [
    "Para criar um modelo de linguagem utilizando n-gram, precisamos processar o corpus para criar uma **matriz de contagem** e uma **matriz de probabilidade**. A matriz de probabilidade terá a informação sobre a probabilidade condicional dos n-grams, e, com ela, vamos relacionar a matriz de probabilidade ao modelo de linguagem.\n",
    "\n",
    "Na matriz de contagem as linhas correspondem ao corpus único N-1 grams. E as colunas correspondem a unica palavra do corpus. Vide o exemplo abaixo:\n",
    "\n",
    "<img src=\"./imgs/n_gram_cmatrix.png\">\n",
    "\n",
    "Para criar a matriz de probabilidade, podemos utilizar a seguinte fórmula\n",
    "\n",
    "$$\n",
    "P(w_n | w^{n-1}_{n-N+1}) = \\frac{\\text{C}(w^{n-1}_{n-N+1}, w_n)}{\\text{C}(w^{n-1}_{n-N+1})}\n",
    "$$\n",
    "\n",
    "$$ \\text{sum(row)} = \\sum_{w\\in{V}}\\text{C}(w^{n-1}_{1}, w) = \\text{C}(w^{n-1}_{n-N+1})$$\n",
    "\n",
    "Agora, dado a matriz de probabilidade, podemos gerar o modelo de linguagem. Podemos computar a probabilidade da sentença e a previsão da palavra seguinte. PAra computar a probabilidade da sequência, precisamos aplicar a seguinte fórmula:\n",
    "\n",
    "$$ P(w_1^n) \\approx \\prod_{i=1}^n P(w_i \\mid w_{i-1}) $$\n",
    "\n",
    "Para evitar _underflow_, já que estamos multiplicando vários números pequenos, podemos multiplicar pelo log:\n",
    "\n",
    "$$ \\log(P(w_1^n)) \\approx \\sum_{i=1}^n \\log(P(w_i \\mid w_{i-1})) $$\n",
    "\n",
    "E, então criando o modelo generativo:\n",
    "\n",
    "<img src=\"./imgs/n_gram_model.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b232f",
   "metadata": {},
   "source": [
    "## Language Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a76478",
   "metadata": {},
   "source": [
    "Para avaliar um modelo, utilizamos a divisão treino, validação e teste. Para corporas menores a proporção geralmente é de 80%, 10% e 10%. Já para corporas maiores é de 98%, 1% e 1%. Podemos dividir de forma contínua ou aleatória.\n",
    "\n",
    "<img src=\"./imgs/splitting_text.png\">\n",
    "\n",
    "**Perplexity** é a métrica utilizada para avaliação, e que mensura a complexicidade em uma amostra de texto. Um texto escrito por humanos é mais provável de ter um score de perplexity menor, logo, quanto menor o score melhor o desempenho. Ela se dá por:\n",
    "\n",
    "$$ PP(W) = P(S_1, S_2, .., S_m)^{-\\frac{1}{m}}$$\n",
    "onde:\n",
    "- W: É o conjunto de teste contendo m sentences s\n",
    "- Si: É a iésima sentença no conjunto de teste, cada uma acabando com $</s>$\n",
    "- m: É a quantidade de todas as palavras em todo o conjunto de teste W, incluindo $</s>$, mas não inclui $<s>$\n",
    "\n",
    "$$ PP(W) = \\sqrt[m]{\\prod_{i=1}^m \\prod_{j=1}^{|s_i|} \\frac{1}{P(w_j^{(i)} \\mid w_{j-1}^{(i)})}} $$\n",
    "\n",
    "onde:\n",
    "- **Perplexidade (PP)**: Representa a perplexidade da sequência de palavras $W$. A perplexidade é uma métrica usada para avaliar a qualidade de um modelo de linguagem. Quanto menor a perplexidade, melhor o modelo em prever uma sequência de palavras.\n",
    "\n",
    "- **Raiz m-ésima ($\\sqrt[m]{\\cdot}$)**: A raiz m-ésima normaliza o produto total para o número de sentenças no conjunto $W$. Aqui, $m$ é o número total de sentenças em $W$.\n",
    "\n",
    "- **Produto Externo ($\\prod_{i=1}^m$)**: Este produto externo itera sobre todas as sentenças no conjunto $W$. $i$ varia de 1 a $m$, onde $m$ é o número total de sentenças.\n",
    "\n",
    "- **Produto Interno ($\\prod_{j=1}^{|s_i|}$)**: Este produto interno itera sobre todas as palavras em uma sentença específica $s_i$. $j$ varia de 1 a $|s_i|$, onde $|s_i|$ é o número de palavras na sentença $s_i$.\n",
    "\n",
    "- **Probabilidade Condicional ($P(w_j^{(i)} \\mid w_{j-1}^{(i)})$)**: Representa a probabilidade de uma palavra $w_j^{(i)}$ dada a palavra anterior $w_{j-1}^{(i)}$ na mesma sentença $s_i$. Esta probabilidade é fornecida pelo modelo de linguagem.\n",
    "\n",
    "- **Recíproco da Probabilidade ($\\frac{1}{P(w_j^{(i)} \\mid w_{j-1}^{(i)})}$)**: O recíproco da probabilidade é usado aqui porque a perplexidade é uma medida inversa da probabilidade. Alta probabilidade implica baixa perplexidade, e vice-versa. Ao tomar o produto dos recíprocos, estamos essencialmente medindo a \"dificuldade\" do modelo em prever a sequência.\n",
    "\n",
    "- $w^{(i)}_j$: O j corresponde a j-ésima palavra da i-ésima sentença. Se concatenarmos todas as sentenças, então $w_i$ é a i-ésima palavra no conjunto de teste.\n",
    "\n",
    "A perplexidade é basicamente a probabilidade inversa do conjunto de testes normalizada pelo número de palavras no conjunto de testes. Portanto, quanto maior o modelo de linguagem estimar a probabilidade do seu conjunto de testes, menor será a perplexidade. \n",
    "\n",
    "Para calcular o log perplexity, utilizamos:\n",
    "\n",
    "$$ PP(W) = \\frac{-1}{m} \\sum^{m}_{i=1}\\log_2 (P(w_i | wi-1))$$\n",
    "\n",
    "Um bom modelo possui o score entre 16 e 20, já o log perplexity entre 4.3 e 5.9\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406b982",
   "metadata": {},
   "source": [
    "### Intuição por Trás da Fórmula\n",
    "\n",
    "A perplexidade mede o quão bem um modelo de linguagem prevê uma sequência de palavras. Aqui está a interpretação intuitiva dos passos:\n",
    "\n",
    "1. **Produto Interno**: Para cada sentença $s_i$, calcule o produto dos recíprocos das probabilidades condicionais das palavras. Isso mede a \"dificuldade\" do modelo em prever cada palavra na sentença.\n",
    "2. **Produto Externo**: Multiplique esses produtos para todas as sentenças. Isso agrega a \"dificuldade\" de todas as sentenças.\n",
    "3. **Raiz m-ésima**: Normalize o produto total pelo número de sentenças $m$. Isso garante que a perplexidade não dependa do número de sentenças.\n",
    "\n",
    "Considere um conjunto $W$ com duas sentenças:\n",
    "\n",
    "1. Sentença 1: \"eu gosto\"\n",
    "2. Sentença 2: \"de aprender\"\n",
    "\n",
    "Suponha que o modelo de linguagem forneça as seguintes probabilidades condicionais:\n",
    "\n",
    "- $P(\\text{eu} \\mid \\text{<s>}) = 0.5$\n",
    "- $P(\\text{gosto} \\mid \\text{eu}) = 0.4$\n",
    "- $P(\\text{de} \\mid \\text{<s>}) = 0.3$\n",
    "- $P(\\text{aprender} \\mid \\text{de}) = 0.6$\n",
    "\n",
    "A perplexidade seria calculada como:\n",
    "\n",
    "$$ PP(W) = \\sqrt[2]{\\left( \\frac{1}{0.5} \\cdot \\frac{1}{0.4} \\right) \\times \\left( \\frac{1}{0.3} \\cdot \\frac{1}{0.6} \\right)} = \\sqrt[2]{(2 \\times 2.5) \\times (3.33 \\times 1.67)} $$\n",
    "\n",
    "O resultado da fórmula dará uma medida de perplexidade que indica quão bem o modelo de linguagem está prevendo o conjunto de sentenças $W$.\n",
    "\n",
    "\n",
    "### Intuição por Trás da Fórmula Perplexidade Logarítmica\n",
    "\n",
    "A fórmula da perplexidade logarítmica é:\n",
    "\n",
    "Vamos criar um exemplo usando a Fórmula 1 para calcular a perplexidade de uma sequência de palavras. \n",
    "\n",
    "$$ PP(W) = \\frac{-1}{m} \\sum_{i=1}^m \\log_2 (P(w_i \\mid w_{i-1})) $$\n",
    "\n",
    "\n",
    "Suponha que temos uma sequência de palavras: \"the cat sat\".\n",
    "\n",
    "Probabilidades condicionais fornecidas pelo modelo:\n",
    "- $P(\\text{the} \\mid <s>) = 0.5$\n",
    "- $P(\\text{cat} \\mid \\text{the}) = 0.3$\n",
    "- $P(\\text{sat} \\mid \\text{cat}) = 0.4$\n",
    "\n",
    "Aqui, $m$ é o número de palavras na sequência, que é 3 (\"the\", \"cat\", \"sat\").\n",
    "\n",
    "Calculando o valor de cada termo:\n",
    "1. $\\log_2(P(\\text{the} \\mid <s>)) = \\log_2(0.5)$\n",
    "2. $\\log_2(P(\\text{cat} \\mid \\text{the})) = \\log_2(0.3)$\n",
    "3. $\\log_2(P(\\text{sat} \\mid \\text{cat})) = \\log_2(0.4)$\n",
    "\n",
    "Vamos calcular cada um desses logaritmos:\n",
    "\n",
    "$$\n",
    "\\log_2(0.5) = -1\n",
    "$$\n",
    "$$\n",
    "\\log_2(0.3) \\approx -1.737\n",
    "$$\n",
    "$$\n",
    "\\log_2(0.4) \\approx -1.322\n",
    "$$\n",
    "\n",
    "Soma dos logaritmos:\n",
    "$$\n",
    "\\sum_{i=1}^3 \\log_2(P(w_i \\mid w_{i-1})) = -1 + (-1.737) + (-1.322) = -4.059\n",
    "$$\n",
    "\n",
    "Média dos logaritmos:\n",
    "$$\n",
    "\\frac{-1}{3} (-4.059) = 1.353\n",
    "$$\n",
    "\n",
    "A perplexidade é dada pela exponenciação com base 2 do valor encontrado:\n",
    "\n",
    "$$\n",
    "PP(W) = 2^{1.353} \\approx 2.55\n",
    "$$\n",
    "\n",
    "Portanto, a perplexidade da sequência \"the cat sat\" é aproximadamente 2.55.\n",
    "\n",
    "Este valor significa que, em média, o modelo de linguagem é tão \"surpreso\" por cada palavra na sequência como se houvesse cerca de 2.55 palavras possíveis em cada ponto. Uma perplexidade mais baixa indica um modelo melhor, pois implica que o modelo é menos \"surpreso\" pelas palavras da sequência."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f96c4a",
   "metadata": {},
   "source": [
    "### Intuição por Trás da Fórmula\n",
    "\n",
    "A perplexidade mede o quão bem um modelo de linguagem prevê uma sequência de palavras. Aqui está a interpretação intuitiva dos passos:\n",
    "\n",
    "1. **Produto Interno**: Para cada sentença $s_i$, calcule o produto dos recíprocos das probabilidades condicionais das palavras. Isso mede a \"dificuldade\" do modelo em prever cada palavra na sentença.\n",
    "2. **Produto Externo**: Multiplique esses produtos para todas as sentenças. Isso agrega a \"dificuldade\" de todas as sentenças.\n",
    "3. **Raiz m-ésima**: Normalize o produto total pelo número de sentenças $m$. Isso garante que a perplexidade não dependa do número de sentenças.\n",
    "\n",
    "Considere um conjunto $W$ com duas sentenças:\n",
    "\n",
    "1. Sentença 1: $\\text{\"<s> eu gosto </s>\"}$\n",
    "2. Sentença 2: $\\text{\"<s> de aprender </s>\"}$\n",
    "\n",
    "Suponha que o modelo de linguagem forneça as seguintes probabilidades condicionais:\n",
    "\n",
    "- $P(\\text{eu} \\mid \\text{<s>}) = 0.5$\n",
    "- $P(\\text{gosto} \\mid \\text{eu}) = 0.4$\n",
    "- $P(\\text{</s>} \\mid \\text{gosto}) = 0.8$\n",
    "- $P(\\text{de} \\mid \\text{<s>}) = 0.3$\n",
    "- $P(\\text{aprender} \\mid \\text{de}) = 0.6$\n",
    "- $P(\\text{</s>} \\mid \\text{aprender}) = 0.7$\n",
    "\n",
    "A perplexidade seria calculada como:\n",
    "\n",
    "$$ PP(W) = \\sqrt[2]{\\left( \\frac{1}{0.5} \\cdot \\frac{1}{0.4} \\cdot \\frac{1}{0.8} \\right) \\times \\left( \\frac{1}{0.3} \\cdot \\frac{1}{0.6} \\cdot \\frac{1}{0.7} \\right)} = \\sqrt[2]{(2 \\cdot 2.5 \\cdot 1.25) \\times (3.33 \\cdot 1.67 \\cdot 1.43)} $$\n",
    "\n",
    "O resultado da fórmula dará uma medida de perplexidade que indica quão bem o modelo de linguagem está prevendo o conjunto de sentenças $W$.\n",
    "\n",
    "### Intuição por Trás da Fórmula Perplexidade Logarítmica\n",
    "\n",
    "A fórmula da perplexidade logarítmica é:\n",
    "\n",
    "Vamos criar um exemplo usando a nova fórmula para calcular a perplexidade de uma sequência de palavras.\n",
    "\n",
    "$$ PP(W) = \\frac{-1}{m} \\sum_{i=1}^m \\log_2 (P(w_i \\mid w_{i-1})) $$\n",
    "\n",
    "Suponha que temos a sequência de palavras: $\"<s> the cat sat </s>\"$\n",
    "\n",
    "Probabilidades condicionais fornecidas pelo modelo:\n",
    "- $P(\\text{the} \\mid \\text{<s>}) = 0.5$\n",
    "- $P(\\text{cat} \\mid \\text{the}) = 0.3$\n",
    "- $P(\\text{sat} \\mid \\text{cat}) = 0.4$\n",
    "- $P(\\text{</s>} \\mid \\text{sat}) = 0.7$\n",
    "\n",
    "Aqui, $m$ é o número de palavras na sequência, que é 4 ($\"<s>\", \"the\", \"cat\", \"sat\", \"</s>\"$).\n",
    "\n",
    "Calculando o valor de cada termo:\n",
    "1. $\\log_2(P(\\text{the} \\mid \\text{<s>})) = \\log_2(0.5)$\n",
    "2. $\\log_2(P(\\text{cat} \\mid \\text{the})) = \\log_2(0.3)$\n",
    "3. $\\log_2(P(\\text{sat} \\mid \\text{cat})) = \\log_2(0.4)$\n",
    "4. $\\log_2(P(\\text{</s>} \\mid \\text{sat})) = \\log_2(0.7)$\n",
    "\n",
    "Vamos calcular cada um desses logaritmos:\n",
    "\n",
    "$$\n",
    "\\log_2(0.5) \\approx -1\n",
    "$$\n",
    "$$\n",
    "\\log_2(0.3) \\approx -1.737\n",
    "$$\n",
    "$$\n",
    "\\log_2(0.4) \\approx -1.322\n",
    "$$\n",
    "$$\n",
    "\\log_2(0.7) \\approx -0.357\n",
    "$$\n",
    "\n",
    "Soma dos logaritmos:\n",
    "$$\n",
    "\\sum_{i=1}^4 \\log_2(P(w_i \\mid w_{i-1})) = -1 + (-1.737) + (-1.322) + (-0.357) = -4.416\n",
    "$$\n",
    "\n",
    "Média dos logaritmos:\n",
    "$$\n",
    "\\frac{-1}{4} (-4.416) = 1.104\n",
    "$$\n",
    "\n",
    "Portanto, a perplexidade da sequência $\\text{\"<s> the cat sat </s>\"}$ é aproximadamente 1.104.\n",
    "\n",
    "Este valor significa que, em média, o modelo de linguagem é tão \"surpreso\" por cada palavra na sequência como se houvesse cerca de 2^1.104 ≈ 2.17 palavras possíveis em cada ponto. Uma perplexidade mais baixa indica um modelo melhor, pois implica que o modelo é menos \"surpreso\" pelas palavras da sequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bfed63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4e413f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:58:13.953359Z",
     "start_time": "2024-06-26T16:58:13.945441Z"
    }
   },
   "outputs": [],
   "source": [
    "# todo: pesquisar sobre perplexity e confimar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d79b3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df1085bc",
   "metadata": {},
   "source": [
    "outro exemplo ....\n",
    "\n",
    "### Intuição por Trás da Fórmula Perplexidade Logarítmica\n",
    "\n",
    "A fórmula da perplexidade logarítmica é:\n",
    "\n",
    "$$ \\log(PP(W)) = \\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^{|s_i|} -\\log(P(w_j^{(i)} \\mid w_{j-1}^{(i)})) $$\n",
    "\n",
    "onde:\n",
    "- **$\\log(PP(W))$**: Logaritmo da perplexidade da sequência de palavras $W$.\n",
    "- **$m$**: Número total de sentenças no conjunto $W$.\n",
    "- **$\\sum_{i=1}^m$**: Soma sobre todas as sentenças no conjunto $W$.\n",
    "- **$\\sum_{j=1}^{|s_i|}$**: Soma sobre todas as palavras na sentença $s_i$.\n",
    "- **$P(w_j^{(i)} \\mid w_{j-1}^{(i)})$**: Probabilidade de uma palavra $w_j^{(i)}$ dada a palavra anterior $w_{j-1}^{(i)}$ na sentença $s_i$.\n",
    "\n",
    "Considere um conjunto $W$ com duas sentenças:\n",
    "\n",
    "1. Sentença 1: \"eu gosto\"\n",
    "2. Sentença 2: \"de aprender\"\n",
    "\n",
    "Suponha que o modelo de linguagem forneça as seguintes probabilidades condicionais:\n",
    "\n",
    "- $P(\\text{eu} \\mid \\text{<s>}) = 0.5$\n",
    "- $P(\\text{gosto} \\mid \\text{eu}) = 0.4$\n",
    "- $P(\\text{de} \\mid \\text{<s>}) = 0.3$\n",
    "- $P(\\text{aprender} \\mid \\text{de}) = 0.6$\n",
    "\n",
    "Primeiro, calculamos os logaritmos das probabilidades:\n",
    "\n",
    "- $\\log(P(\\text{eu} \\mid \\text{<s>})) = \\log(0.5) = -0.693$\n",
    "- $\\log(P(\\text{gosto} \\mid \\text{eu})) = \\log(0.4) = -0.916$\n",
    "- $\\log(P(\\text{de} \\mid \\text{<s>})) = \\log(0.3) = -1.204$\n",
    "- $\\log(P(\\text{aprender} \\mid \\text{de})) = \\log(0.6) = -0.511$\n",
    "\n",
    "Agora, somamos os logaritmos das probabilidades para cada sentença:\n",
    "\n",
    "- Sentença 1: $ \\log(P(\\text{eu} \\mid \\text{<s>})) + \\log(P(\\text{gosto} \\mid \\text{eu})) = -0.693 + -0.916 = -1.609 $\n",
    "- Sentença 2: $ \\log(P(\\text{de} \\mid \\text{<s>})) + \\log(P(\\text{aprender} \\mid \\text{de})) = -1.204 + -0.511 = -1.715 $\n",
    "\n",
    "Em seguida, somamos essas somas:\n",
    "\n",
    "$$ \\text{Soma total} = -1.609 + -1.715 = -3.324 $$\n",
    "\n",
    "Finalmente, calculamos a perplexidade logarítmica média dividindo pela quantidade de sentenças:\n",
    "\n",
    "$$ \\log(PP(W)) = \\frac{1}{2} \\times (-3.324) = -1.662 $$\n",
    "\n",
    "Para encontrar a perplexidade $PP(W)$, calculamos a exponencial do valor encontrado:\n",
    "\n",
    "$$ PP(W) = e^{-1.662} \\approx 0.19 $$\n",
    "\n",
    "\n",
    "A perplexidade logarítmica fornece uma medida de quão bem o modelo prevê a sequência de palavras, mas faz isso em uma escala logarítmica. Isso ajuda a evitar problemas numéricos com multiplicação de pequenas probabilidades. Quanto menor a perplexidade, melhor é o modelo em prever a sequência.\n",
    "\n",
    "Usar o logaritmo das probabilidades facilita o cálculo e a interpretação da perplexidade, evitando problemas com a multiplicação de números pequenos e proporcionando uma métrica mais estável para avaliar a qualidade de modelos de linguagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eed6a7b",
   "metadata": {},
   "source": [
    "As duas fórmulas apresentadas calculam a perplexidade de uma sequência de palavras, mas utilizam diferentes abordagens. Vamos detalhar as diferenças e as nuances de cada fórmula.\n",
    "\n",
    "### Fórmula 1:\n",
    "\n",
    "$$ PP(W) = \\frac{-1}{m} \\sum^{m}_{i=1}\\log_2 (P(w_i \\mid w_{i-1})) $$\n",
    "\n",
    "### Fórmula 2:\n",
    "\n",
    "$$ \\log(PP(W)) = \\frac{1}{m} \\sum_{i=1}^m \\sum_{j=1}^{|s_i|} -\\log(P(w_j^{(i)} \\mid w_{j-1}^{(i)})) $$\n",
    "\n",
    "### Elementos Comuns:\n",
    "\n",
    "- Ambas as fórmulas visam calcular a perplexidade de um modelo de linguagem.\n",
    "- Ambas utilizam probabilidades condicionais $P(w_i \\mid w_{i-1})$, que representam a probabilidade de uma palavra $w_i$ dada a palavra anterior $w_{i-1}$.\n",
    "- Ambas envolvem o logaritmo dessas probabilidades.\n",
    "\n",
    "### Diferenças Principais:\n",
    "\n",
    "1. **Base do Logaritmo**:\n",
    "   - **Fórmula 1**: Usa logaritmo na base 2 ($\\log_2$).\n",
    "   - **Fórmula 2**: Usa logaritmo natural ($\\log$).\n",
    "\n",
    "2. **Tratamento da Média**:\n",
    "   - **Fórmula 1**: Toma a média dos logaritmos das probabilidades condicionais para todas as palavras em todas as sentenças.\n",
    "   - **Fórmula 2**: Toma a média dos logaritmos das probabilidades condicionais para todas as palavras em todas as sentenças, mas com uma estrutura mais detalhada de dupla soma.\n",
    "\n",
    "3. **Negatividade do Logaritmo**:\n",
    "   - **Fórmula 1**: Multiplica por $-1$ explicitamente no cálculo da média.\n",
    "   - **Fórmula 2**: O sinal negativo é incluído dentro da soma através do termo $-\\log(P(w_j^{(i)} \\mid w_{j-1}^{(i)}))$.\n",
    "\n",
    "4. **Forma da Perplexidade**:\n",
    "   - **Fórmula 1**: Dá diretamente a perplexidade média ($PP(W)$).\n",
    "   - **Fórmula 2**: Dá o logaritmo da perplexidade média ($\\log(PP(W))$).\n",
    "\n",
    "5. **Granularidade da Soma**:\n",
    "   - **Fórmula 1**: A soma é feita diretamente sobre todas as palavras, presumivelmente em uma única sequência.\n",
    "   - **Fórmula 2**: A soma é feita primeiro sobre todas as palavras em cada sentença, e depois sobre todas as sentenças. Isso permite lidar explicitamente com um conjunto de múltiplas sentenças.\n",
    "\n",
    "### Interpretação Intuitiva:\n",
    "\n",
    "- **Fórmula 1**: É uma versão simplificada e direta que calcula a média dos logaritmos das probabilidades condicionais para todas as palavras em uma sequência, usando logaritmo na base 2. Isso é útil para medir a perplexidade em termos de bits.\n",
    "\n",
    "- **Fórmula 2**: Fornece uma abordagem mais detalhada e robusta para calcular a perplexidade quando se lida com múltiplas sentenças. A saída da fórmula é o logaritmo natural da perplexidade, o que ajuda a evitar problemas numéricos ao lidar com pequenas probabilidades.\n",
    "\n",
    "### Exemplo Prático:\n",
    "\n",
    "Suponha que temos um conjunto $W$ com duas sentenças:\n",
    "1. Sentença 1: \"a b\"\n",
    "2. Sentença 2: \"c d\"\n",
    "\n",
    "Probabilidades condicionais fornecidas pelo modelo:\n",
    "- $P(a \\mid <s>) = 0.5$\n",
    "- $P(b \\mid a) = 0.4$\n",
    "- $P(c \\mid <s>) = 0.3$\n",
    "- $P(d \\mid c) = 0.6$\n",
    "\n",
    "Para a **Fórmula 1**:\n",
    "$$ PP(W) = \\frac{-1}{4} (\\log_2(0.5) + \\log_2(0.4) + \\log_2(0.3) + \\log_2(0.6)) $$\n",
    "\n",
    "Para a **Fórmula 2**:\n",
    "$$ \\log(PP(W)) = \\frac{1}{2} \\left( (-\\log(0.5) - \\log(0.4)) + (-\\log(0.3) - \\log(0.6)) \\right) $$\n",
    "\n",
    "Ambas as fórmulas resultam em uma medida de perplexidade, mas diferem na base do logaritmo e na forma de agregação dos termos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979df75d",
   "metadata": {},
   "source": [
    "## Out of Vocabulary Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23acb486",
   "metadata": {},
   "source": [
    "Um vocabulário é um conjunto de palavras únicas suportadas pelo seu modelo de linguagem. Em algumas tarefas, como reconhecimento de fala ou resposta a perguntas, encontraremos e geraremos palavras apenas a partir de um conjunto fixo de palavras. Portanto, um **vocabulário fechado**.\n",
    "\n",
    "**Vocabulário aberto** significa que você pode encontrar palavras fora do vocabulário, como o nome de uma nova cidade no conjunto de treinamento. Com ele, podemos lidar com palavras desconhecidas.\n",
    "\n",
    "- Crie vocabulário V\n",
    "- Substitua qualquer palavra em corpus e não em V por <snk>\n",
    "- Conte a probabilidade de <UNK> asim como qualquer palavra\n",
    "    \n",
    "<img src=\"./imgs/oov.png\">\n",
    "    \n",
    "O exemplo acima mostra como podemos usar min_frequency e substituir todas as palavras que aparecem menos vezes que min_frequency por UNK. Podemos então tratar UNK como uma palavra normal.\n",
    "\n",
    "Critérios para criar o vocabulário:\n",
    "- Frequência mínima de palavras f\n",
    "- Max |V|, inclui palavras por frequência\n",
    "- Use <UNK> com moderação\n",
    "- Perplexidade - compare apenas LMs com o mesmo V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a74090",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4e8536",
   "metadata": {},
   "source": [
    "Aqui está a mensagem reescrita com as fórmulas em LaTeX:\n",
    "\n",
    "Os três principais conceitos abordados aqui são lidar com n-grams ausentes, suavização, e Backoff e interpolação.\n",
    "\n",
    "A probabilidade condicional de um n-grama é dada por:\n",
    "\n",
    "$$ P(w_n \\mid w_{n-N+1}^{n-1}) = \\frac{C(w_{n-N+1}^{n-1}, w_n)}{C(w_{n-N+1}^{n-1})} \\text{ O resultado pode ser 0}$$\n",
    "\n",
    "Aqui, $ C(w_{n-N+1}^{n-1}, w_n) $ é o número de vezes que o n-grama $ w_{n-N+1}^{n-1} $ seguido por $ w_n $ aparece no corpus, e $ C(w_{n-N+1}^{n-1}) $ é o número de vezes que o n-grama $ w_{n-N+1}^{n-1} $ aparece.\n",
    "\n",
    "Se $ C(w_{n-N+1}^{n-1}, w_n) $ pode ser zero, então podemos corrigir este problema adicionando suavização ao adicionar +1:\n",
    "\n",
    "$$ P(w_n \\mid w_{n-1}) = \\frac{C(w_{n-1}, w_n) + 1}{\\sum_{w \\in V} (C(w_{n-1}, w) + 1)} = \\frac{C(w_{n-1}, w_n) + 1}{C(w_{n-1}) + V} $$\n",
    "\n",
    "A suavização Add-k é muito semelhante:\n",
    "\n",
    "$$ P(w_n \\mid w_{n-1}) = \\frac{C(w_{n-1}, w_n) + k}{\\sum_{w \\in V} (C(w_{n-1}, w) + k)} = \\frac{C(w_{n-1}, w_n) + k}{C(w_{n-1}) + k \\cdot V} $$\n",
    "\n",
    "Quando usar Backoff:\n",
    "- Se o n-grama de ordem N estiver ausente, usamos o n-grama de ordem (N-1), e assim por diante. Isso distorce a distribuição de probabilidade. Especialmente para corpora menores, alguma probabilidade precisa ser descontada dos n-gramas de ordem superior para ser usada nos n-gramas de ordem inferior.\n",
    "- O desconto de probabilidade, por exemplo, o Backoff de Katz, faz uso desses descontos.\n",
    "- Backoff \"Stupid\": Se a probabilidade do n-grama de ordem superior estiver ausente, a probabilidade do n-grama de ordem inferior é usada, simplesmente multiplicada por uma constante. Uma constante de aproximadamente 0,4 mostrou-se experimentalmente eficaz.\n",
    "\n",
    "<img src=\"./imgs/smoothing.png\">\n",
    "\n",
    "Também é possível usar interpolação ao calcular probabilidades da seguinte forma:\n",
    "\n",
    "$$ \\hat{P}(w_n \\mid w_{n-2} w_{n-1}) = \\lambda_1 \\times P(w_n \\mid w_{n-2} w_{n-1}) + \\lambda_2 \\times P(w_n \\mid w_{n-1}) + \\lambda_3 \\times P(w_n) $$\n",
    "\n",
    "Onde \n",
    "$$ \\lambda_1 + \\lambda_2 + \\lambda_3 = 1 $$\n",
    "$$ = $$\n",
    "$$ \\sum_i\\lambda_i = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8084a8c",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embedding with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cdaf84",
   "metadata": {},
   "source": [
    "## Basic Word Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140ff7a1",
   "metadata": {},
   "source": [
    "As representações de palavras pode ser classificadas como **inteiros, one-hot vectors e word embeddings**.\n",
    "\n",
    "<img src=\"./imgs/basic_word_representation.png\">\n",
    "\n",
    "Na imagem acima, temos um exemplo em que utilizamos inteniros para representar palavras. O problama com essa abordagem é que não temos motivo para dar mais pesos certas a palavras. E, para resolver esse problema, foram introduzidos os vetores one hot. Para implementar, um vetor de zeros é inicializado com dimensão V e então é atribuído o valor de 1 ao indice correspondente a palavra que está sendo representadada. Esse método também possui pontos positivos e negativos, sendo os positivos a sua **simplicidade e não necessidade de ordenação de palavras**, e como pontos negativos temos o **tamanho do vetor e a não codificação de \"sentido\"** (já que é verificado somente a presença da palavra)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01aa00e5",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54388a5",
   "metadata": {},
   "source": [
    "Word embedding nada mais é que uma representação numérica de palavras utilizando vetores. Existem representações de 2 a centenas de dimensões. Essa abordagem de representação de palavras lida com os pontos negativos da representação one hot, já que o tamanho do vetor terá um tamanho fixo e o \"sentido\" da palavra vai ser codificado de **forma contextual**, frequentemente chamado de \"semântica\" (apesar de não ser exatamente semântica).\n",
    "\n",
    "<img src=\"./imgs/word_embedding.png\">\n",
    "\n",
    "Na imagem acima podemos ver no plot em 2d que palavras que intuitivamente classificaríamos como representando coisas próximas (como filhote e gatinho, ou _rage_ e _anger_) estão próximas graças a representação através de embedding. Nela, talvez a primeira coordenada represente se uma palavra é positiva ou negativa. A segunda coordenada informa se a palavra é abstrata ou concreta. Este é apenas um exemplo, no mundo real os embeddings terão centenas de dimensões como falado anteriormente. Podemos pensar em cada coordenada como um número que lhe diz algo sobre a palavra. Mas, quanto mais dimensões o embedding tem, mais difícil atribuir um sentido ao número da coordenada, ou as palavras agrupadas nessa coordenada.\n",
    "\n",
    "O termo \"word vector\" representa os vetores one hot e também word embedding, mas é mais utilizado para se referir aos word embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f81efbc",
   "metadata": {},
   "source": [
    "## How to Create Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a66866",
   "metadata": {},
   "source": [
    "Para criar **word embeddings**, você sempre precisa de um corpus de texto e de um método de incorporação.\n",
    "\n",
    "O contexto de uma palavra informa que tipo de palavras tendem a ocorrer perto dessa palavra específica. O contexto é importante porque é o que dará sentido à incorporação de cada palavra.\n",
    "\n",
    "Existem muitos tipos de métodos possíveis que permitem aprender a palavra embeddings. O modelo de aprendizado de máquina executa uma tarefa de aprendizado, e os principais subprodutos dessa tarefa são os embeddings de palavras. A tarefa poderia ser aprender a prever uma palavra com base nas palavras circundantes em uma frase do corpus, como no caso do **continous bag of words (CBOW)**.\n",
    "\n",
    "A tarefa é **auto-supervisionada**: é não-supervisionada, no sentido de que os dados de entrada — o corpus — não são rotulados, e supervisionada, no sentido de que os próprios dados fornecem o contexto necessário que normalmente constituiria os rótulos.\n",
    "\n",
    "Ao treinar vetores de palavras, existem alguns parâmetros que você precisa ajustar. (ou seja, a dimensão do vetor de palavras)\n",
    "\n",
    "<img src=\"./imgs/word_embedding_process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bafed2",
   "metadata": {},
   "source": [
    "## Word Embedding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a253ec98",
   "metadata": {},
   "source": [
    "Existem diversos métodos de word embeddings, podendo ser agrupados em **métodos classicos** e de **deep learning** ou **contextual embeddings**.\n",
    "\n",
    "**Métodos Clásscicos**:\n",
    "- **word2vec (Google, 2013)**: Aprende representações vetoriais de palavras utilizando shallow neural networks, onde palavras que ocorrem em contextos semelhantes têm vetores próximos no espaço vetorial. Ele possui duas abordagens principais: Continuous Bag-of-Words (CBOW) e Continuous Skip-gram\n",
    "  - **Continuous bag-of-words (CBOW)**: Aprende a prever a palavra central dadas algumas palavras de contexto.\n",
    "  - **Continuous skip-gram / Skip-gram with negative sampling (SGNS)**: o modelo aprende a prever as palavras que cercam uma determinada palavra de entrada.\n",
    "- **Global Vectors (GloVe) (Stanford, 2014)**: fatora o logaritmo da matriz de coocorrência de palavras do corpus, semelhante à matriz de contagem.\n",
    "- **fastText (Facebook, 2016)**: baseado no modelo skip-gram e leva em consideração a estrutura das palavras, representando as palavras como um n-grama de caracteres. Ele suporta palavras fora do vocabulário (OOV).\n",
    "\n",
    "**Deep learning, contextual embeddings**:\n",
    "\n",
    "Nestes modelos mais avançados, as palavras têm diferentes embeddings dependendo do seu contexto e usam deep neural networks.\n",
    "\n",
    "- **BERT (Google, 2018)**: Aprende a prever palavras mascaradas em uma frase e a relação entre frases, utilizando a técnica de atenção para capturar o contexto bidirecional completo de uma palavra em uma frase.\n",
    "- **ELMo (Allen Institute for AI, 2018)**: Aprende representações de palavras sensíveis ao contexto usando modelos de linguagem bidirecionais empilhados, permitindo que o significado de uma palavra varie com base em seu contexto na frase.\n",
    "- **GPT-2 (OpenAI, 2018)**: Aprende a prever a próxima palavra em uma sequência de texto, utilizando modelos de linguagem unidirecionais (da esquerda para a direita) para gerar texto coerente e contextualizado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c84660",
   "metadata": {},
   "source": [
    "## Continuous Bag-of-Words Model (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79832032",
   "metadata": {},
   "source": [
    "Para criar embeddings de palavras, precisamos de um corpus e de um algoritmo de aprendizagem. O subproduto desta tarefa seria um conjunto de embedding de palavras. No caso do **continous bag of words (CBOW)**, o objetivo da tarefa é prever uma palavra faltante com base nas palavras circundantes.\n",
    "\n",
    "<img src=\"./imgs/cbow1.png\">\n",
    "\n",
    "Aqui está uma visualização que mostra como os modelos funcionam.\n",
    "\n",
    "<img src=\"./imgs/cbow2.png\">          \n",
    "          \n",
    "Note que o tamanho da janela na imagem acima é 5. O tamanho do contexto, C, é 2. C geralmente informa **quantas palavras antes ou depois da palavra central o modelo usará para fazer a previsão**. Aqui está outra visualização que mostra uma visão geral do modelo.\n",
    "\n",
    "<img src=\"./imgs/cbow3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a990c1",
   "metadata": {},
   "source": [
    "## Cleaning and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f5846",
   "metadata": {},
   "source": [
    "Antes de implementar qualquer algoritmo de processamento de linguagem natural, podemos querer limpar os dados e tokenizá-los.\n",
    "\n",
    "<img src=\"./imgs/cleaning1.png\">\n",
    "\n",
    "Podemos implementar esse tratamento utilizando python de várias formas, como:\n",
    "\n",
    "<img src=\"./imgs/cleaning2.png\">\n",
    "\n",
    "Além disos, podemos adicionar quantas condições forem necessárias nas linhas correspondentes ao retângulo verde acima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabde3d1",
   "metadata": {},
   "source": [
    "## Sliding Window of Words in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45323d7",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/sliding_window.png\">\n",
    "\n",
    "O código acima mostra uma função que recebe dois parâmetros:\n",
    "- Words: uma lista de palavras\n",
    "- C: o tamanho do contexto.\n",
    "\n",
    "Primeiro _settamos_ i para C. Em seguida, destacamos center_word e context_words. Em então, produzimos esses valores e incrementamos i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "438644fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T16:58:13.971348Z",
     "start_time": "2024-06-26T16:58:13.958660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'am', 'because', 'i'] happy\n",
      "['i', 'am', 'because', 'i']\thappy\n",
      "['am', 'happy', 'i', 'am'] because\n",
      "['am', 'happy', 'i', 'am']\tbecause\n",
      "['happy', 'because', 'am', 'learning'] i\n",
      "['happy', 'because', 'am', 'learning']\ti\n"
     ]
    }
   ],
   "source": [
    "def get_windows(words, C):\n",
    "    i = C\n",
    "    while i < len(words) - C:\n",
    "        center_word = words[i]\n",
    "        context_words = words[(i - C):i] + words[(i+1):(i+C+1)]\n",
    "        print(context_words, center_word)\n",
    "        yield context_words, center_word\n",
    "        i += 1\n",
    "        #print(i)\n",
    "                \n",
    "# words = [\"<s>\", \"i\", \"am\", \"happy\", \"because\", \"i\", \"am\", \"learning\", \"</s>\"]\n",
    "words = [\"i\", \"am\", \"happy\", \"because\", \"i\", \"am\", \"learning\"]\n",
    "\n",
    "for x, y in get_windows(words, 2):\n",
    "    print(f\"{x}\\t{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d79786a",
   "metadata": {},
   "source": [
    "## Transforming Words into Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b995d25b",
   "metadata": {},
   "source": [
    "Para transformar os vetores de contexto em um único vetor, podemos usar o seguinte.\n",
    "\n",
    "<img src=\"./imgs/transform_word_into_vector.png\">\n",
    "<img src=\"./imgs/transform_word_into_vector2.png\">\n",
    "\n",
    "Note que começamos com vetores únicos one-hot para as palavras de contexto e os transformamos em um único vetor calculando uma média. Como resultado acabamos tendo os seguintes vetores que podem usados no treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852df5dc",
   "metadata": {},
   "source": [
    "## Architecture of the CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a9436",
   "metadata": {},
   "source": [
    "A arquitetura do modelo CBOW pode ser descrita da seguinte forma\n",
    "\n",
    "<img src=\"./imgs/cbow_architecture.png\">\n",
    "\n",
    "Dado uma entrada, $X$, que é a média de todos os vetores de contexto. Em seguida, multiplicamos por $W_1$ e adiciona $b_1$. O resultado passa por uma função ReLU para fornecer sua camada oculta. Essa camada é então multiplicada por $W_2$ e então adicionamos $b_2$. O resultado passa por um softmax que fornece uma distribuição sobre $V$, palavras do vocabulário. Por fim, escolhemos a palavra do vocabulário que corresponde ao arg-max da saída."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e087e8",
   "metadata": {},
   "source": [
    "## Architecture of the CBOW Model: Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76d042",
   "metadata": {},
   "source": [
    "As equações para o modelo acima são:\n",
    "\n",
    "$$ z_1 = W_1x + b_1$$\n",
    "$$ h = \\text{ReLU}(z_1)$$\n",
    "$$ z_2 = W_2 h + b_2$$\n",
    "$$ \\hat{y} = \\text{softmax}(z_2)$$\n",
    "\n",
    "Aqui, podemos ver as dimensões\n",
    "\n",
    "<img src=\"./imgs/cbow_architecture2.png\">\n",
    "\n",
    "**1. Entrada (Contexto)**: A entrada do CBOW é composta pelas palavras de contexto ao redor da palavra-alvo. Se temos uma janela de contexto de tamanho `2c`, então para cada palavra-alvo, consideramos `c` palavras à esquerda e `c` palavras à direita. Suponha que a janela de contexto seja de tamanho 2 (ou seja, `c=1`), e a frase seja \"the quick brown fox\". Para a palavra-alvo \"quick\", as palavras de contexto são [\"the\", \"brown\"].\n",
    "\n",
    "**2. Vetores de Entrada**: Cada palavra de contexto é representada como um vetor one-hot de dimensão igual ao tamanho do vocabulário `V`. Em vez de usar diretamente os vetores one-hot, eles são transformados em embeddings de dimensão `D` através de uma matriz de embeddings `W`. Portanto, cada palavra de contexto é um vetor de dimensão `D`.\n",
    "\n",
    "**3. Representação Média**: Os vetores de embeddings das palavras de contexto são combinados (geralmente através da média) para formar uma única representação de dimensão `D`.\n",
    "\n",
    "**4. Projeção para o Espaço do Vocabulário**: A representação média é então projetada de volta para o espaço do vocabulário através de uma segunda matriz de pesos `W'` de dimensão `V x D`. Isso resulta em um vetor de dimensão `V` que contém pontuações para cada palavra no vocabulário.\n",
    "\n",
    "**5. Probabilidades de Saída**: Finalmente, a função softmax é aplicada ao vetor de pontuações para obter as probabilidades de previsão para cada palavra no vocabulário. A palavra com a maior probabilidade é a previsão do modelo para a palavra-alvo.\n",
    "\n",
    "**Resumo das Dimensões**:\n",
    "\n",
    "1. **Tamanho do Vocabulário (V)**: Número total de palavras no vocabulário.\n",
    "2. **Dimensão dos Embeddings (D)**: Dimensão dos vetores de embeddings para cada palavra.\n",
    "3. **Tamanho da Janela de Contexto (c)**: Número de palavras de contexto à esquerda e à direita da palavra-alvo.\n",
    "\n",
    "**Exemplo com Dimensões**\n",
    "\n",
    "Suponha que:\n",
    "- O vocabulário tem tamanho `V = 10,000`.\n",
    "- A dimensão dos embeddings é `D = 300`.\n",
    "- A janela de contexto é de tamanho 2 (c = 1).\n",
    "\n",
    "Então, para a palavra-alvo \"quick\" com as palavras de contexto [\"the\", \"brown\"]:\n",
    "\n",
    "1. Cada palavra de contexto é um vetor one-hot de dimensão `10,000`.\n",
    "2. Esses vetores one-hot são transformados em embeddings de dimensão `300` através de uma matriz de embeddings `W` de dimensão `10,000 x 300`.\n",
    "3. Os embeddings das palavras de contexto são combinados (media) para formar um único vetor de dimensão `300`.\n",
    "4. Esse vetor de dimensão `300` é projetado de volta para o espaço do vocabulário através de uma matriz `W'` de dimensão `10,000 x 300`, resultando em um vetor de dimensão `10,000`.\n",
    "5. A função softmax é aplicada para obter as probabilidades de previsão para cada palavra no vocabulário.\n",
    "\n",
    "Portanto, o CBOW utiliza essas dimensões para transformar o contexto em uma previsão para a palavra-alvo, aprendendo representações de palavras eficazes ao longo do processo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f539d",
   "metadata": {},
   "source": [
    "Para lidar com inputs batchs, podemos empilhar oe exemplos como colunas. Depois, podemos processeguir multiplicando as matrizes da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/cbow_architecture3.png\">\n",
    "\n",
    "Noa diagrama acima, podemos ver as dimensões de cada matriz. Note que $\\hat{Y}$ possui dimensão V por m. Cada coluna é a predição da coluna correspondente as palavras de contexto. Então, a primeira coluna $\\hat{Y}$ é a predição correspondente a primeira coluna de $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caad600",
   "metadata": {},
   "source": [
    "## Architecture of the CBOW Model: Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121eeeee",
   "metadata": {},
   "source": [
    "A arquitetura CBOW possui duas funções de ativação, a **ReLU** e a **Softmax**.\n",
    "\n",
    "**Função ReLU**:\n",
    "\n",
    "A rectified linear unit (Relu), é uma das funções de ativaçao mais populares. Ao passar um vetor x na função ReLU, teremos $x = \\text{max}(0, x)$. Ela zera valores negativos e mantêm o valores positivos.\n",
    "\n",
    "<img src=\"./imgs/cbow_relu.png\">\n",
    "\n",
    "**Função Softmax**:\n",
    "\n",
    "Já  afunção softmax recebe um vetor e o transforma numa distribuição de probabilidade. Por exemplo, dado o vetor z, podemos transformá-lo na seguite distribuição de probabilidade:\n",
    "\n",
    "<img src=\"./imgs/cbow_softmax.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e3255f",
   "metadata": {},
   "source": [
    "## Training a CBOW Model: Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7ba61",
   "metadata": {},
   "source": [
    "A função de custo para o CBOW é a **cross-entropy loss** definida como:\n",
    "\n",
    "$$ J = -\\sum^{V}_{k = 1} y_k \\log\\hat{y}_k$$\n",
    "\n",
    "<img src=\"./imgs/cbow_cost_function.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826811b7",
   "metadata": {},
   "source": [
    "## Training a CBOW Model: Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1c90cd",
   "metadata": {},
   "source": [
    "Para calcular o loss do batch, precisamos computar o seguinte\n",
    "\n",
    "$$ J_{\\text{batch}} = -\\frac{1}{m} \\sum^{m}_{i=1} \\sum^{V}_{j=1} y_j^{(i)} \\log\\hat{y}_j^{(i)}$$\n",
    "\n",
    "As imagens abaixo descrevem o processo de propagação da esquerda para a direita. Dado a palavra central predita no word matrix e a palavra real, podemos computar o loss.\n",
    "\n",
    "<img src=\"./imgs/cbow_forward1.png\">\n",
    "<img src=\"./imgs/cbow_forward2.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1ca18a",
   "metadata": {},
   "source": [
    "O processo de forward propagation no modelo Continuous Bag-of-Words (CBOW) é a etapa onde as palavras de contexto são usadas para prever a palavra alvo. Aqui está uma explicação passo a passo desse processo:\n",
    "\n",
    "1. **Entrada: Palavras de Contexto**\n",
    "   - Suponha que queremos prever a palavra alvo $ w_t $ usando as palavras de contexto $ w_{t-m}, \\ldots, w_{t-1}, w_{t+1}, \\ldots, w_{t+m} $, onde $ 2m $ é o tamanho da janela de contexto.\n",
    "\n",
    "2. **Representação de Palavras: Vetores de Embedding**\n",
    "   - Cada palavra de contexto é representada por um vetor de embedding. Esses vetores são extraídos da matriz de embeddings $ W $, onde cada linha corresponde ao vetor de embedding de uma palavra no vocabulário.\n",
    "   - Se $ V $ é o tamanho do vocabulário e $ d $ é a dimensão dos vetores de embedding, então $ W $ é uma matriz $ V \\times d $.\n",
    "\n",
    "3. **Cálculo da Média dos Vetores de Embedding**\n",
    "   - Os vetores de embedding das palavras de contexto são somados e a média desses vetores é calculada. Isso resulta em um vetor de embedding médio $ \\mathbf{v} $ de dimensão $ d $.\n",
    "     $$\n",
    "     \\mathbf{v} = \\frac{1}{2m} \\sum_{i=1}^{2m} \\mathbf{v}_{w_{t-i}}\n",
    "     $$\n",
    "\n",
    "4. **Camada Oculta (Opcional)**\n",
    "   - Em algumas variantes, este vetor médio é passado por uma camada oculta, mas no CBOW clássico, o vetor médio $ \\mathbf{v} $ é diretamente utilizado na próxima etapa.\n",
    "\n",
    "5. **Cálculo das Probabilidades das Palavras Alvo**\n",
    "   - O vetor médio $ \\mathbf{v} $ é multiplicado pela matriz de pesos $ W' $ para gerar uma pontuação para cada palavra no vocabulário.\n",
    "     $$\n",
    "     \\mathbf{u} = W' \\mathbf{v}\n",
    "     $$\n",
    "   - $ W' $ é uma matriz de pesos de dimensão $ d \\times V $.\n",
    "\n",
    "6. **Softmax**\n",
    "   - A pontuação $ \\mathbf{u} $ é passada por uma função softmax para converter essas pontuações em probabilidades.\n",
    "     $$\n",
    "     P(w_t | w_{t-m}, \\ldots, w_{t+m}) = \\frac{\\exp(u_j)}{\\sum_{k=1}^{V} \\exp(u_k)}\n",
    "     $$\n",
    "\n",
    "No forward propagation do CBOW:\n",
    "1. Extraímos os vetores de embedding das palavras de contexto.\n",
    "2. Calculamos a média desses vetores.\n",
    "3. Multiplicamos o vetor médio pela matriz de pesos para obter as pontuações das palavras do vocabulário.\n",
    "4. Usamos a função softmax para converter essas pontuações em probabilidades preditas para a palavra alvo.\n",
    "\n",
    "Este processo permite que o modelo preveja a palavra alvo com base nas palavras de contexto fornecidas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5120660d",
   "metadata": {},
   "source": [
    "## Training a CBOW Model: Backpropagation and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994aed18",
   "metadata": {},
   "source": [
    "Para realizar a **retropropagação (backpropagation)** precisamos calcular as derivadsa parciais do custo em relação aos pesos e bias.\n",
    "\n",
    "Quando computamos a backpropagation nesse modelo, computamos o seguinte:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J_{\\text{batch}}}{\\partial W_1}, \\quad \\frac{\\partial J_{\\text{batch}}}{\\partial W_2}, \\quad \\frac{\\partial J_{\\text{batch}}}{\\partial b_1}, \\quad \\frac{\\partial J_{\\text{batch}}}{\\partial b_2}\n",
    "$$\n",
    "\n",
    "Já o **gradient descent** atualiza os pesos e bias. E, para atualizar os pesos precisamos iterar o seguinte:\n",
    "\n",
    "$$\n",
    "W_1 := W_1 - \\alpha \\frac{\\partial J_{\\text{batch}}}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_2 := W_2 - \\alpha \\frac{\\partial J_{\\text{batch}}}{\\partial W_2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_1 := b_1 - \\alpha \\frac{\\partial J_{\\text{batch}}}{\\partial b_1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b_2 := b_2 - \\alpha \\frac{\\partial J_{\\text{batch}}}{\\partial b_2}\n",
    "$$\n",
    "\n",
    "\n",
    "Um alfa menor permite atualizações mais graduais dos pesos e tendências, enquanto um número maior permite uma atualização mais rápida dos pesos. Se $\\alpha$ for muito grande, você podemos não aprender nada; se for muito pequeno, o modelo levará uma eternidade para treinar.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed966bd",
   "metadata": {},
   "source": [
    "## Extracting Word Embedding Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f0029",
   "metadata": {},
   "source": [
    "Os vetores de embedding são **os pesos da camada de embedding**.Existem duas opções para extrair embeddings de palavras após treinar o CBOW. Podemos usar $W_1$ da seguinte maneira:\n",
    "\n",
    "<img src=\"./imgs/extracting_embedding_vectors1.png\">\n",
    "\n",
    "Se usarmos $W_1$, cada coluna corresponderá aos embeddings de uma palavra específica. Também podemos usar $W_2$ da seguinte maneira:\n",
    "\n",
    "<img src=\"./imgs/extracting_embedding_vectors2.png\">\n",
    "\n",
    "A opção final é calcular a média de ambas as matrizes da seguinte forma:\n",
    "\n",
    "<img src=\"./imgs/extracting_embedding_vectors3.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "56c72bd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T19:30:44.733882Z",
     "start_time": "2024-06-26T19:30:44.721586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector de embedding para happy: tensor([ 1.6582, -0.4397,  0.6625, -1.0784, -0.4663])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Suponha que você tenha uma classe CBOW definida\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).mean(dim=0)\n",
    "        out = self.linear1(embeds)\n",
    "        return out\n",
    "\n",
    "# Suponha que você tenha treinado o modelo CBOW\n",
    "vocab_size = 5 #10000  # exemplo de tamanho do vocabulário\n",
    "embedding_dim = 5 #100  # exemplo de dimensão do embedding\n",
    "model = CBOW(vocab_size, embedding_dim)\n",
    "\n",
    "# Após o treinamento, extraia os vetores de embedding\n",
    "word_embeddings = model.embeddings.weight.data\n",
    "\n",
    "# Se você tiver um dicionário de mapeamento de palavras para índices\n",
    "word_to_index = {'i': 0, 'm': 1, 'happy':2, 'because':3, 'learning':4}  # exemplo de mapeamento\n",
    "\n",
    "# Para obter o vetor de embedding de uma palavra específica\n",
    "word = 'happy'\n",
    "word_index = word_to_index[word]\n",
    "word_vector = word_embeddings[word_index]\n",
    "\n",
    "print(f\"Vector de embedding para {word}: {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "365d17ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T19:30:40.156888Z",
     "start_time": "2024-06-26T19:30:40.152222Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Passos para Treinamento do CBOW:\n",
    "# # 1. Preparação dos Dados\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # Dados de exemplo\n",
    "# corpus = [\n",
    "#     ['i', 'am', 'happy', 'because', 'i', 'am', 'learning'],\n",
    "#     ['he', 'is', 'happy', 'because', 'he', 'is', 'learning']\n",
    "# ]\n",
    "\n",
    "# # Construindo o vocabulário e o mapeamento de palavras para índices\n",
    "# word_to_index = {'<pad>': 0}  # adicionamos um padding token\n",
    "# for sentence in corpus:\n",
    "#     for word in sentence:\n",
    "#         if word not in word_to_index:\n",
    "#             word_to_index[word] = len(word_to_index)\n",
    "\n",
    "# index_to_word = {index: word for word, index in word_to_index.items()}\n",
    "\n",
    "# # Função para converter uma frase em índices de palavras\n",
    "# def sentence_to_indices(sentence, word_to_index):\n",
    "#     return [word_to_index[word] for word in sentence]\n",
    "\n",
    "# # Convertendo o corpus para índices\n",
    "# indexed_corpus = [sentence_to_indices(sentence, word_to_index) for sentence in corpus]\n",
    "\n",
    "# # Exemplo de como fica o corpus indexado\n",
    "# print(\"Corpus indexado:\")\n",
    "# for indexed_sentence in indexed_corpus:\n",
    "#     print(indexed_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d56c4ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T19:30:33.611357Z",
     "start_time": "2024-06-26T19:30:33.606021Z"
    }
   },
   "outputs": [],
   "source": [
    "# # # 2. Definição do Dataset e Dataloader\n",
    "\n",
    "# # class CBOWDataset(Dataset):\n",
    "# #     def __init__(self, corpus, window_size):\n",
    "# #         self.corpus = corpus\n",
    "# #         self.window_size = window_size\n",
    "\n",
    "# #     def __len__(self):\n",
    "# #         return len(self.corpus)\n",
    "\n",
    "# #     def __getitem__(self, idx):\n",
    "# #         context = []\n",
    "# #         target = []\n",
    "# #         sentence = self.corpus[idx]\n",
    "        \n",
    "# #         for i, word in enumerate(sentence):\n",
    "# #             start = max(0, i - self.window_size)\n",
    "# #             end = min(len(sentence), i + self.window_size + 1)\n",
    "            \n",
    "# #             context.append([sentence[j] for j in range(start, end) if j != i])\n",
    "# #             target.append(word)\n",
    "        \n",
    "# #         return torch.tensor(context), torch.tensor(target)\n",
    "\n",
    "# # # Exemplo de uso do Dataset\n",
    "# # window_size = 2\n",
    "# # cbow_dataset = CBOWDataset(indexed_corpus, window_size)\n",
    "# # cbow_dataloader = DataLoader(cbow_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# class CBOWDataset(Dataset):\n",
    "#     def __init__(self, corpus, window_size):\n",
    "#         self.corpus = corpus\n",
    "#         self.window_size = window_size\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.corpus)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         context = []\n",
    "#         target = []\n",
    "#         sentence = self.corpus[idx]\n",
    "        \n",
    "#         for i, word in enumerate(sentence):\n",
    "#             start = max(0, i - self.window_size)\n",
    "#             end = min(len(sentence), i + self.window_size + 1)\n",
    "            \n",
    "#             context.extend([sentence[j] for j in range(start, end) if j != i])\n",
    "#             target.append(word)\n",
    "        \n",
    "#         return torch.tensor(context), torch.tensor(target)\n",
    "\n",
    "# # Exemplo de uso do Dataset\n",
    "# window_size = 2\n",
    "# cbow_dataset = CBOWDataset(indexed_corpus, window_size)\n",
    "# cbow_dataloader = DataLoader(cbow_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e49a064",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T19:30:28.089792Z",
     "start_time": "2024-06-26T19:30:28.084458Z"
    }
   },
   "outputs": [],
   "source": [
    "# #3. Definição do Modelo CBOW e Treinamento\n",
    "\n",
    "# class CBOW(nn.Module):\n",
    "#     def __init__(self, vocab_size, embedding_dim):\n",
    "#         super(CBOW, self).__init__()\n",
    "#         self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "#         self.linear1 = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "#     def forward(self, inputs):\n",
    "#         embeds = self.embeddings(inputs).mean(dim=1)  # média ao longo da dimensão 1\n",
    "#         out = self.linear1(embeds)\n",
    "#         return out\n",
    "\n",
    "# # Parâmetros de treinamento\n",
    "# vocab_size = len(word_to_index)\n",
    "# embedding_dim = 10  # dimensão do embedding\n",
    "# learning_rate = 0.001\n",
    "# num_epochs = 100\n",
    "\n",
    "# # Inicialização do modelo\n",
    "# model = CBOW(vocab_size, embedding_dim)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# # Função de treinamento\n",
    "# def train_model(model, dataloader, criterion, optimizer, num_epochs):\n",
    "#     model.train()\n",
    "#     for epoch in range(num_epochs):\n",
    "#         total_loss = 0\n",
    "#         for contexts, targets in dataloader:\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(contexts)\n",
    "#             loss = criterion(outputs.view(-1, vocab_size), targets)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_loss += loss.item()\n",
    "#         print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(dataloader)}')\n",
    "\n",
    "# # Treinamento do modelo\n",
    "# train_model(model, cbow_dataloader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a80ab74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-26T19:30:30.191565Z",
     "start_time": "2024-06-26T19:30:30.187616Z"
    }
   },
   "outputs": [],
   "source": [
    "# #4. Extração dos Vetores de Embedding\n",
    "\n",
    "# # Obtendo os vetores de embedding\n",
    "# word_embeddings = model.embeddings.weight.data\n",
    "\n",
    "# # Exemplo de uso: vetor de embedding da palavra 'happy'\n",
    "# word = 'happy'\n",
    "# word_index = word_to_index[word]\n",
    "# word_vector = word_embeddings[word_index]\n",
    "\n",
    "# print(f\"Vetor de embedding para '{word}': {word_vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fa1676",
   "metadata": {},
   "source": [
    "## Evaluating Word Embeddings: Intrinsic Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb45b8",
   "metadata": {},
   "source": [
    "A **avaliação intrínseca** permite testar relações entre palavras. Ele permite capturar analogias semânticas como “França” está para “Paris” assim como “Itália” está para $<?>$ e também analogias sintáticas como “visto” está para “viu” assim como “sido” está para $<?>$.\n",
    "\n",
    "Casos ambíguos podem ser muito mais difíceis de rastrear:\n",
    "\n",
    "<img src=\"./imgs/intrinsic_eval1.png\">\n",
    "\n",
    "Aqui estão algumas maneiras que permitem usar a avaliação intrínseca.\n",
    "\n",
    "<img src=\"./imgs/intrinsic_eval2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4db12b",
   "metadata": {},
   "source": [
    "## Evaluating Word Embeddings: Extrinsic Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6007f437",
   "metadata": {},
   "source": [
    "A **avaliação extrínseca** testa o embedding de palavras em tarefas externas, como reconhecimento de entidade nomeada (NER), marcação de classes gramaticais (Part-of-speech tagging), etc. A utilizade dos embeddings é avaliada a partir de métricas como acurácia ou F1 score. O desempenho do classificador na métrica de avaliação representa o desempenho combinado das tarefas de incorporação e classificação. Os métodos de avaliação extrínseca são os testes finais para garantir que os embeddings de palavras sejam realmente úteis. No entanto, as suas principais **desvantagens** são que a avaliação consumirá mais tempo do que a avaliação intrínseca e que, se o desempenho for fraco, as métricas de desempenho não fornecem informações sobre quais partes específicas do processo de ponta a ponta são responsáveis, o próprio word embedding ou a tarefa externa usada para testá-la."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0545220",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Classification and Vector Spaces, disponível em https://www.coursera.org/learn/probabilistic-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fb4b88",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.422px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
