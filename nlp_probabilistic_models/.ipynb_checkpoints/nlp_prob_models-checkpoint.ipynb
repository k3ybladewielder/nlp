{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f2ee7a2",
   "metadata": {},
   "source": [
    "# Natural Language Processing with Probabilistic Models\n",
    "\n",
    "Notas sobre o curso Natural Language Processing with Classification and Vector Spaces da DeeplearninigAI. O notebook é composto majoritariamente de material original, salvo as figuras, que foram criadas pela **Deep Learning AI** e disponibilizadas em seu curso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4106c1",
   "metadata": {},
   "source": [
    "# Week 1 - Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515f96a8",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Word probabilities\n",
    "- Dynamic programming\n",
    "- Minimum edit distance\n",
    "- Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437ccaa",
   "metadata": {},
   "source": [
    "## Autocorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93265af1",
   "metadata": {},
   "source": [
    "A autocorreção em NLP é uma técnica que corrige automaticamente erros ortográficos ou de digitação em texto. Ela é amplamente utilizada em aplicativos de mensagens, processadores de texto e mecanismos de busca para melhorar a experiência do usuário. Ela é geralmente implementada usando modelos de **linguagem probabilísticos**, como os modelos de N-gramas. Esses modelos calculam a probabilidade de uma sequência de palavras e sugerem a sequência mais provável, dada uma palavra mal digitada. Eles são treinados em grandes corpora de texto para aprender padrões comuns na linguagem.\n",
    "\n",
    "<img src=\"./imgs/autocorrect_phone.png\">\n",
    "\n",
    "\n",
    "Para implementar um sistema de autocorreção, precisamos seguir as seguintes etapas:\n",
    "\n",
    "1. **Identificação do Erro:**\n",
    "   - O primeiro passo é identificar que uma palavra pode estar incorreta. Isso é feito comparando a palavra digitada com um dicionário ou usando técnicas mais avançadas, como distância de edição/edit distance (Levenshtein), que mede a diferença entre duas palavras com base nas operações necessárias para transformar uma na outra (inserção, exclusão, substituição).\n",
    "\n",
    "2. **Sugestão de Correção:**\n",
    "   - Uma vez identificado o erro, o sistema de autocorreção sugere uma ou mais correções com base na probabilidade das palavras no contexto. Isso pode ser feito usando modelos de N-gramas, onde a probabilidade de uma palavra é calculada com base nas palavras anteriores.\n",
    "\n",
    "3. **Escolha da Melhor Correção:**\n",
    "   - A sugestão de correção é escolhida com base em critérios como a probabilidade da palavra correta no contexto, a distância de edição mínima da palavra digitada e outras heurísticas.\n",
    "\n",
    "4. **Aplicação da Correção:**\n",
    "   - A correção é então aplicada ao texto, substituindo a palavra mal digitada pela palavra correta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf3360",
   "metadata": {},
   "source": [
    "**Exemplos**\n",
    "\n",
    "1. **Erro de Digitação Simples:**\n",
    "   - **Texto Digitado:** \"Eu gosto de mangaa.\"\n",
    "   - **Correção Proposta:** \"Eu gosto de manga.\"\n",
    "   - **Explicação:** O sistema identifica que \"mangaa\" pode ser um erro de digitação para \"manga\" com base na proximidade no teclado e sugere a correção.\n",
    "\n",
    "2. **Inversão de Letras:**\n",
    "   - **Texto Digitado:** \"Estarmos esperndo por você.\"\n",
    "   - **Correção Proposta:** \"Estaremos esperando por você.\"\n",
    "   - **Explicação:** O sistema percebe que \"esperndo\" pode ser um erro de inversão de letras em \"esperando\" e sugere a correção.\n",
    "\n",
    "3. **Erros de Espaçamento:**\n",
    "   - **Texto Digitado:** \"Nãoseinada\"\n",
    "   - **Correção Proposta:** \"Não sei nada\"\n",
    "   - **Explicação:** O sistema identifica que \"Nãoseinada\" pode ser uma combinação de palavras e sugere a separação correta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f17f90d",
   "metadata": {},
   "source": [
    "**Limitações**\n",
    "- **Palavras Fora do Vocabulário:** Se uma palavra está muito fora do vocabulário do modelo, a autocorreção pode falhar em sugerir a correção correta.\n",
    "- **Ambiguidade:** Em casos de ambiguidade, onde várias correções são possíveis, a autocorreção pode sugerir uma correção incorreta.\n",
    "- **Contexto Limitado:** Modelos de N-gramas têm um contexto limitado, então a autocorreção pode não capturar nuances mais complexas da linguagem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da94655a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c52c246",
   "metadata": {},
   "source": [
    "### Métodos de Identificação de Palavras Erradas**\n",
    "\n",
    "Identificar palavras erradas é o primeiro passo crucial no processo de autocorreção em NLP. Isso envolve a detecção de erros ortográficos, de digitação, e até erros gramaticais. Vamos explorar os métodos mais comuns para identificar palavras erradas, com explicações e exemplos.\n",
    "\n",
    "\n",
    "1. **Comparação com um Dicionário:**\n",
    "   - **Descrição:** Um método básico é comparar cada palavra do texto com um dicionário de palavras válidas. Se uma palavra não estiver no dicionário, é marcada como errada.\n",
    "   - **Exemplo:** Se o texto contiver \"applle\", a palavra \"applle\" não será encontrada no dicionário e será marcada como incorreta.\n",
    "\n",
    "\n",
    "2. **Distância de Edição (Levenshtein):**\n",
    "   - **Descrição:** A distância de edição mede quantas operações (inserção, deleção, substituição) são necessárias para transformar uma palavra em outra. Palavras com uma pequena distância de edição em relação a palavras do dicionário são consideradas erradas.\n",
    "   - **Exemplo:** A palavra \"bok\" tem uma distância de edição de 1 em relação a \"book\" (substituição de 'k' por 'o'), indicando um possível erro de digitação.\n",
    "\n",
    "\n",
    "3. **Modelos de N-gramas:**\n",
    "   - **Descrição:** Modelos de N-gramas podem identificar palavras erradas com base na improbabilidade de uma sequência de palavras. Se uma palavra resulta em uma sequência de N-gramas que raramente ocorre no corpus de treinamento, pode ser considerada errada.\n",
    "   - **Exemplo:** Em \"I have a blak cat\", a sequência \"a blak cat\" pode ser menos provável que \"a black cat\", indicando que \"blak\" pode estar errado.\n",
    "\n",
    "\n",
    "4. **Redes Neurais e Modelos de Linguagem:**\n",
    "   - **Descrição:** Modelos de linguagem avançados, como BERT ou GPT, podem identificar palavras erradas ao avaliar a coerência do contexto. Eles são treinados em grandes quantidades de texto e podem detectar palavras que não fazem sentido no contexto dado.\n",
    "   - **Exemplo:** Em \"She drived the car\", um modelo de linguagem pode identificar que \"drived\" é incorreto no contexto e sugerir \"drove\".\n",
    "\n",
    "#### Exemplos Práticos de Identificação de Palavras Erradas\n",
    "\n",
    "1. **Exemplo com Dicionário:**\n",
    "   - **Texto:** \"I am lerning NLP.\"\n",
    "   - **Identificação:** A palavra \"lerning\" não está no dicionário.\n",
    "   - **Correção Proposta:** \"I am learning NLP.\"\n",
    "\n",
    "2. **Exemplo com Distância de Edição:**\n",
    "   - **Texto:** \"He went to the shcool.\"\n",
    "   - **Identificação:** A palavra \"shcool\" tem uma distância de edição de 1 em relação a \"school\".\n",
    "   - **Correção Proposta:** \"He went to the school.\"\n",
    "\n",
    "3. **Exemplo com Modelos de N-gramas:**\n",
    "   - **Texto:** \"The quick brown fox jmps over the lazy dog.\"\n",
    "   - **Identificação:** A sequência \"fox jmps\" é muito improvável.\n",
    "   - **Correção Proposta:** \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "4. **Exemplo com Modelos de Linguagem:**\n",
    "   - **Texto:** \"He gived her a gift.\"\n",
    "   - **Identificação:** O modelo de linguagem identifica que \"gived\" não faz sentido no contexto e sugere \"gave\".\n",
    "   - **Correção Proposta:** \"He gave her a gift.\"\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Palavras Novas ou Jargões:** Palavras que são novas, jargões ou nomes próprios podem não estar em um dicionário, levando a falsos positivos.\n",
    "- **Erros Contextuais:** Alguns erros só podem ser identificados corretamente no contexto adequado, o que modelos de linguagem mais avançados fazem melhor.\n",
    "- **Desempenho:** Métodos mais avançados, como modelos de linguagem, geralmente têm melhor desempenho, mas requerem mais poder computacional e dados para treinamento.\n",
    "\n",
    "\n",
    "Identificar palavras erradas é um processo que pode ser feito de várias maneiras, desde métodos simples como comparação com um dicionário até técnicas avançadas envolvendo modelos de linguagem. Cada método tem seus prós e contras, e a escolha do método adequado depende do contexto e dos requisitos específicos da aplicação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290ee17",
   "metadata": {},
   "source": [
    "### Cálculo da Distância de Edição\n",
    "\n",
    "A distância de edição, ou distância de Levenshtein, é uma métrica que mede o quão diferentes duas strings são, calculando o número mínimo de operações necessárias para transformar uma string em outra. As operações permitidas são inserção, deleção e substituição de caracteres.\n",
    "\n",
    "O algoritmo de Levenshtein usa uma abordagem de programação dinâmica para calcular a distância de edição entre duas strings. Aqui está uma descrição do algoritmo:\n",
    "\n",
    "1. **Inicialização:**\n",
    "   - Crie uma matriz $d$ de tamanho $(m+1) \\times (n+1)$, onde $m$ é o comprimento da primeira string $s$ e $n$ é o comprimento da segunda string $t$.\n",
    "   - Inicialize $d[i][0] = i$ para $i = 0, 1, ..., m$.\n",
    "   - Inicialize $d[0][j] = j$ para $j = 0, 1, ..., n$.\n",
    "\n",
    "2. **Recorrência:**\n",
    "   - Para cada $i = 1, ..., m$ e $j = 1, ..., n$:\n",
    "     - Se $s[i-1] = t[j-1]$, então $custo = 0$; caso contrário, $custo = 1$.\n",
    "     - $d[i][j] = \\min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + custo)$.\n",
    "\n",
    "3. **Resultado:**\n",
    "   - A distância de edição é encontrada em $d[m][n]$.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Aqui está uma implementação do cálculo da distância de edição em Python:\n",
    "\n",
    "```python\n",
    "def distancia_de_edicao(s, t):\n",
    "    m = len(s)\n",
    "    n = len(t)\n",
    "    \n",
    "    # Criação da matriz\n",
    "    d = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "    \n",
    "    # Inicialização da matriz\n",
    "    for i in range(m + 1):\n",
    "        d[i][0] = i\n",
    "    for j in range(n + 1):\n",
    "        d[0][j] = j\n",
    "    \n",
    "    # Preenchimento da matriz com os valores de distância\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                custo = 0\n",
    "            else:\n",
    "                custo = 1\n",
    "            d[i][j] = min(d[i - 1][j] + 1,  # Deleção\n",
    "                          d[i][j - 1] + 1,  # Inserção\n",
    "                          d[i - 1][j - 1] + custo)  # Substituição\n",
    "    \n",
    "    # Distância de edição é encontrada na última célula da matriz\n",
    "    return d[m][n]\n",
    "\n",
    "# Exemplos\n",
    "s1 = \"kitten\"\n",
    "s2 = \"sitting\"\n",
    "print(f\"Distância de edição entre '{s1}' e '{s2}': {distancia_de_edicao(s1, s2)}\")\n",
    "\n",
    "s3 = \"flaw\"\n",
    "s4 = \"lawn\"\n",
    "print(f\"Distância de edição entre '{s3}' e '{s4}': {distancia_de_edicao(s3, s4)}\")\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Strings:** \"kitten\" e \"sitting\"\n",
    "   - **Cálculo:** Transformar \"kitten\" em \"sitting\" envolve as operações:\n",
    "     - Substituir 'k' por 's': \"sitten\"\n",
    "     - Substituir 'e' por 'i': \"sittin\"\n",
    "     - Inserir 'g' no final: \"sitting\"\n",
    "   - **Distância de Edição:** 3\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Strings:** \"flaw\" e \"lawn\"\n",
    "   - **Cálculo:** Transformar \"flaw\" em \"lawn\" envolve as operações:\n",
    "     - Substituir 'f' por 'l': \"law\"\n",
    "     - Inserir 'n' no final: \"lawn\"\n",
    "   - **Distância de Edição:** 2\n",
    "\n",
    "#### Visualização da Matriz\n",
    "\n",
    "Para ilustrar como a matriz $d$ é preenchida, vejamos o exemplo das strings \"kitten\" e \"sitting\":\n",
    "\n",
    "```\n",
    "     '' s i t t i n g\n",
    "  '' 0  1 2 3 4 5 6 7\n",
    "  k  1  1 2 3 4 5 6 7\n",
    "  i  2  2 1 2 3 4 5 6\n",
    "  t  3  3 2 1 2 3 4 5\n",
    "  t  4  4 3 2 1 2 3 4\n",
    "  e  5  5 4 3 2 2 3 4\n",
    "  n  6  6 5 4 3 3 2 3\n",
    "```\n",
    "\n",
    "A distância de edição é uma métrica poderosa para comparar a similaridade entre duas strings, sendo amplamente utilizada em tarefas de NLP, como correção ortográfica e reconhecimento de padrões. O algoritmo de Levenshtein é eficiente e fácil de implementar, fornecendo uma base sólida para muitas aplicações de processamento de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a8a90b",
   "metadata": {},
   "source": [
    "### Métodos para Filtrar Palavras Candidatas\n",
    "\n",
    "Filtrar palavras candidatas é um passo crucial após identificar palavras erradas para fornecer sugestões de correção. Este processo envolve gerar uma lista de possíveis correções para uma palavra mal escrita e, em seguida, classificar ou selecionar as melhores opções com base em vários critérios. Aqui estão os principais métodos para filtrar palavras candidatas, com explicações e exemplos.\n",
    "\n",
    "\n",
    "1. **Distância de Edição:**\n",
    "   - **Descrição:** Calcular a distância de edição entre a palavra errada e cada palavra do dicionário. As palavras com a menor distância de edição são consideradas as melhores candidatas.\n",
    "   - **Exemplo:** Para a palavra \"speling\", palavras como \"spelling\" (distância 1) e \"spieling\" (distância 2) seriam consideradas, com \"spelling\" sendo a melhor candidata devido à menor distância de edição.\n",
    "\n",
    "2. **N-gramas e Modelos de Linguagem:**\n",
    "   - **Descrição:** Utilizar modelos de N-gramas ou modelos de linguagem para avaliar a probabilidade das palavras candidatas no contexto da frase.\n",
    "   - **Exemplo:** Em \"I have a blak cat\", o modelo pode sugerir \"black\" como correção para \"blak\" porque \"a black cat\" é uma sequência mais comum e provável do que \"a blak cat\".\n",
    "\n",
    "3. **Frequência no Corpus:**\n",
    "   - **Descrição:** As palavras candidatas são classificadas com base na sua frequência em um corpus de texto. Palavras mais comuns são mais prováveis de serem correções corretas.\n",
    "   - **Exemplo:** Se \"hte\" foi digitado incorretamente e \"the\" é muito mais frequente no corpus do que outras combinações possíveis, \"the\" será a principal sugestão.\n",
    "\n",
    "4. **Contexto Semântico:**\n",
    "   - **Descrição:** Analisar o contexto semântico utilizando embeddings de palavras ou modelos contextuais (como BERT) para selecionar palavras que façam sentido no contexto da frase.\n",
    "   - **Exemplo:** Em \"I visited the captial city\", o modelo pode sugerir \"capital\" em vez de \"captial\" porque \"visited the capital city\" faz sentido semanticamente.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Vamos ver como esses métodos podem ser combinados para filtrar palavras candidatas. Para simplificação, usaremos apenas a distância de edição e a frequência no corpus neste exemplo.\n",
    "\n",
    "1. Calcular a distância de edição para cada palavra do dicionário.\n",
    "2. Classificar palavras por distância de edição.\n",
    "3. Utilizar frequência no corpus para ordenar palavras com a mesma distância.\n",
    "\n",
    "```python\n",
    "def distancia_de_edicao(s, t):\n",
    "    m = len(s)  # Comprimento da primeira string\n",
    "    n = len(t)  # Comprimento da segunda string\n",
    "\n",
    "    # Inicializa uma matriz (m+1) x (n+1) com zeros\n",
    "    d = [[0 for _ in range(n + 1)] for _ in range(m + 1)]\n",
    "\n",
    "    # Inicializa a primeira coluna da matriz\n",
    "    for i in range(m + 1):\n",
    "        d[i][0] = i\n",
    "\n",
    "    # Inicializa a primeira linha da matriz\n",
    "    for j in range(n + 1):\n",
    "        d[0][j] = j\n",
    "\n",
    "    # Preenche a matriz com os valores de distância de edição\n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s[i - 1] == t[j - 1]:\n",
    "                custo = 0  # Nenhum custo se os caracteres são iguais\n",
    "            else:\n",
    "                custo = 1  # Custo de substituição se os caracteres são diferentes\n",
    "            d[i][j] = min(d[i - 1][j] + 1,    # Custo de deleção\n",
    "                          d[i][j - 1] + 1,    # Custo de inserção\n",
    "                          d[i - 1][j - 1] + custo)  # Custo de substituição\n",
    "\n",
    "    return d[m][n]  # Retorna a distância de edição entre as duas strings\n",
    "\n",
    "def filtrar_candidatos(palavra_errada, dicionario, frequencia_corpus):\n",
    "    candidatos = []\n",
    "    # Calcula a distância de edição para cada palavra do dicionário\n",
    "    for palavra in dicionario:\n",
    "        dist = distancia_de_edicao(palavra_errada, palavra)\n",
    "        candidatos.append((palavra, dist))  # Adiciona a palavra e sua distância à lista de candidatos\n",
    "\n",
    "    # Ordena os candidatos primeiro pela distância de edição e depois pela frequência no corpus (em ordem decrescente)\n",
    "    candidatos.sort(key=lambda x: (x[1], -frequencia_corpus.get(x[0], 0)))\n",
    "\n",
    "    # Retorna apenas as palavras candidatas, sem as distâncias\n",
    "    return [candidato[0] for candidato in candidatos]\n",
    "\n",
    "# Exemplo de dicionário e frequências\n",
    "dicionario = [\"spelling\", \"spieling\", \"selling\", \"smiling\"]\n",
    "frequencia_corpus = {\"spelling\": 500, \"spieling\": 5, \"selling\": 300, \"smiling\": 150}\n",
    "\n",
    "# Palavra errada\n",
    "palavra_errada = \"speling\"\n",
    "\n",
    "# Filtrar palavras candidatas\n",
    "candidatos = filtrar_candidatos(palavra_errada, dicionario, frequencia_corpus)\n",
    "print(f\"Candidatos para '{palavra_errada}': {candidatos}\")\n",
    "\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Palavra Errada:** \"speling\"\n",
    "   - **Dicionário:** [\"spelling\", \"spieling\", \"selling\", \"smiling\"]\n",
    "   - **Frequência no Corpus:** {\"spelling\": 500, \"spieling\": 5, \"selling\": 300, \"smiling\": 150}\n",
    "   - **Candidatos:** [\"spelling\", \"selling\", \"smiling\", \"spieling\"]\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Palavra Errada:** \"recieve\"\n",
    "   - **Dicionário:** [\"receive\", \"recipe\", \"recite\"]\n",
    "   - **Frequência no Corpus:** {\"receive\": 1000, \"recipe\": 400, \"recite\": 50}\n",
    "   - **Candidatos:** [\"receive\", \"recipe\", \"recite\"]\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Balanceamento entre Precisão e Performance:** Calcular a distância de edição para um grande dicionário pode ser computacionalmente caro. Técnicas como truncagem de dicionário ou uso de índices podem melhorar a performance.\n",
    "- **Combinação de Critérios:** Usar múltiplos critérios (distância de edição, frequência no corpus, contexto semântico) pode aumentar a precisão das sugestões.\n",
    "- **Personalização:** Ajustar a frequência do corpus com base no domínio específico (por exemplo, termos médicos para um dicionário médico) pode melhorar os resultados.\n",
    "\n",
    "\n",
    "Filtrar palavras candidatas é um processo multifacetado que combina técnicas de comparação de strings, análise de frequência e compreensão contextual. Implementar uma abordagem robusta pode significativamente melhorar a precisão das sugestões de correção em sistemas de autocorreção e outras aplicações de NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260f913",
   "metadata": {},
   "source": [
    "### Métodos para Calcular as Probabilidades das Palavras\n",
    "\n",
    "Calcular as probabilidades das palavras é um passo importante para melhorar a precisão de sugestões em tarefas como correção ortográfica. Esse processo envolve a utilização de modelos de linguagem que atribuem uma probabilidade a cada palavra candidata com base em diferentes critérios, como frequência no corpus, contexto e modelos de n-gramas.\n",
    "\n",
    "\n",
    "1. **Frequência no Corpus:**\n",
    "   - **Descrição:** As palavras mais frequentes em um grande corpus de texto são consideradas mais prováveis.\n",
    "   - **Exemplo:** Se a palavra \"the\" aparece 5000 vezes em um corpus, sua probabilidade é maior do que a de uma palavra que aparece apenas 5 vezes.\n",
    "\n",
    "<img src=\"./imgs/calculating_word_prob.png\">\n",
    "\n",
    "2. **Modelos de N-gramas:**\n",
    "   - **Descrição:** Usar a frequência de sequências de palavras (n-gramas) para calcular a probabilidade de uma palavra em um dado contexto.\n",
    "   - **Exemplo:** Em um modelo de bigrama, a probabilidade de \"cat\" seguir \"the\" pode ser calculada como $(P(cat|the) = \\frac{C(the\\ cat)}{C(the)} $, onde $C$ denota contagens no corpus.\n",
    "\n",
    "\n",
    "3. **Modelos de Linguagem Baseados em Redes Neurais:**\n",
    "   - **Descrição:** Modelos como Word2Vec, GloVe, BERT, ou GPT podem gerar probabilidades para palavras com base em embeddings de palavras e contexto.\n",
    "   - **Exemplo:** Dado o contexto \"I have a bl__ cat\", um modelo de linguagem pode calcular a probabilidade de várias palavras preencherem o espaço em branco, como \"black\", \"blue\", etc.\n",
    "\n",
    "#### Implementação em Python\n",
    "\n",
    "Vamos ver como podemos implementar o cálculo de probabilidades de palavras usando frequências no corpus e um simples modelo de bigrama. Para isso precisamos seguir os seguintes passos:\n",
    "\n",
    "1. **Frequência no Corpus:** Calcular a probabilidade baseada na frequência relativa da palavra no corpus.\n",
    "2. **Modelo de Bigramas:** Calcular a probabilidade de uma palavra dado seu contexto anterior usando bigramas.\n",
    "\n",
    "\n",
    "```python\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Corpus de exemplo\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog sat on the log\",\n",
    "    \"the cat chased the dog\",\n",
    "    \"the dog barked at the cat\"\n",
    "]\n",
    "\n",
    "# Função para calcular frequências unigrama e bigrama\n",
    "def calcular_frequencias(corpus):\n",
    "    unigramas = Counter()\n",
    "    bigramas = defaultdict(Counter)\n",
    "    \n",
    "    for frase in corpus:\n",
    "        palavras = frase.split()\n",
    "        for i in range(len(palavras)):\n",
    "            unigramas[palavras[i]] += 1\n",
    "            if i > 0:\n",
    "                bigramas[palavras[i-1]][palavras[i]] += 1\n",
    "    \n",
    "    total_palavras = sum(unigramas.values())\n",
    "    return unigramas, bigramas, total_palavras\n",
    "\n",
    "# Calcular frequências no corpus\n",
    "unigramas, bigramas, total_palavras = calcular_frequencias(corpus)\n",
    "\n",
    "# Função para calcular probabilidade unigrama\n",
    "def probabilidade_unigrama(palavra):\n",
    "    return unigramas[palavra] / total_palavras\n",
    "\n",
    "# Função para calcular probabilidade bigrama\n",
    "def probabilidade_bigrama(palavra_anterior, palavra):\n",
    "    if palavra_anterior in bigramas and palavra in bigramas[palavra_anterior]:\n",
    "        return bigramas[palavra_anterior][palavra] / unigramas[palavra_anterior]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Exemplo de cálculo de probabilidades\n",
    "palavra = \"cat\"\n",
    "palavra_anterior = \"the\"\n",
    "\n",
    "print(f\"Probabilidade unigrama de '{palavra}': {probabilidade_unigrama(palavra)}\")\n",
    "print(f\"Probabilidade bigrama de '{palavra}' dado '{palavra_anterior}': {probabilidade_bigrama(palavra_anterior, palavra)}\")\n",
    "\n",
    "output:\n",
    "Probabilidade unigrama de 'cat': 0.13043478260869565\n",
    "Probabilidade bigrama de 'cat' dado 'the': 0.375\n",
    "```\n",
    "\n",
    "#### Exemplos Práticos\n",
    "\n",
    "1. **Exemplo 1:**\n",
    "   - **Palavra:** \"cat\"\n",
    "   - **Probabilidade Unigrama:** Calculada como a frequência da palavra \"cat\" dividida pelo total de palavras no corpus.\n",
    "   - **Probabilidade Bigrama:** Calculada como a frequência da sequência \"the cat\" dividida pela frequência de \"the\".\n",
    "\n",
    "2. **Exemplo 2:**\n",
    "   - **Palavra:** \"dog\"\n",
    "   - **Probabilidade Unigrama:** Calculada como a frequência da palavra \"dog\" dividida pelo total de palavras no corpus.\n",
    "   - **Probabilidade Bigrama:** Calculada como a frequência da sequência \"the dog\" dividida pela frequência de \"the\".\n",
    "\n",
    "#### Considerações\n",
    "\n",
    "- **Balanceamento de Dados:** Um corpus pequeno ou desequilibrado pode levar a estimativas de probabilidade imprecisas. É importante utilizar um corpus representativo e suficientemente grande.\n",
    "- **Suavização:** Técnicas de suavização, como Laplace ou Good-Turing, são usadas para lidar com bigramas ou n-gramas que não aparecem no corpus.\n",
    "- **Modelos Avançados:** Modelos de linguagem baseados em redes neurais (como BERT ou GPT) podem fornecer probabilidades mais precisas ao considerar o contexto completo de uma frase.\n",
    "\n",
    "Calcular as probabilidades das palavras é um componente essencial em muitas aplicações de NLP, incluindo correção ortográfica e autocompletar. Usando frequências no corpus e modelos de n-gramas, podemos estimar essas probabilidades de forma eficaz. Em aplicações mais avançadas, modelos de linguagem baseados em redes neurais podem proporcionar uma compreensão mais profunda e precisa do contexto, resultando em sugestões de palavras ainda mais acertadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a8dc88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Minimum edit distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52640aa0",
   "metadata": {},
   "source": [
    "### Operações de Distância Mínima de Edição\n",
    "\n",
    "A distância mínima de edição (ou distância de Levenshtein) é uma métrica que mede o número mínimo de operações necessárias para transformar uma string em outra. As operações permitidas são inserção, deleção e substituição de caracteres. Vamos detalhar cada operação e como elas são usadas para calcular a distância mínima de edição, seguido de exemplos práticos.\n",
    "\n",
    "\n",
    "1. **Inserção (Insert):** Adicionar um caractere à string. Exemplo: Transformar \"cat\" em \"cats\" requer uma inserção de 's' no final. A operação é `Insert('s')`.\n",
    "\n",
    "2. **Deleção (Delete):** Remover um caractere da string. Exemplo: Transformar \"cats\" em \"cat\" requer uma deleção de 's'. A operação é `Delete('s')`.\n",
    "\n",
    "3. **Substituição (Replace):** Substituir um caractere por outro. Exemplo: Transformar \"cat\" em \"cut\" requer uma substituição de 'a' por 'u'. A operação é `Replace('a', 'u')`.\n",
    "\n",
    "Podemos atribuir pesos diferentes para cada operação, para assim, buscar otimizar a menor distancia. Por exemplo, atribuir os pesos de 1, 1 e 2 para insert, delete e replace. Para buscar a menor distancia utilizamos o algoritmod e Levenshtein.\n",
    "\n",
    "<img src=\"./imgs/edit_distance_w.png\">\n",
    "\n",
    "O algoritmo de Levenshtein usa programação dinâmica para calcular a distância mínima de edição entre duas strings. Como já citado acima, segue os seguintes passos:\n",
    "\n",
    "1. **Inicialização:**\n",
    "   - Crie uma matriz `d` de tamanho `(m+1) x (n+1)`, onde `m` é o comprimento da primeira string `s` e `n` é o comprimento da segunda string `t`.\n",
    "   - Inicialize `d[i][0] = i` para `i = 0, 1, ..., m`.\n",
    "   - Inicialize `d[0][j] = j` para `j = 0, 1, ..., n`.\n",
    "\n",
    "2. **Preenchimento da Matriz:**\n",
    "   - Para cada `i = 1, ..., m` e `j = 1, ..., n`:\n",
    "     - Se `s[i-1] == t[j-1]`, então `custo = 0`; caso contrário, `custo = 1`.\n",
    "     - `d[i][j] = min(d[i-1][j] + 1, d[i][j-1] + 1, d[i-1][j-1] + custo)`.\n",
    "\n",
    "3. **Resultado:**\n",
    "   - A distância de edição é encontrada em `d[m][n]`.\n",
    "\n",
    "(Ver #2.3.2.1 a implementação e exemplos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc08fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Minimum edit distance algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08d98a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T18:46:18.739139Z",
     "start_time": "2024-06-18T18:46:18.722360Z"
    }
   },
   "source": [
    "Quando estamos computando a distancia mínima de edição, começamos com a palavra fonte e temos que transformá-la na palavra alvo. \n",
    "\n",
    "<img src=\"./imgs/minimum_edit_target.png\">\n",
    "     \n",
    "Partindo de # para #, teremos um custo de 0. Partindo de p para #, teremos um custo de 1, porque faremos uma deleção. De p para s, teremos um custo de 2, porque esse é o custo mínimo que se poderia ter para ir de p a s. Podemos continuar assim preenchendo um elemento por vez."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176acace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T18:53:33.139135Z",
     "start_time": "2024-06-18T18:53:33.133892Z"
    }
   },
   "source": [
    "Para preencher a seguinte tabela:\n",
    "\n",
    "<img src=\"./imgs/minimum_edit1.png\">\n",
    "\n",
    "\n",
    "Existem três equações:\n",
    "\n",
    "- $D[i,j] = D[i-1, j] + \\text{del_cost}$: indica que você deseja preencher a célula atual (i,j) usando o custo na célula encontrada diretamente acima.\n",
    "\n",
    "- $D[i,j] = D[i, j-1] + \\text{ins_cost}$: indica que você deseja preencher a célula atual (i,j) usando o custo na célula localizada diretamente à sua esquerda.\n",
    "\n",
    "- $D[i,j] = D[i-1, j-1] + \\text{rep_cost}$: o custo rep pode ser 2 ou 0 dependendo se você vai realmente substituí-lo ou não.\n",
    "\n",
    "A cada passo você verifica os três caminhos possíveis de onde pode vir e seleciona o menos caro. Quando terminar, você obterá o seguinte:\n",
    "\n",
    "<img src=\"./imgs/minimum_edit2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4131c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## Minimum edit distance algorithm II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6564c41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "Explique, descreva e dê exemplos de mispelled words e additive words em autocorrects em NLP\n",
    "Explique, descreva e dê exemplos de como calcular probabilidades de palavras corretas em autocorrects em NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e68af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## SUBTITULO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03473649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## SUBTITULO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a4e44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## SUBTITULO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f198f6e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## SUBTITULO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2986f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## SUBTITULO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240f7335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## SUBTITULO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c3ecdb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## SUBTITULO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4376801",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## SUBTITULO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd52992",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-18T17:08:15.369408Z",
     "start_time": "2024-06-18T17:08:15.365911Z"
    }
   },
   "source": [
    "## SUBTITULO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53900e8d",
   "metadata": {},
   "source": [
    "# Week 2 - Part of Speech Tagging and Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a317cd59",
   "metadata": {},
   "source": [
    "# Week 3 - Autocomplete and Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6810f4e",
   "metadata": {},
   "source": [
    "# Week 4 - Word Embedding with Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356405a1",
   "metadata": {},
   "source": [
    "# Referência\n",
    "- Natural Language Processing with Classification and Vector Spaces, disponível em https://www.coursera.org/learn/probabilistic-models-in-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae14037c",
   "metadata": {},
   "source": [
    "# Licença\n",
    "- CC BY-SA 2.0 LEGAL CODE. Attribution-ShareAlike 2.0 Generic\n",
    "- Para detalhes sobre a licença, verifique https://creativecommons.org/licenses/by-sa/2.0/legalcode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317.448px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
